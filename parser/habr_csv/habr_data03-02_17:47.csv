post_id'post_id'title'description'source_link'body'image'images
0'717710'Как мы быстро запустили мобильное приложение и споткнулись о свой успех'Привет, Хабр! Мы крупная производственная компания с 50К+ сотрудников, и в 2019 году поняли, что нам нужно мобильное приложение. Срок реализации 5 месяцев. Какой стек вы бы выбрали при такой...'https://habr.com/ru/post/717710/'"Привет, Хабр! Мы крупная производственная компания с 50К+ сотрудников, и в 2019 году поняли, что нам нужно мобильное приложение. Срок реализации 5 месяцев. Какой стек вы бы выбрали при такой скорости? Мы выбрали нативные Kotlin и Swift. Поначалу запилили всего 6 сервисов (новости, зарплатный лист, отпуска, блоги, регистрацию опасностей, выдачу СИЗ), и даже при том, что нанесли минимальную пользу, приложение очень зашло, количество пользователей начало расти лавинообразно. И тут мы поняли, что серверная часть на node.js + PostgreSQL создана без всякой мысли о развитии и масштабировании, решала исключительно локальные задачи. Все было на неоптимальной монолитной архитектуре развивать и поддерживать которую просто нельзя.

Расскажу, как мы решили проблему.

___________

Сейчас количество нативных сервисов в нашем приложении более 20. Помимо перечисленных выше, из самых интересных у нас имеются:

· мобильный пропуск, который позволяет проходить через турникеты на проходных по смартфонам;

· бронирование рабочих мест и парковок – для гибридных режимов работы офисных сотрудников;

· карта комбината, которая позволяет сотрудникам на липецкой площадке (26 кв. км) в деталях увидеть границы цехов, пешеходные дорожки, столовые и другие точки притяжения, такие как автобусные остановки или парк на территории комбината, медицинские пункты, проходные;

· Сервис заказа справок, чьей особенностью является полностью динамическое формирование полей на фронтенде на основании структуры, которая передается с бэка.

· И многое другое…

· 5 месяцев - время разработки мобильного приложения

· 30 000 MAU – количество уникальных пользователей

· 50 000 сотрудников в Группе НЛМК

Сервисов много и все они на бэкенде сидят, как уже было сказано выше, на монолитной и трудно поддерживаемой архитектуре. Это произошло исторически, в связи с фокусом на быстрые результаты в ущерб качественно проработанному решению. Что, я считаю, вполне нормально в концепции любых стартапов или создании MVP. Однако, когда мы говорим о развитии, то такие застарелые монолиты нуждаются в реформировании. В противном случае команда продукта будет похоронена под тяжестью legacy вкладывая все силы в его поддержку и стабилизацию вместо развития по все возрастающим потребностям внутри компании.

В общем, решили рядом с чистого листа создать новое решение на гибкой архитектуре. Дело за малым, выбрать брать готовое решение делать свое.

Из имеющихся наиболее полных и комплексных решений был только LoopBack 3, но он жестко завязан на собственную модель данных, а нам хотелось иметь свободу выбора.

Решили делать свой велосипед. Благо, начало проекта с небольших сервисов позволяло немного экспериментировать.

Архитектура послойная, напоминающая структуру слоев модели OSI. Слои снизу-вверх:

1. Сервер TCP соединений

2. Сервер HTTP

3. Обработка конкретных запросов HTTP (GET, HEAD, POST, PUT, DELETE)

4. Контроллер обрабатывает тело запроса и формирует тело ответа

5. Сервис принимает данные, излеченные из тела запроса контроллером и возвращает данные, которые контролер помещает в тело ответа. Именно тут содержится бизнес логика

6. Источники данных, модели, которые используются сервисами для реализации бизнес логики

Преимущество такого подхода заключается в том, что слои абстрагируют свою внутреннюю логику. Это позволяет, при необходимости, безболезненно менять реализацию внутреннего механизма, не перестраивая все приложение.

Задачу первых двух уровней взял на себя Express, который имеет так же удобный механизм создания скелета проекта.

И так, расскажу, как и что делали

Установку Node.js пропустим, предполагая, что он уже стоит.

Устанавливаем express проект генератор:

$ npm install -g express-generator

Затем с его помощью создаем болванку проекта. Так как мы реализует API, то движок шаблонов на не нужен, указываем флаг –-no-view :

$ express my_rest_api –-no-view

Переходим в целевую папку my_rest_api и устанавливаем зависимости:

$ cd my_rest_api

$ npm install

Удаляем парку public, она нам не нужна. В итоге получаем следующую структуру:

. ├── app.js ├── bin │ └── www ├── node_modules ├── package.json ├── package-lock.json └── routes ├── index.js └── users.js

Сервер уже работоспособен и если ввести команду:

$ npm start

то сервер запуститься и будет обрабатывать запросы на порту по умолчанию 3000.

Удаляем все содержимое папки routes, маршруты у нас будут выглядеть по-другому.

Начинаем с исправления файла app.js, содержащего код сервера:

const express = require('express') const ExpressPinoLogger = require('express-pino-logger') const cookieParser = require('cookie-parser') const pino = ExpressPinoLogger({ serializers: { req: (req) => ({ method: req.method, url: req.url, user: req.raw.user, }), }, }) const app = express() app.use(pino) app.use(express.json()) app.use(express.urlencoded({extended: false})) app.use(cookieParser()) app.use('/', require('./routes')) app.use((req, res, next) => { res.status(404) .json({ code: 404, title: `That resource ""${req.url}"" was not found`, description: `Ресурс ""${req.url}"" не найден` }) }) module.exports = app

Меняем все var на const.

Добавляем модуль express-pino-logger который позволит писать логи в формате json в заданным набором полей.

В самой последней директиве use добавляем метод обработчик ошибок 404.

Создадим в корне проекта парку base, в ней будут лежать вспомогательные классы и методы, которые помогут нам создать полноценное REST API.

Первый метод getRoutes формирует маршруты на основе структуры папок, он вызывается в модулях в директории routes. Листинг метода выглядит так:

get_routes.js

const fs = require('fs') const path = require('path') /** * * @param {string}dirName * @param {object}router */ module.exports = (dirName, router) => { const basename = 'index.js' fs.readdirSync(dirName) .filter(item => { return (item.indexOf('.') !== 0) && (item !== basename) }) .forEach(item => { const itemBody = require(path.join(dirName, item)) let pathName = fs.lstatSync(path.join(dirName, item)).isDirectory() ? item : item.replace('.js', '') router.use('/' + pathName, itemBody) }) }

Метод сканирует текущую директорию и подгружает модули в директиву use.

При следующей структуре парки routes:

routes ├── api │ ├── v1 │ │ ├── users │ │ │ └── index.js │ │ ├── groups │ │ │ └── index.js │ │ └── index.js │ └── index.js └── index.js

Мы получаем следующее REST API:

· GET /api/v1/users – лист пользователей

· GET /api/v1/users/{userId} – один пользователей

· POST /api/v1/users – создать одного пользователя

· PUT /api/v1/users/{userId} – обновить одного пользователя

· DELETE /api/v1/users/{userId} – удалить одного пользователя

· GET /api/v1/groups – лист групп

· GET /api/v1/ groups /{groupId} – одна группа

· POST /api/v1/ groups – создать одну группу

· PUT /api/v1/ groups /{groupId} – обновить одну группу

· DELETE /api/v1/ groups /{groupId} – удалить одну группу

Индексные файлы index.js тут двух видов:

index.js – без обработки запросов, транзитные, которые сканируют директорию и формируют маршруты

const router = require('express').Router() const getModules = require('../base/get_routes') getModules(__dirname, router) module.exports = router

index.js – индексные файлы с обработкой запросов, которые делают то же самое что и первые, а также обрабатывают запросы методов http (GET, POST etc.)

const router = require('express').Router() const getRoutes = require('../../base/get_routes') const baseCrud = require('../../base/base_crud') const exampleController = require('../../controllers/example') getRoutes(__dirname, router) baseCrud(router, exampleController) module.exports = router

Для реализации обработчиков мы интегрируем с текущим экземпляром router экземпляр контроллера exampleController.

Код метода baseCrud, добавляющий обработчики, выглядит так:

/** * * @param {Router}router * @param {BaseController}controller */ function baseCrud(router, controller) { router.get('/', (req, res, next) => controller.index(req, res, next)) .post('/', (req, res, next) => controller.store(req, res, next)) .get('/:id', (req, res, next) => controller.show(req, res, next)) .put('/:id', (req, res, next) => controller.update(req, res, next)) .delete('/:id', (req, res, next) => controller.destroy(req, res, next)) } module.exports = baseCrud

Первыми аргументом передается экземпляр маршрута, вторым контроллер, отвечающий за обработку запросов.

Базовый класс контроллер выглядит так:

class BaseController { /** * {BaseService} */ service /** * * @param {BaseService}service */ constructor(service) { this.service = service } /** * Обработка результата работы сервиса и возврат его клиенту * @param {Promise<any>}serviceMethod * @param req * @param res */ sendReplay(serviceMethod, req, res) { serviceMethod .then(result => res.json(result)) .catch(error => { req.log.child({module: 'controller'}).error(error) res.status(error.status || 500) .json({status: error.status, message: error.message, description: error.description}) }) } /** * Обзаботка запроса на получение одной сущности по уникальному идентификатору id * @param req * @param res */ show(req, res) { this.sendReplay(this.service.getById(req.params.id, req.user), req, res) } /** * Обзаботка запроса на получение листа сущностей по пареметрам фильтра, указанным в query * @param req * @param res */ index(req, res) { this.sendReplay(this.service.getList(req.query, req.user), req, res) } /** * Обзаботка запроса на создание сущности из данных переданных в body * @param req * @param res */ store(req, res) { this.sendReplay(this.service.createItem(req.body, req.user), req, res) } /** * Обзаботка запроса на обновление сущности из данных переданных в body с идентификацией ее по id * @param req * @param res */ update(req, res) { this.sendReplay(this.service.updateItem(req.params.id, req.body, req.user), req, res) } /** * Обзаботка запроса на удаление сущности по ее id * @param req * @param res */ destroy(req, res) { this.sendReplay(this.service.destroyItem(req.params.id, req.user), req, res) } } module.exports = BaseController

Реализация экземпляра класса контроллера:

const BaseController = require('../base/base_controller') const ExampleService = require('../services/example') module.exports = new BaseController(new ExampleService())

В конструкторе ему передается экземпляр целевого сервиса. И методы контроллера взывают методы сервиса, передавая им в параметрах нужные значения из req и отдают результат работы в res в формате json.

Класс базовый сервис выглядит так:

const MethodNotImplementedError = require('./errors/method_not_implemented_error') class BaseService { /** * * @param {{object}}data * @param {{object}}user * @returns {Promise<never>} */ async createItem(data, user) { throw new MethodNotImplementedError() } /** * * @param id * @param {{object}}data * @param {{object}}user * @returns {Promise<never>} */ async updateItem(id, data, user) { throw new MethodNotImplementedError() } /** * * @param {string}id * @param {{object}}user * @returns {Promise<never>} */ async destroyItem(id, user) { throw new MethodNotImplementedError() } /** * * @param {string}id * @param {{object}}user * @returns {Promise<never>} */ async getById(id, user) { throw new MethodNotImplementedError() } /** * * @param {{object}}data * @param {{object}}user * @returns {Promise<any>} */ async getList(data, user) { throw new MethodNotImplementedError() } } module.exports = BaseService

Методы базовый класса сервиса имеют заглушки и при их вызове выбрасывается исключение MethodNotImplementedError.

Рабочий сервис выглядит так:

const BaseService = require('../base/base_service') class Example extends BaseService { async getList(data, user) { return [{itemName: 'SomeItem'}] } async getById(id, user) { return {itemName: 'SomeItem'} } async createItem(data, user) { return {itemName: 'SomeItem'} } async destroyItem(id, user) { return {} } async updateItem(id, data, user) { return {itemName: 'SomeItem'} } } module.exports = Example

В нем переопределённые методы сервиса содержат бизнес логику. Именно тут разработчик и реализует механизм работы с данными.

Для обработки ошибочных ситуаций мы используем исключения. Классы ошибок, которые выбрасываются, содержат нужные поля для отображения описания и коды ошибки http для клиента API.

Классы ошибок лежат в папке /base/errors

http_error.js – базовый класс ошибки, с кодом 500

class HttpError extends Error { status = 500 message = 'Internal server error' description = 'Внутренняя ошибка сервера' constructor(description = null) { super(); this.description = description || this.description } } module.exports = HttpError

Если он выбрасывается в исключении, то клиент получает:

{ ""status"": 500, ""message"": ""Internal server error"", ""description"": ""Внутренняя ошибка сервера"" }

Таким образом, мы полностью абстрагируем сервисный уровень от формирования ошибок с заданными кодами http.

auth_error.js – класс ошибки авторизации

const HttpError = require('./http_error') class AuthError extends HttpError { status = 401 message = 'Not authorized' description = 'Вы неавторизованы' } module.exports = AuthError

forbidden_error.js – класс ошибки прав доступа

const HttpError = require('./http_error') class ForbiddenError extends HttpError { status = 403 message = 'Forbidden' description = 'Данный ресурс вам недоступен' } module.exports = ForbiddenError

method_not_implemented_error.js – класс ошибки не реализованного метода

const HttpError = require('./http_error') class MethodNotImplementedError extends HttpError { status = 405 message = 'Not implemented' description = 'Метод не рализован' } module.exports = MethodNotImplementedError

При желании можно реализовать любой вид ошибки, расширяя класс HttpError.

Что же мы получили в итоге

А вот, что: теперь мы имеем готовую болванку для реализации REST API. Программисту не нужно думать, как организовывать код, в какие папки его помещать. Все структурированно и разделено по слоям.

Мы просто клонируем в заданную папку на машине разработчика репозиторий болванку и допиливаем ее до нужной кондиции, добавляя идентичные модули маршрутов, контроллеров, и лишь в модулях сервиса реализуем ту специфику, которую требует конкретное приложение.

Все это позволяет поставить на поток создание типового микросеривса, которых зачастую надо очень много и быстро.

Такой подход позволяет нам без затруднений строить приложения в рамках мобильной платформы. Используя отлаженные механизмы. Это можно сравнить с типовым жилищным строительством. Когда имеется типовой проверенный проект здания, типовые составные части, типовой технологический процесс создания составных частей и типовой процесс сборки частей в готовое здание. Основная масса решений, как правило, типовые, что это ускоряет и удешевляет процесс. Для индивидуальных проектных решений используются так же типовые элементы, но они более мелкие, на уровне базовых технологий.

А теперь спрашивайте, отвечу!"'https://habrastorage.org/getpro/habr/upload_files/b97/d3f/257/b97d3f257d4252bdad4d1ff038a76a8d.jpg'"['https://habrastorage.org/getpro/habr/company/f94/8cb/bd3/f948cbbd32b9bb3aaffe413086a85906.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w32/getpro/habr/avatars/aec/d0f/fe4/aecd0ffe47cd9a5eb1b97422b6947930.jpg', 'https://habrastorage.org/getpro/habr/upload_files/abf/53a/1b2/abf53a1b2d11ef0e1fbca7f79fdfd990.jfif', 'https://habrastorage.org/getpro/habr/upload_files/b97/d3f/257/b97d3f257d4252bdad4d1ff038a76a8d.jpg', 'https://habrastorage.org/getpro/habr/avatars/aec/d0f/fe4/aecd0ffe47cd9a5eb1b97422b6947930.jpg']"
1'720028'Как я организовал продажу электронной версии Книги нормального фрилансера и с какими трудностями столкнулся'Я написал Книгу нормального фрилансера. В ней поделился опытом работы проектировщиком (UX-дизайнером) с 2006 года по сегодняшний день. На момент написания этой статьи с книгой познакомилось не меньше...'https://habr.com/ru/post/720028/'"Я написал Книгу нормального фрилансера. В ней поделился опытом работы проектировщиком (UX-дизайнером) с 2006 года по сегодняшний день. На момент написания этой статьи с книгой познакомилось не меньше 2 000 человек. Десятки положительных отзывов и благодарностей — и пока ни одного негативного.

Друзья недоумевали, зачем я выкладываю книгу бесплатно на своём сайте и при этом выставляю её на продажу. Ведь так её точно никто не купит! А я хотел, чтобы, во-первых, её мог прочитать каждый, даже если у него нет денег, а, во-вторых, чтобы те, кто в состоянии её приобрести, могли бы меня таким образом поддержать. И вот что из этого вышло…

Чтобы продавать книгу, сначала мне нужно было её написать и отредактировать. Я уже выкладывал отдельный пост с рассказом об этом. Здесь скажу только, что потихоньку занимался этим с 2019 по 2023 годы, периодически отвлекаясь на что-то другое. А ещё мне пришлось пару раз переписать книгу. В первый раз — когда понял, что не опытом делюсь, а выводами из этого опыта. Во второй — уже во время редактуры.

А ещё дописываешь одну главу — а на её месте появляется две новых, незапланированных. Потому что чем больше я делился опытом — тем больше всплывало важных моментов, о которых тоже хотелось бы рассказать. Поэтому к концу 2022 года я решил остановиться на том, что есть, иначе это превратилось бы в бесконечную работу. А новые главы приберёг для следующих изданий.

Настало время экспортировать книгу в разных форматах для читалок. Я работал над рукописью в гугл.доках. Это было удобно и с точки зрения доступа с разных устройств, и комментирования, и приглашения к работе бета-ридеров. В какой-то момент я выделил весь текст книги, скопировал его из исходного документа и вставил в новый — чистовик. Удивился, что при одинаковом количестве символов и форматировании документы различались на несколько страниц по объёму, и так и не разобрался, почему это произошло. В чистовике удалил все лишние наработки и экспортировал его в форматах docx и pdf. Получилось неплохо.

Затем я попросил друга сконвертировать docx в epub с помощью приложения Pages. И, наконец, самое сложное — fb2. Я потратил несколько часов, перебирая конвертеры, и в итоге получил более-менее приемлемый результат. Однако в моём списке задач всё ещё стоит пункт «разобраться до конца с fb2».

Получившиеся файлы — docx, pdf, epub и fb2 — выложил на хостинг, на котором лежит «сайт нормального фрилансера». За хостинг я плачу около 200 рублей в месяц, на нём, в том числе, уже больше десяти лет живёт сайт Проектората, моего бренда по проектированию интерфейсов. Помимо этого необходимо оплачивать покупку и продление доменного имени и ssl-сертификатов. Суммарно в пределах 1 500 рублей в год.

После того как у меня появились прямые ссылки на скачивание книги, я отправился в сервис JustClick (Джастклик). Он позволяет принимать платежи за электронные товары. Я давно являюсь их клиентом, раньше активно продавал курсы по проектированию и работе в Axure. Сервис этот никому не рекомендую. В первую очередь из-за скорости вывода средств (в среднем я каждый раз жду около месяца после запроса). Но если у меня уже есть там аккаунт и договор с моим ИП, то почему бы и нет? Самый дешёвый тариф — 990 рублей в месяц, именно на нём я и сижу. В рамках этого тарифа дополнительно приходится платить 5% с каждой продажи. Оставшаяся сумма по запросу (через месяц ожидания) уходит на мой счёт ИП, где я плачу ещё 6% в виде налогов на доход.

В Джастклике я сформировал товар (электронную книгу), получил ссылку, с помощью которой можно принимать за него деньги, и настроил, что происходит после оплаты. А после оплаты покупателю приходит электронное письмо с благодарностями и прямыми ссылками на скачивание файлов с книгой в разных форматах. Из персональных данных клиентов я собираю только их имена и емейлы.

Чтобы хоть кто-то увидел ссылку на оплату, я разместил её на страницах «сайта нормального фрилансера». Добавил отдельный раздел «Купить и скачать» и сослался на него в основном меню навигации, а также в конце каждой главы, опубликованной на сайте. Сам сайт я сделал неспешно за неделю в середине осени 2022 на бесплатной платформе Wordpress. Никаких своих программистских или верстальщических навыков не применял, хотя и собирался. Особенно внимательно отнёсся к версии для мобильных устройств, чтобы книгу удобно было читать и так, и так.

Трафик на сайт я веду с помощью контент-маркетинга: публикую статьи о книге на популярных площадках, появляюсь тут и там в тематических сообществах, активно занимаюсь телеграм-каналом о жизни на фрилансе. Потихоньку разбираюсь с тем, как писать в блог с учётом поисковых запросов потенциальной аудитории, чтобы привлечь поисковый трафик.

Также я создал лендинг для книги, но не занимался его продвижением, т.к. там ещё нет отзывов, обложки и прочих дополнительных штук, побуждающих к покупке.

На момент написания этой статьи мне удалось продать 24 электронных книги. Цена не менялась с начала продаж 27 декабря 2022 года: 499 рублей за штуку.

Скриншот из личного кабинета Джастклика. Как видите — желающих оплатить почти в два раза больше оплативших.

За этот же период на сайте нормального фрилансера побывало 2 900 уникальных посетителей. Обычно люди заходят, читают одну-две главы и возвращаются позже. Страницу покупки посетили 732 раза, конверсия в продажу оставляет желать лучшего. Впрочем, я думаю, что это, в первую очередь, из-за цены. 499 рублей за электронную книгу — это выше среднего по рынку. Плюс зачем платить, если можно удобно прочитать бесплатно?

Скриншот из Яндекс.метрики сайта нормального фрилансера

Проблемы, с которыми столкнулся

Как я уже говорил, пришлось повозиться с fb2. До сих пор разбираюсь с тем, как сделать так, чтобы заработало оглавление, не было слипшихся слов и всё было красиво отформатировано.

Через неделю после старта продаж получил письмо от одного из покупателей с сообщением: «Книгу оплатил, а до сих пор не получил». Почта была указана на gmail. Я попросил человека проверить папку «Спам» и, разумеется, письмо было именно там. В тот же день я добавил на страницу продажи инструкцию со следующими тезисами:

После оплаты письмо с книгой должно приходить мгновенно;

Проверяйте папку «Спам»;

Если в течение часа везде пусто, пишите мне.

По итогу проблемы возникали только у пользователей гуглопочты и мэйл.ру. Последние вообще не получали писем, поэтому теперь, если я вижу оплаченный счёт, то сразу проверяю емейл пользователя. И если там мэйл.ру, то отправляю ему письмо вручную, не дожидаясь обращения.

Также я создал страницу «Спасибо». На неё покупатели попадают сразу после оплаты. На странице продублировал благодарности и ссылки на скачивание.

Ссылки ничем не защищены. Любой, кто купил у меня книгу, теоретически может поделиться ими с друзьями. С одной стороны, это делает меня не очень грамотным продавцом книг, а, с другой, я рассуждаю так: книга хороша (если судить по отзывам и благодарностям) и она и так рано или поздно окажется в открытом доступе. И пусть читателям будет удобнее делиться ссылкой на первоисточник, чем на пиратов.

Ссылки будут жить, пока живёт «сайт нормального фрилансера». Я регулярно обновляю файлы, публикуя самые свежие и полные версии. Те, кто приобрёл у меня версию 2022 года, легко смогут прочитать более полное издание с новыми главами, скачав файлы по тем же ссылкам.

Ещё одна проблема: книгу не оплатить вне России. До сих пор думаю, как обойти это ограничение, не нарушая никаких законов. Но не сильно беспокоюсь, помня о том, что её очень удобно читать онлайн бесплатно.

——

У меня не очень амбициозная цель на 2023: продать 200 электронных книг. Такой суммы хватит ровно на то, чтобы оплатить тариф Джастклика за год, обслуживание моего ИП счёта в банке (Альфа дерёт 2 000 рублей в месяц), а также покрыть пенсионные и страховые взносы. То есть, если бы я больше ничем не занимался, то только после этой цифры я начал бы зарабатывать что-то для себя, а не для структур, обслуживающих всю эту цепочку.

А теперь ссылки:

Сайт нормального фрилансера . Здесь можно бесплатно прочитать Книгу нормального фрилансера или поддержать автора покупкой;

Канал нормального фрилансера в телеграме . Пишу много и по делу. Если активно занимаетесь работой на себя — будет полезно. Кстати, там можно послушать аудиоверсию книги. Планирую подготовить отдельную статью о том, как я её создавал.

Готов ответить на ваши вопросы. Увидимся в комментариях!"'https://habr.com/share/publication/720028/1733e5e0e1caa29c7e62650b2f915157/'"['https://habrastorage.org/r/w32/getpro/habr/avatars/7c0/656/7dc/7c06567dca412fa9298a8ff777e5b746.png', 'https://mc.yandex.ru/watch/24049213', 'https://habr.com/share/publication/720028/1733e5e0e1caa29c7e62650b2f915157/', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/cc1/cbf/52f/cc1cbf52fffd8ce271bd7ea343d015f5.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/398/115/13b/39811513b0b998b994d196e3545bdce9.png', 'https://habrastorage.org/getpro/habr/avatars/7c0/656/7dc/7c06567dca412fa9298a8ff777e5b746.png']"
2'720026'Правила выживания начинающего программиста или как работает эффект Данинга-Крюгера'Эффект Данинга-Крюгера - это такое когнитивное искажение, которое выражает зависимость уверенности в предмете от компетентности в нём. Компетентность практически равна времени, потраченного на...'https://habr.com/ru/post/720026/'"Эффект Данинга-Крюгера - это такое когнитивное искажение, которое выражает зависимость уверенности в предмете от компетентности в нём. Компетентность практически равна времени, потраченного на обучение.

Представим, что у нас есть сферический конь Вася, который решил изучить китайский язык. Он открыл ютуб, посмотрел пару роликов. И всё! Ему уже кажется, что чуть-чуть и можно ехать в солнечный Китай, общаться с прекрасными китайцами. Но чем дальше он учит, тем сложнее он ему кажется - эти бесконечные иероглифы, тональность, нюансы культуры и так далее. В какой-то момент он решит бросить эту затею. Но если нам повезло и Вася не сдался, то постепенно он становится экспертом в Китайском. Но никогда он не станет нэйтивом, никогда он не поймет до конца эту тонкую китайскую душу.

Тут есть два нюанса. Во-первых, расширяя зону компетентности мы видим больше из того, что еще надо изучить. Во-вторых, поначалу мы едем на рельсах предыдущего опыта. Наш мозг, сталкиваясь с чем-то новым, выхватывает самое простое и знакомое. Процесс обучения идёт стремительно - пока еще ничто не успело забыться и не страшит множество информации - безумие и отвага. Здесь довольно высокий уровень абстракции и многое воспринимается на веру. Это период горячего фанатизма, когда есть только белое и чёрное. Помните, как во времена коронавируса все стали экспертами-вирусологами, а потом политиками?

Но со временем, когда человек начинает копать глубже, вопросов становится всё больше. Прогресс становится не такой заметный, старые знания начинают забываться, и мозгу уже не достаточно новой порции знаний для выработки такого же количества эндорфина, как в начале.

При этом в действительности ты двигаешься с той же скоростью, что и в начале, но по ощущениям кажется, что стоишь на месте.

Зависимость самооценки 🤓 от реальной компетенции ~ часов обучения ⏳ можно описать таким графиком

При чём здесь ты?

Всем очевидно, мы не можем усвоить большой объём информации за раз. Поглотить то можем, но запомнить - нет. Да и вредно так - мозг можно и надорвать, как мышцу. Первый мой совет - учиться по чуть-чуть, но регулярно: освоил одну тему - отдохнул, закрепил материал - снова отдохнул, повторил, что прошёл, взял с полки пирожок, и двинулся дальше. Два шага вперед, один назад.

Я буду приводить примеры из мира фронтенда, но мой читатель может подставить что-то своё, ведь это универсальная история и подходит ко всему, что угодно - бэкенд, дизайн, тестирование, да даже лепка из пластилина.

Медовый месяц 🍯

И вот, ты решил стать программистом.

Канули в лету те времена, когда нужно было выискивать всё по крупицам на просторах необъятной метавселенной. Информации стало так много, что учись 100 лет, да ещё и останется. Курсов - как грибов после дождя… Какой выбрать - не понятно... А вдруг, в той школе учат лучше и быстрее?!

Но не будем на этом останавливаться. Это уже другой разговор.

Рано или поздно ты находишь курс, где тебе с розовыми пони объясняют элементарные вещи. Всё кажется проще простого. Тебя ведут за ручку по широкой проторенной дорожке. До этого ты ничего не знал и не умел, а тут раз, и запускаешь сайт с многообещающей надписью “Hello world!”, этими вот, собственными, руками написанный! И кажется вот-вот, и ты - настоящий программист.

Я тучка-тучка-тучка, я вовсе не медведь.

Поздравляю, ты находишься на первом этапе покорения вершины - “Медовый месяц”.

Это очень важный этап. Чем больше в это время ты посвятишь кодингу и не будешь отвлекаться на другие, безусловно, очень полезные вещи - дота, ютуб, дверца холодильника, курсы игры на балалайке, - тем тебе будет легче… но потом.

Сейчас из тебя хлещет энтузиазм и энергия, которые позволяют часами сидеть за компом, читать статьи и не засыпать от скуки. Глаза горят. Ты грезишь себя крутым программистом, который сидит под зонтиком на пляже и зарабатывает 300к в наносекунду.

Правила выживания на этом этапе

⛳ Верность курсу . Попробуй несколько, но выбери только один - и следуй ему до конца

📅 Распорядок дня . Тут тебе может помочь Pomidoro , который не даст перегореть и испортить здоровье

🚀 Ведение задач . Тасктрекер поможет следить за своим прогрессом и концентрироваться на задаче. Подойдёт trello , notion , jira

📒 Записная книжка. Конспектируй всё, что кажется полезным. Знания очень быстро забываются, а записи помогут быстрее вспомнить пройденный материал

😴 Здоровый сон . Доказано, что именно он помогает усваиваться знаниям

🥕 Здоровая еда . Желудок - второй мозг

💪 Физические нагрузки. Не забывай про тело, оно должно тоже получать свои нагрузки

Длится

От 1 до 3 месяцев

В школе я очень хотел научиться играть на гитаре. Мне казалось, что нужно сразу купить крутой инструмент, но сестра сказала: “Учись пока на этой “деревяшке”, и если не бросишь через три месяца, то покупай”. Я бросил… Но потом всё равно выучился, сильно позже, в студенческие годы

Скала смятения 🗿

Неизбежно ты перейдёшь ко второй фазе “Скала смятения”

Когда ты прошёл свой первый вводный курс, а может и не первый. У тебя появляются задачи посложнее. Некоторые темы ты не до конца понял, оставив на потом - не страшно. Ты уже сам гуглишь вопросы и начал пользоваться stackoverflow . Но каждый следующий шаг даётся всё сложнее и сложнее. ""Опять эти непонятные ошибки! 🤬"", ""И почему код не компилится!? 🤯"" Ты в смятении. Ты не знаешь, куда двигаться дальше. Подумываешь уже перейти в дизайн, тестирование или ещё куда-нибудь… Вроде много делаешь, но рост не такой очевидный, как раньше.

Разбежавшись, прыгну со скалы...

Именно здесь очень полезно иметь наставника, который умеет мотивировать и наводит на правильное решение, не давая завязнуть в отладке. Тут у тебя даже могут возникнуть мысли: “Это не моё. Вон, у других то всё сразу получается”. Но это заблуждение. Держись! Самое страшное впереди! 😁

Правила выживания на этом этапе

🥸 Найди ментора . Если есть друг, то не стесняйся его использовать по полной!

🎯 Мотивация . Не забывай, ради чего ты это делаешь - что тебя драйвит: челленджи, люди, деньги, самореализация, сам кодинг? Потому что именно сейчас, как никогда, хочется свинтить, но нужно продолжать бить в одну цель

🐸 Съешь жабу . Сперва делай самую неприятную работу

🥾 Путь в тысячу вёрст начинается с первого шага . Легче взяться за маленькую задачи, а дльше, как пойдёт. Эта уловка помогает сделать очень много.

🐈 Котики и шоколадки. Не всегда кодинг доставляет удовольствие, даже у ярых гиков. Поэтому должна быть ещё какая-нибудь радость.

На этой стадии отваливается максимальное количество людей. Одни возвращаются к прошлой работе, другие уходят в смежные направления, проходя заново те же этапы. Но мы двигаемся дальше!

Длится

От 3 до 6 месяцев.

Пустыня отчаяния 🐫

С большим трудом, может не идеально, но ты прошёл большой курс по программированию. Выучил JS, HTML, CSS и ещё какой-нибудь фреймворк. Тебе хочется попробовать свои силы в деле. Но у тебя нет чёткого плана и ты ходишь кругами. Вспоминаешь, как легко, на самом деле, было во времена курса. Там все помогали, а теперь ты сам по себе. Ты готовишься к собеседованию, учишь теорию, пишешь тестовые задания, пет проекты. Но, бывает, тратишь целый день на решение какой-то дурацкой проблемы, которая в итоге решается одной строкой. Порой ты заходишь в тупик и делаешь лишнюю работу. Становится сложно заставить себя сесть за комп и не листать при этом ленту новостей. Прокрастинация - твой верный спутник

Даже котики не спасают

Поздравляю, мой друг, ты на этапе “Пустыня отчаяния”.

Правила выживания на этом этапе

💅 Рабочее место . Создай вокруг себя комфортные условия. Хороший стул, стол. Можно поэкспериментировать с коленным стулом или стоячим рабочим местом

🎳 Разделяй и властвуй . Разбей цель на понятные подцели. Понимание прогресса придаст тебе сил

🌻 Утро вечера мудренее . Давай себе отдых, если зашёл в тупик. На свежую голову множество проблем решаются проще

👘 Комьюнити . Найди таких же ребят, как и ты, или вступи в сообщество. Они помогут быстрее разобраться с проблемой и справиться с прокрастинацией. Можно смотреть истории успеха, чтобы поверить в себя

🪒 Бритва Оккама. Отбрось всё лишнее и сосредоточься на главном.

Длится

От 6 месяцев до года

Пацан к успеху шёл 🦄

Постепенно ты снова начинаешь верить в свои силы. У тебя уже есть несколько проектов, доведённых до конца. Ну да, спагетти код, зато работает! Появляется уверенность в знаниях. Чувствуешь, что ещё чуть-чуть и получишь первый оффер.

Если хочешь достигнуть цели - беги к ней, если не можешь бежать - иди, если не можешь идти - ползи, если не можешь ползти - ляг в её направлении

Правила выживания на этом этапе:

📚 Систематизация. Выпишите кратко основные темы, правила, паттерны проектирования, стайлгайды, чтобы легче было им следовать и улучшать со временем

🗝️ Воруйте . Берите лучшие практики у лучших

🧠 Повторение . Заполняйте пробелы в знаниях, они есть у всех

⏳ Выжидайте . Не спешите принимать первый попавшийся оффер. Выбирайте ту компанию, в которой будешь быстрее расти, где есть крутые разработчики и хорошая культура онбоардинга

❌ Не фрилансь. Даже если фриланс идеально подходит под твою натуру, попробуй сперва поработать в команде, это даст неоценимый опыт в будущем

Длится

До тех пор, пока не получишь оффер, но это не точно.

У самурая нет цели, есть только путь 🗻

Нужно помнить, что программирование - это образ жизни, и тебе нужно выстроить этот образ под себя, причём выстраивать постоянно, ища максимально удобные лично тебе техники и лайфхаки, окружать себя людьми, которые так или иначе будут поддерживать; искать удобный стул, монитор, клавиатуру; выстраивать распорядок дня, не забывая жить полной жизнью - строить отношения и посвящать время своим увлечениям (походы в горы, йога, танцы). Ответь себе на вопрос: “Зачем я это делаю?”

""Хватит прикрываться оправданиями. Ты единственный, кто решает, как тебе жить"" (Самурай Чамплу)

Заключение

На пути тебя ждёт очень много разочарований. Ты несколько раз захочешь всё бросить или сменить направление. Это нормально. До конца дойдут не все. Но если ты будешь соблюдать гигиену ума, то шансы значительно вырастают!

😴 Сон . Усвоение знаний происходит только во время сна или отдыха.

🍙 Еда . Правильно питайтесь, чтобы тело не отвлекало вас от учёбы. Не экономьте на еде - ведь, скоро ты будешь получать большие деньги, а экономия на еде может помешать тебе это сделать

🍅 Отдых . Глазам и телу нужно отдыхать. Да и часто после отдыха задача решается в разы легче. Иногда полезно переспать с проблемой.

🚀 Тасктрекер . Так вы будете видеть свой прогресс и это пригодится, когда вы устроитесь на работу

⚖️ Баланс. Помимо умственных занятий должны быть и физические. Это может быть не только спорт, но и йога, танцы, воркаут. Подставка под голову тоже должна работать, иначе она о себе напомнит не самым приятным способом

""Видя бой со стороны, каждый мнит себя стратегом"" (Козьма Прутков)

Когда устроишься на работу - можешь выдохнуть и выпить кокос, но это не обязательно. Теперь тебе предстоит бороться с синдромом самозванца и выгоранием, а это уже другая история…"'https://habrastorage.org/getpro/habr/upload_files/e60/588/85c/e6058885c3bfd9d8b797e8a0a6c8894f.jpg'"['https://habrastorage.org/r/w780q1/getpro/habr/upload_files/958/2cc/e16/9582cce16486ca4b4159a1f4477dc408.jpg', 'https://habrastorage.org/getpro/habr/avatars/22e/73c/9b2/22e73c9b2d87b0eeee406c8772412149.png', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/e60/588/85c/e6058885c3bfd9d8b797e8a0a6c8894f.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/951/2fe/3b8/9512fe3b80dd05745bd71f663eeca80f.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/e14/eb2/785/e14eb278526e93ab789dfa7e4c455a80.jpg', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w32/getpro/habr/avatars/22e/73c/9b2/22e73c9b2d87b0eeee406c8772412149.png', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/f59/77a/118/f5977a1185c44b10d731ff0960f704e1.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/9db/bc6/c50/9dbbc6c50583b89b2fe05129feae7ae7.jpg', 'https://habrastorage.org/getpro/habr/upload_files/e60/588/85c/e6058885c3bfd9d8b797e8a0a6c8894f.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/d57/789/20c/d5778920c65254bbe9e2148ae5056580.jpg']"
3'720024'Нехимическая зависимость: простое определение, механизм и диверсификация рисков'"Что вы себе представляете, когда слышите слова ""зависимость"" и ""аддикция""? Зачастую это страшные картинки наркозависимых, образ шприца и перетянутой вены, в общем, тот еще ""Реквием по мечте"". У..."'https://habr.com/ru/post/720024/'"Что вы себе представляете, когда слышите слова ""зависимость"" и ""аддикция""? Зачастую это страшные картинки наркозависимых, образ шприца и перетянутой вены, в общем, тот еще ""Реквием по мечте"". У кого-то первым на ум придет алкоголь, бутылка, разруха в квартире, некормленые дети и прочий социальный кризис. Если идти по-лайту, то это сигареты, желтые зубы, задымленные помещения и рак легких.

И эти жуткие образы затмевают гораздо более распространенный вариант нехимических (поведенческих) аддикций, в которые входят куда более популярные зависимости, вроде: интернет-аддикции, гэмблинга (тяги к азартным играм), трудоголизм и другие. Хотя их совокупный вред не меньше приходящих на ум первыми ""страшилок"".

Как это работает, почему от них сложно отказаться и как снизить риски попадания? Попробуем разобраться.

Дисклеймер: В силу подхода к работе так сложилось, что обычно я консультирую людей интеллектуального труда. В последний год это преимущественно управленцы и сотрудники IT‑сферы. Это люди, которые всю жизнь решали любые проблемы «через голову». Такой подход неизбежно накладывает отпечаток на личность и внепрофессиональную жизнь. Минимизацией негативных последствий таких особенностей я занимаюсь. И в рамках статей на данном ресурсе стараюсь обобщить профессиональный опыт и дать полезную информацию.

Если брать строгое определение нехимической аддикции, то мы увидим примерно следующее:

Поведенческая зависимость (нехимическая зависимость) — аддикция, при которой объектом зависимости становится какой-либо поведенческий паттерн, а не психоактивное вещество.

И тут как в анекдоте: ""ответ совершенно точный и абсолютно бесполезный"" Как-то раз Шерлок Холмс и доктор Ватсон позволили себе лишнего. По-русски говоря, наклюкались до беспамятства. И вот обнаруживают они себя с утра в корзине воздушного шара, который парит в небе. Внизу они видят поле. На поле пасётся стадо овец. За стадом присматривает пастух. — Сэр! — кричит Ватсон, сложив руки рупором. — Да-да, сэр, это я вас зову! Будьте любезны, сэр, скажите, где мы находимся? Пастух поднял голову и ответил: — В корзине воздушного шара, который в данный момент времени находится в состоянии полёта, сэр. Обескураженный Ватсон смотрит на Холмса, а тот меланхолично произносит: — Странная история. Почему программист пасёт овец? Ватсон удивляется ещё больше. — Холмс, но с чего вы взяли, что он программист? — Элементарно, Ватсон. Только программист мог дать такой абсолютно точный и при этом совершенно бесполезный ответ.

Поэтому мы зайдем немного с другой стороны и за основу для данной статьи возьмем упрощенное определение:

Зависимость - это возможность достигнуть определенного состояния только одним способом

Здесь стоит сделать поправку, что это определение сферической поведенческой зависимости в вакууме. То есть, если вы можете испытать радость только наркотиками и алкоголем, то нельзя говорить про отсутствие зависимости. Но можно предположить, что вам легче будет достичь радости, чем при только наркотической зависимости.

Как это работает?

Я не зря сказал про радость. Большинство зависимостей имеют атарактически-гедонистический механизм действия. Если говорить просто (как нам и нужно), то это выглядит так:

Атарактический механизм (мотив) - когда вам плохо, вы что-то делаете, и вам становится просто нормально. Гедонистический механизм (мотив) - когда вам нормально, вы что-то делаете, и вам становится хорошо. Атарактически-гедонистический механизм - когда вам плохо, вы что-то можете сделать, и вам станет хорошо.

Визуально это отобразить можно следующим образом:

Почему это важно? Из этих мотивов мы можем выделить две основные направленности:

Что я делаю, чтобы мне перестало быть плохо?

Что я делаю, чтобы мне стало хорошо?

Задайте себе эти вопросы. Сколько у вас будет ответов на каждый? Если вы сходу, не задумываясь, дадите менее пяти ответов, а то вообще зависните в прострации, то можно говорить о наличие некоего риска.

Почему важно говорить про ответ ""сходу""? Потому что есть такой эффект как тоннельное мышление:

Тоннельнное мышление (психология) - В психологии этим понятием обозначают сконцентрированность человека на какой-либо одной идее, ощущении, воспоминании. Мышление становится ограниченным, категоричным, возникает навязчивость. Как понятие возникло по аналогии с «тоннельным зрением» в медицине. Офтальмологи применяют его для обозначения нарушений, которые вызывают ограниченность или утрату периферического обзора.

Говоря иначе, когда вам плохо/скучно/тревожно, вы редко начинаете придумывать что-то новое, генерировать идеи и искать неожиданные пути. Нет, в такие моменты чаще всего вы используете уже наработанные и зарекомендовавшие себя модели поведения. Если вы слышали фразу ""В критической ситуации ты не поднимешься до уровня своих ожиданий, а упадёшь до уровня своей подготовки"", то это именно оно.

Теперь давайте объединим это с другим феноменом - закреплением успешной поведенческой модели. Об этом еще писал Б. Скиннер (категорически интересный бездушный человек). Его суть заключается в том, что поведение, которое дает успешный ожидаемый результат на регулярной основе, закрепляется и назначается ""поведением по умолчанию"".

А дальше механизм очень прост:

Человек тестирует различные поведенческие модели для решения какой-либо проблемы;

Постепенно определяется наиболее успешная модель по соотношению затраты/результат;

Модель закрепляется, совершенствуется и становится основной;

Неэффективные модели забываются/отваливаются;

При сбое основной модели человеку трудно обратиться к другим моделям, так как навыки утрачены и результат еще хуже, чем был на первом этапе.

В контексте нашей темы, это выглядит так, что у человека формируется определенный способ снимать стресс/повышать настроение. Фактически, есть некое поведение, которое дает нужные стимулы. И всё логично, казалось бы. Всё выглядит хорошо, пока мы не учитываем последнюю переменную в нашей теме - привыкание:

Привыкание (габитуáция) — это постепенное уменьшение ответной реакции как результат продолжающейся или повторяющейся стимуляции в нормальных условиях Следует отличать привыкание от процессов мышечного утомления, а также от сенсорной адаптации. Определяющим привыкание параметрами является частота и интенсивность стимула. В то же время привыкание происходит при условии соразмерности частоты поведению.

Когда возникает ощущение привыкания (необходимости усиления стимула для получения ожидаемого результата), то соотношение затрат/результата начинает изменяться. Но других моделей поведения уже нет. Человек начинает ""выжимать"" занятие до предела, получая все больше неприятных побочных эффектов и рисков, но не получая ожидаемого легкого результата. Это рождает неудовлетворенность. Которую хочется снять. А способ не работает. Человек прикладывает еще больше усилий, но неудовлетворенность всё растет. Которую хочется снять... и т.д. и т.п.

Всё замечательно, а что с этим делать?

Поняв логику механизма, можно задуматься о его применении. И хотя логичнее было бы говорить отдельно про работу с атарактической мотивацией и отдельно про работу с гедонистической мотивацией, это привело бы к плавному переходу от статьи к монографии (которые я читал, пока обучался на магистратуре по зависимостям). Но мне хочется дать выжимку, поэтому рекомендации будут общие на оба случая, но максимально конкретные:

1) Отслеживать КПД каждого метода, а также динамику его изменения. Это будет вашей сигнальной системой, которая предупредит о том, что вы попадаете в рискованное положение. Это можно сделать при помощи таблицы, про которую я уже ранее писал в этой статье. Можете скачать готовую форму с моего ЯндексДиска.

2) Сознательно снижать КПД лидирующей модели. Если вы чувствуете, что у вас формируется некий поведенческий паттерн, вытесняющий другие, то снизьте его привлекательность для себя за счет повышения трудозатрат. Например, если вы импульсивно привыкли заказывать еду, то удалите с телефона все приложения доставки. Вам это не помешает при желании организовать доставку, но необходимость установить-авторизоваться снизит риск привычной стратегии;

3) Повышать КПД исчезающих моделей. Обратная история - снижать затраты. Например, вы привыкли снимать стресс компьютерными играми, а книги забросили (как в нашем примере). В этом случае, заранее (пока вы в состоянии стояния) затарьтесь книгами интересными, разложите их по дому. Чтобы они всегда были под рукой и вам не пришлось их искать в моменты стресса и скуки;

4) Не ""выжимать"" какое-то увлечение. Даже если вы с головой ушли в какое-либо занятие, нужно отдавать себе отчет, что это очень временное решение. Поэтому, даже если в моменте вам кажется, что это помогает, сразу же ищите себе другое занятие. По идее, у вас должно быть не менее трех-четырех хобби/занятий для того, чтобы минимизировать риски;

5) Заранее подготовить кнопку ""разбить в случае пожара"". Так как от тоннельного мышления никто не застрахован, то некое эмоциональное НЗ - план на 3-5 часов, прописанный полностью, вместе со ссылками на материал (игры/фильмы/др.), заказываемую еду и т.д. Это нужно, так как в моменты стресса, усталости и раздражения человек не разрабатывает новые модели поведения, а использует имеющиеся. Следовательно, соломку лучше подстелить заранее;

6) Расширить социальный круг увлеченных людей. Зачастую нам бывает сложно заставить себя попробовать что-то новое. Для упрощения входа (читай, изменения КПД новых занятий) - найдите сообщества людей, которые чем-то увлечены и создайте себе возможность присоединиться к ним на каком-либо мероприятии;

7) Сохраняйте и повышайте адаптивные способности своей психики. Пробуйте что-то новое даже в мелочах. Иной бизнес-ланч, ищите новую дорогу на работу, меняйте стиль одежды, ходите в непривычные заведения, слушайте музыку непривычных жанров. Целенаправленно иногда ""выбивайте себя из колеи"". В этом случае ваша готовность к изменениям будет выше.

В заключение

Наша жизнь - это постоянный менеджмент ресурсов, чувств и мыслей. Обмен, конвертация, поиск. И если мы пускаем этот процесс на самотек, то рано или поздно окажемся в ловушке, в которую нас завел собственный мозг. Несмотря на то, что именно ему мы благодарны за все достижения, он тоже не идеален и слепо доверять ему не стоит. Поставьте его решения под сомнение. А это можно сделать с помощью вышеописанных методов. Даже если они иногда кажутся контринтуитивными.

С уважением

Сергей Максимов.

Психолог.

P.S. Если вдруг интересны другие статьи по психологии, они есть в моем ТГ-канале. Только статьи, без мемов и спама. https://t.me/maximov_psy"'https://habrastorage.org/getpro/habr/upload_files/947/496/b3e/947496b3e80fb0f8170ff268f1790905.png'"['https://habrastorage.org/r/w1560/getpro/habr/post_images/a69/133/35b/a6913335bb96a6ff1a38293970b0fdd3.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/2ec/104/bb0/2ec104bb04ea28faedf45214dc5ce3c0.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/avatars/f50/203/a89/f50203a89ec1ecdb547f0dda98de4ce9.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/a3e/7ca/7fc/a3e7ca7fc9dea2ad5c04155cdef13f73.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/8d3/df5/e24/8d3df5e243444265eadd2871c95f0d7e.png', 'https://habrastorage.org/r/w32/getpro/habr/avatars/f50/203/a89/f50203a89ec1ecdb547f0dda98de4ce9.jpg', 'https://habrastorage.org/getpro/habr/upload_files/947/496/b3e/947496b3e80fb0f8170ff268f1790905.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/947/496/b3e/947496b3e80fb0f8170ff268f1790905.png']"
4'719860'KMongo и неструктурированные данные'Несмотря на то, что MongoDB начало движение в сторону строгости реляционной модели (добавление опциональной схемы данных, join-запросов, агрегаций и т.п.), она по прежнему остается документной базой...'https://habr.com/ru/post/719860/'"Несмотря на то, что MongoDB начало движение в сторону строгости реляционной модели (добавление опциональной схемы данных, join-запросов, агрегаций и т.п.), она по прежнему остается документной базой данных и предполагает возможность сохранения документов произвольной структуры. И при использовании MongoDB в языках с динамической типизацией (JavaScript, Python) сохранение или генерация объектов не вызывает сложностей, поскольку заранее не требуется определить структуру извлекаемого или сохраняемого объекта. Но как действовать в случае использования драйверов для MongoDB для языков со строгой типизацией — например, библиотеки KMongo для Kotlin?

В этой статье мы разберем приемы для работы с неструктурированными данными, которые позволят сохранить преимущества использования сериализации (Jackson или kotlinx.serializable) с механизмами рефлексии для извлечения произвольных документов.

Для начала несколько слов о самой библиотеке KMongo. Open Source библиотека основана на Core MongoDB Java Driver и предоставляет удобный синтаксис для выполнения запросов и итерации по базам данных, коллекциям и результатам запроса с использованием возможностей Kotlin (в частности используется определение инфиксных операторов для создания запросов вида Jedi::age lt yoda?.age ), а также обеспечивает поддержку неявной сериализации при чтении документов из Mongo и сохранения новых документов. Также KMongo предоставляет возможность регистрации адаптеров для использования механизмов неблокирующего выполнения операций (с использованием Rx или корутин).

Для установки KMongo необходимо добавить основную библиотеку и адаптер для неблокирующего выполнения в build.gradle, например для поддержки корутин и kotlinx.serialization:

dependencies { implementation(""org.litote.kmongo:kmongo-serialization:4.8.0"") implementation(""org.litote.kmongo:kmongo-coroutine-serialization:4.8.0"") }

Для создания подключения к MongoDB нужно обратиться к функции createClient клиента KMongo. При установке поддержки неблокирующего исполнения от результата можно обратиться к get-методу coroutine для получения асинхронного варианта подключения, например так:

val client = KMongo.createClient(connectionString = ""mongodb://mongoip:27017"").coroutine

Полученный объект CoroutineClient позволяет просмотреть список доступных баз данных ( listDatabases ), подключиться к базе данных getDatabase(name) , а также подписаться на поток изменений ( watch ). Например, для подключения к базе данных test можно использовать следующий вызов:

val database = client.getDatabase(""test"")

Если клиент был получен как CoroutineClient , то и база данных также будет доступна через обертку CoroutineDatabase . Далее для подключенной базы данных можно выполнять запросы и здесь мы впервые встретимся с необходимостью работы со структурированными объектами. Структура может определяться как любой data-класс и используется при выполнении запросов, а также для определения условий выбора. Например, для выбора всех сообщений со значением числового поля type==1 можно выполнить следующий фрагмент кода:

import org.litote.kmongo.* import kotlinx.serialization.* @Serializable data class TestItem(val type:Int, val message:String) suspend fun getItems(database:CoroutineDatabase):List<TestItem> { val collection = database.getCollection<TestItem>(""items"") return collection.find(TestItem::type eq 1) }

Аналогично могут быть добавлены новые записи в коллекцию:

collection.insertOne(TestItem(1, ""Test""))

Но сложности начинаются при необходимости добавления или извлечения данных со сложными типами данных или произвольной структуры. Для первого сценария есть возможность создавать кодеки, которые смогут интерпретировать произвольные (в том числе, двоичные) типы данных в Mongo в объекты Kotlin. Даты поддерживаются изначально, для остальных можно зарегистрировать кодек через ObjectMappingConfiguration.addCustomCodec для соответствующего типа результатами. А вот со вторым случаем возникает существенная сложность из-за отсутствия механизма сериализации для динамических структур. Предположим, что в коллекции items будет также храниться дополнительное поле meta, в котором будет размещаться map со строковыми ключами и произвольными типами в значении. Казалось бы решением могло бы быть следующее определение:

data class TestItem(val type:Int, val message:String, val meta:Map<String,An

Но сложность тут будет в том, что динамические типы данных не поддерживаются при декодировании ответа Mongo. Как же действовать в такой ситуации? Здесь мы можем использовать объект класса BsonDocument , который позволяет работать с отдельными именованными полями в Mongo-объекте, в том числе, получать список всех существующих полей в документе и определять их тип. BsonDocument может использоваться как при выполнении запроса, так и внутри сериализуемого объекта. Попробуем извлечь структуру данных с неструктурированным полем:

suspend fun getData(db:CoroutineDatabaes):List<TestItem> { val items = mutableListOf<TestItem>() val collection = db.getCollection<BsonDocument>(""items"") collection.consumeEach { item -> val metaData = mutableMapOf<String,Any>() val type = item.getInt32(""type"").value val message = item.getString(""message"").value val meta = item[""meta""]?.asDocument() meta?.keys?.forEach { key -> metaData[key] = meta.getValue(key).value } items.add(TestItem(type, message, metaData)) } return items }

Здесь мы используем возможности итерации по списку ключей BsonDocument и общий интерфейс для получения значений для любого поля (также может быть возвращен BsonDocument и, в этом случае, можно обработать его рекурсивно). Аналогично может быть сохранен объект с произвольной структурой:

suspend fun addData(db:CoroutineDatabase, data:TestItem) { val meta = BsonDocument() data.meta.keys.forEach { key -> val value = data.meta[key] if (value is String) { meta[key] = BsonString(value)) } else if (value is Boolean) { meta[key] = BsonBoolean(value)) } else if (value is Double) { meta[it] = BsonDouble(value) } else if (value is Int) { meta[it] = BsonInt64(value) } } val doc = BsonDocument( listOf( BsonElement(""type"", BsonInt32(data.type)), BsonElement(""message"", BsonString(data.message)), BsonElement(""meta"", meta), ) ) db.getCollection<BsonDocument>(""items"").insertOne(doc) }

Таким образом, можно сохранять и извлекать документы произвольной структуры из MongoDB в приложениях на Java/Kotlin. Конечно же, есть возможность создания собственных сериализаторов для kotlinx.serialization / Jackson и кодеков для драйвера, но в большинстве случаев прямого доступа к BsonDocument оказывается достаточным для работы со сложными документами.

Завтра в OTUS состоится открытое занятие, которое будет посвящено установке mongo и компаса, также спикер покажет основные операции. Если кому-то интересно — записывайтесь по ссылке."'https://habrastorage.org/getpro/habr/upload_files/163/076/9a3/1630769a30f3f4769d2f0af068091824.png'"['https://habrastorage.org/getpro/habr/upload_files/163/076/9a3/1630769a30f3f4769d2f0af068091824.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/avatars/651/3fb/4ae/6513fb4aec629fee26795ee8f11e9a50.png', 'https://habrastorage.org/r/w32/getpro/habr/avatars/651/3fb/4ae/6513fb4aec629fee26795ee8f11e9a50.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/df5/9d8/fcf/df59d8fcf337f3ad35ecb4a4199b18de.png', 'https://habrastorage.org/getpro/habr/company/2d5/0ed/b57/2d50edb57cf45fa07cc4f39f53b78395.png']"
5'719944'Оказание первой помощи — мифы и страхи'— Тут человеку плохо, помогите! Есть врач? — кричала бы я, если бы попала в ситуацию, когда человеку нужно помочь, а я не знаю, как.  Знакомо? Попадали сами в такую ситуацию? Надеюсь, что нет и...'https://habr.com/ru/post/719944/'"— Тут человеку плохо, помогите! Есть врач? — кричала бы я, если бы попала в ситуацию, когда человеку нужно помочь, а я не знаю, как.

Знакомо? Попадали сами в такую ситуацию? Надеюсь, что нет и что не придётся. Я решила не ждать подобной ситуации и пошла на курсы по оказанию первой помощи.

А в этом небольшом посте я хочу развеять самые популярные мифы, связанные с (не)оказанием помощи, и дать немного советов по теме.

Миф #1 — помощь могут оказывать только медики

Развею, пожалуй, самый устойчивый миф, что оказывать помощь могут только врачи. Если коротко — нет, это не так.

Для начала давайте разграничим два понятия: Первая Помощь и Медицинская Помощь.

Это очень важно, так как эти понятия четко разграничиваются законом 323 « Об основах охраны здоровья граждан в Российской Федерации ». Оказывать первую помощь имеет право любой человек и даже ребенок при наличии у него соответствующей подготовки и навыков.

Итак, медицинской помощью считается применение любых медикаментов, даже на первый взгляд самых безобидных и всем знакомых, например, перекись водорода или нашатырь (про эти лекарства мы поговорим подробнее в следующих постах).

Так вот, применять медикаменты, если вы не врач, вы не имеете права. Вдруг вы пытаетесь дать человеку безобидную для вас таблетку, а у него на этот препарат дичайшая аллергия и вообще ему только хуже станет. В случае травм, таких как вывих и перелом, не нужно пытаться вправить конечности обратно, это тоже задача для медиков. Ваша же задача — обеспечить безопасность для пострадавшего и зафиксировать травмированную конечность.

А вот оказать первую помощь вы имеете право.

По поводу соответствующей подготовки: стоит отметить, что нет никакого регламента или требований к вашим навыкам или подготовке. То есть, если вы фанат Малышевой или Малахова, то вы тоже можете оказать первую помощь⛑ (минутка юмора)

Миф #2 — если я причиню вред пострадавшему, меня посадят

За первую помощь не наказывают. Даже если вы не обладаете достаточными навыками или практикой, вы же хотите помочь.

Самая распространенная травма, которую наносят пострадавшему при сердечно-легочной реанимации — это сломанные ребра. У реаниматологов даже есть своя профессиональная поговорка: «Если при реанимации не сломал ни одного ребра — то плохо делал реанимацию».

Поэтому не бойтесь помогать, если вы действительно этого хотите.

Миф #3 — ОК, но за бездействие-то точно накажут

Это тоже довольно распространенный страх, при котором кажется, что вас накажут, если вы не окажете помощь. Поэтому многие торопятся уйти как можно поскорее и подальше. Законом не предусмотрено наказания за неоказание помощи. Существует наказание для людей спецслужб, которые по регламенту обязаны иметь навык по оказанию помощи (МЧС, полиция и прочие).

Наказание существует за конкретный поступок — если вы оставили пострадавшего в опасности.

Например, водитель, сбивший человека, скрылся с места ДТП. Его больше накажут именно за то, что он оставил пострадавшего в опасности и не предпринял мер по оказанию или вызову помощи. По-хорошему, конечно, водители должны уметь оказывать первую помощь, но в автошколе, к сожалению, кроме беглой теории больше ничего не дают. По крайней мере, так было в моей автошколе.

Или, например, ребенок упал с качелей, а мама в это время пошла в магазин, а не к ребенку — это тоже оставление в опасности. Так вот, если вы оказались в такой ситуации, когда человеку нужна помощь, а вы не знаете, чем помочь, то вызов скорой уже будет считаться оказанием помощи. Вы среагировали, вы донесли до соответствующих служб ситуацию, вы вызвали специалистов. Соответственно, вас не могут наказать. Это, пожалуй, самый тяжелый страх, который сковывает в действительно стрессовой критической ситуации.

Страх быть лидером ситуации или как не послать зевак

Вот вы гуляете по парку и видите, как где-то впереди образовывается толпа зевак. Велосипедист без шлема упал, ударился головой и потерял сознание. И вот появляетесь вы, готовый и знающий, что делать, и приступаете к оказанию первой помощи. Что в этот момент происходит с толпой зевак? Правильно, внезапно оказывается, что все всё знают лучше вас, начинают давать безостановочные советы, перекрикивая друг друга, да и вообще — суматоха, бедлам, хаос.

В этот момент необходимо убрать таких «помощников», для этого нужно занять их делом. Первый пусть ищет точный адрес, второй пусть вызывает скорую, третий пусть найдёт идеально ровную палку, неважно зачем, важно, что эти люди будут чувствовать свою причастность и искренне верить, что они помогают.

Немного напутствий

Знаете, я довольно долго не могла решиться пойти на курсы, ведь в жизни проще идти по пути наименьшего сопротивления. Я честно никогда не оказывалась в критической ситуации, при которой требовалась бы моя помощь. Один раз только на моих глазах упала пожилая женщина и повредила ногу (был холодный зимний вечер), и никто к ней не подходил. Я не смогла пройти мимо, помогла ей подняться, вызвала скорую и так и держала ее на себе, пока скорая не приехала. И, знаете, подошли несколько зевак, но мне никто не помог подержать старушку. А я не здоровенный парень на 180 см, которому не впервой держать старушек на себе. В такие моменты становится и страшно, и ужасно, и злость берет. Злость берет от равнодушия, честное слово, хуже равнодушия нет ничего.

И да, я понимаю, что если бы человеку нужна была бы реальная помощь, а я не знала, что делать, то все описанные страхи точно так же жили во мне. И эти же страхи являются внутренними оправданиями нашему бездействию. Как раз для избавления от них я пошла на курсы.

Оказалось, что они ничем не отличаются от любых других курсов, вы просто получаете новые знания и новый навык. И пусть он лучше вам никогда не пригодится, но вы будете знать, что делать.

PS. Люди, будьте чуточку добрее, помните, что каждый из нас может оказаться в ситуации, когда нам может понадобиться помощь.

Да будет запущен круг добра!"'https://habrastorage.org/getpro/habr/upload_files/352/50c/daa/35250cdaab36d0f35d08cfc2fdc62820.png'"['https://habrastorage.org/r/w780q1/getpro/habr/upload_files/bcf/1a3/bcb/bcf1a3bcb0705f6b8fbdbb86334a851c.jpg', 'https://habrastorage.org/getpro/habr/upload_files/352/50c/daa/35250cdaab36d0f35d08cfc2fdc62820.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/branding/5cc/de7/796/5ccde7796e80bd7e31097b3cfabe3eaf.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/ef6/a8e/ca2/ef6a8eca2695d7f2878937d1d837e7a7.png', 'https://habrastorage.org/getpro/habr/company/729/ece/5ac/729ece5acd5f1172e23ed478ef76d63f.jpg']"
6'720010'[recovery mode] Про Linux на встроенной графике Intel'Я уже довольно долго собираю и настраиваю десктопы с Linux для дома и офиса, и последнее время не без удовольствия выбираю конфигурации со встроенной графикой Intel. Когда-то я начинал с машинки, в...'https://habr.com/ru/post/720010/'"Я уже довольно долго собираю и настраиваю десктопы с Linux для дома и офиса, и последнее время не без удовольствия выбираю конфигурации со встроенной графикой Intel. Когда-то я начинал с машинки, в которую поставил с Core i3-2105, (HD Graphics 3000), позднее — более новый Core i3-9000 (UHD Graphics 630), а совсем недавно мне очень недорого достался Intel NUC5PPYH, разумеется тоже с фирменным графическим контроллером Intel.

Сразу хочу сказать, что если вы не играете в коммерческие игры в Linux, то графические ""встройки"" Intel — это лучший выбор в плане стабильности и поддержки производителя. Видеодрайвер уже много лет есть в ядре, и он просто работает: с аппаратным ускорением из коробки, без тиринга, без дополнительных проприетарных блобов и прочей головной боли. Более того, таких драйверов минимум два: это традиционный 'intel' и более новый 'modesetting'. Графика Intel с самого начала лучше всего работала в Wayland, не будем забывать и об этом.

Поводом для этой заметки стало наблюдение: эффекты рабочего стола могут тормозить на старых ""встройках"" Intel при том, что в OpenGL-приложениях может быть вполне высокий FPS и хорошая плавность. Я наблюдал разные признаки торможения в двух самых популярных рабочих окружениях (KDE Plasma 5 и Gnome 4) как в X11, так и в Wayland. Я хочу поделиться советом про то, как ситуацию можно исправить на примере дистрибутива Fedora $releasever (на момент описания это 37).

Итак, дано: вы ощущаете, что анимация рабочего стола тормозит (ключевые слова: laggy, jerky, stuttering).

Если речь идёт о Gnome, то решением может стать включение динамической тройной буферизации кадров. Прямо сейчас это изменение ещё не принято в Mutter, но для Fedora есть отдельный Copr-репозиторий с нужными патчами. Сходите по ссылке, там есть все нужные инструкции. Кстати, в Ubuntu 22.04 подобный патч уже применён.

Далее, на скорость отрисовки интерфейса влияет работа framebuffer. Есть смысл включить сжатие фреймбуфера ради повышение FPS в анимациях (и не только). Для этого следует создать файл /etc/modprobe.d/i915.conf с таким содержимым:

options i915 enable_fbc=1

И дать команду:

sudo dracut --force

Эффект наступит после перезагрузки машины.

Если у вас KDE Plasma, то фокус с Mutter, разумеется, не актуален. Plasma имеет свойство более медленно работать в Wayland нежели в X11, что иногда расстраивает (например, если хочется погонять Waydroid). Однако, я случайно нашёл быстрый фикс. Посмотрите, какой вывод у вас даёт команда:

cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

Почти наверняка там будет либо 'powersave', либо 'schedutil'. Изменим его на 'performance':

echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

Вопреки распространённому мнению, это не приведёт ни к опустошению батареи ноутбука, ни к излишнему нагреву CPU. Смысл изменения: расширить рамки динамического регулирования частоты CPU и GPU. Изменения по-прежнему будут происходить по запросу. Здесь есть интересные детали.

В контексте KDE Plasma 5 указанный фикс делает анимации оконного менеджера Kwin мягкими и шелковистыми. Да, это похоже на костыль, но он работает. Если вам и этого мало, попробуйте отключить в эффектах Размытие, это поможет точно.

Наконец, нашёлся ещё один, довольно неожиданный способ победить ""необъяснимые тормоза"" эффектов рабочего стола Plasma. Внезапно, иногда kwin не может правильно определить частоту обновления кадров вашего монитора и требует ручной настройки. Попробуйте в файл ~/.config/kwinrc в разделе [Compositing] добавить такие строки:

MaxFPS=60 RefreshRate=60

Вместо 60 может быть и другая цифирь (75, 144 etc), в зависимости от параметров экрана. Эффект наступит после перезапуска оконного менеджера.

Спасибо, что дочитали. Надеюсь, что мои советы помогут кому-нибудь."'https://habrastorage.org/getpro/habr/upload_files/cd9/a76/091/cd9a76091b19140daf17efdcaa3b41b4.png'"['https://habrastorage.org/getpro/habr/avatars/305/1eb/5bc/3051eb5bcda26b4672897f0e9f2fb14a.png', 'https://habrastorage.org/r/w32/getpro/habr/avatars/305/1eb/5bc/3051eb5bcda26b4672897f0e9f2fb14a.png', 'https://habrastorage.org/getpro/habr/upload_files/cd9/a76/091/cd9a76091b19140daf17efdcaa3b41b4.png', 'https://mc.yandex.ru/watch/24049213']"
7'719874'Как сделать своего “Марка”? Обучение'Привет, ты уже знаешь, как генерировать новости с помощью Марка. Теперь расскажем, как же так получилось, что мы обучили языковую модель генерации новостей. Время пришло! Немного истории В статье...'https://habr.com/ru/post/719874/'"Привет, ты уже знаешь, как генерировать новости с помощью Марка. Теперь расскажем, как же так получилось, что мы обучили языковую модель генерации новостей.

Время пришло!

Немного истории

В статье “Тестим Марка: как происходит генерация новостей” ты узнал, что люди придумывают языковые модели, чтобы начать общаться с компьютерами и поддерживать двустороннюю коммуникацию.

Огромную популярность сейчас приобрела модель ChatGPT, потому что она умеет вести диалог с человеком и поддерживать контекст. Но мало кто знает, что эта модель — немного доработанная версия модели GPT-3, которая лежит на hugging face ещё c 2020-ого.

Инфа для тех, кто знает: большинство телеграм-ботов с названием ChatGPT — это не ChatGPT, а GPT-3 c правильной входной формулировкой, потому что ко второй модели доступ сильно проще из-за меньшего потока людей, а генерируют они обе практически одно и то же)

Представляешь, как давно на самом деле существуют такие технологии?

Интересные факты:

Обсудили исторический контекст, теперь перейдём к самой модели.

Языковая модель GPT-3

GPT-3 — это большая модель, которая умеет генерировать текст по запросу. Иными словами, это ИИ, который берёт строку текста и стремится предсказать, какое слово «должно» (или, скорее всего, будет) идти дальше.

Чтобы данная языковая модель хорошо улавливала семантику слов, в неё заключили 175 миллиардов обучаемых параметров — это те самые параметры для слов, чтобы компьютер мог “почувствовать” их значение. Но, чтобы модель была действительно умной, разработчики из OpenAI заставили GPT-3 «просматривать» миллиарды слов в Интернете, в новостных статьях, сообщениях на форумах, веб-сайтах и т.д.

❓ Зачем? Представь себе человека, который за всю свою жизнь прочитал только Колобка, это была единственная информация, которую он “потребил”. Как думаешь, можно ли с ним поговорить о космосе? Он ведь даже слова такого не знает. Так и с языковой моделью: ей нужно понять мировой контекст, какие есть слова, знания, формулировки, правила языка и прочее.

В итоге после обучения получилась модель, которая много дней потребляла информацию из интернета (600 гигабайт текста) и весит почти 3 гигабайта. Эта языковая модель умеет генерировать текст и понимать запрос человека .

Обучение vs дообучение

Факт: чтобы обучать большие и мощные модели (такие как GPT-3), нужно много денег и времени. Если не верится, вот тут попытка обучить GPT-3 для русского языка, только в 13 раз меньше.

Для нашего генератора новостей мы использовали ruGPT-3 среднего размера, и, если бы мы обучали её сами, мы бы потратили 5.6 млн рублей и больше 30 тысячи часов. Неплохо?

НО! Из-за того, что тратить такое огромное количество ресурсов могут позволить себе только корпорации, а исследовать хочется всем, было найдено решение: дообучение (fine-tuning).

💡 Основная идея дообучения: нам не нужно заново учить языковую модель понимать мировой контекст, нужно сфокусировать её внимание на той информации, которая нас интересует.

Так и человек: он один раз научился читать и понимать значения слов в языке, а если он хочет стать программистом, то ему нужно сфокусироваться на потреблении релевантной информации, попутно запоминая новые словечки.

Взрыв мозга: такие большие нейросети содержат внутри специальный математический слой, названный Attention , через который пропускается весь текст для обучения и этот слой буквально заставляет модель уделить внимание нужной информации (ключевым словам). Гениально, правда?

Разрабатываем новостник

Целиком код с обучением и генерацией тут — COLAB

Пайплайн для разработки генератора такой:

Найти данные: собрать новости по интересующей нас тематике (например, IT), отделяя заголовок от описания (как это сделать не ручками будет в следующей статье) Например, как тут: Обработать данные: скачиваем эту таблицу, сохраняя её где-нибудь, где будут храниться все наши файлы, и записываем в переменную data в формате датафрейма (та же табличка, только по-умному): !wget <https://www.dropbox.com/s/89rmm4wucjve2ll/news.csv?dl=0> -O /content/news.csv my_path = '/content/' # путь, где будут сохраняться все файлы data = pd.read_csv(f'{my_path}news.csv') Теперь нужно почистить наши данные: Удаляем дубликаты (вдруг в наших данных есть две одинаковые новости?) и пустые строчки: data.drop_duplicates(subset = ['title'], inplace=True) data.dropna(inplace=True)

Пишем общую функцию, куда можно добавить разные условия очистки. Например, избавляться от слишком коротких и поэтому несодержательных описаний (если их оставить, модель будет путаться и генерировать затравки вместо фактов). Каждое короткое описание мы заменяем специальным значением NaN, которое обозначает пустышку: def clear_data(text: str): # если длина описания меньше 20 слов-удаляем новость if len( text.split(' ')) < 20: return np.nan return text

Применяем эту функцию для очистки нашего столбца с описаниями и удаляем строки, где теперь лежат пустышки: data['text'] = data['text'].apply(clear_data) data.dropna(subset=['text'], inplace=True) Подключаемся к вычислительным мощностям: DEVICE = torch.device(""cuda"") if torch.cuda.is_available() else None В колабе можно сменить среду выполнения и подключиться к аппаратному ускорителю GPU, тогда твой код будет выполняться в разы быстрее. Так что если обучать самому: попробуй подключить видеокарту локально (гугл в помощь) или запросить мощности у колаба.

Импортируем нашу большую модель по API, как мы это делали в генерации. У нас появляются две главные сущности: модель и токенайзер. from transformers import GPT2LMHeadModel, GPT2Tokenizer model_name_or_path = ""sberbank-ai/rugpt3medium_based_on_gpt2"" tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path) model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(DEVICE) Добавляем специальные токены: этот шаг тоже был в генерации, только теперь нам нужно расширить размер входящих токенов в модель, чтобы она их тоже учла при обучении (и потом генерации): SPECIAL_TOKENS = {'bos_token':'<bos>','eos_token' :'<eos>', 'pad_token':'<pad>', 'sep_token': '<sep>'} tokenizer.add_special_tokens(SPECIAL_TOKENS) model.resize_token_embeddings(len(tokenizer))

Создаем специальный датасет — это класс, к которому модель будет обращаться по ключевым функциям (важно: это просто структура, которая нужна нашей языковой модели в дальнейшем — лучше её не менять): class myDataset(Dataset): def __init__(self, data, tokenizer, gpt2_type=""gpt2"", max_length=150): self.tokenizer = tokenizer # the gpt2 tokenizer we instantiated self.input_ids = [] self.attn_masks = [] for i in data.index.to_list(): title = data['title'][i] description = data['text'][i] if type(data['text'][i]) == str else '' form = '<bos>'+ title + '<sep>' + description + '<eos>' encodings_dict = tokenizer(form, truncation=True, max_length=max_length, padding=""max_length"") self.input_ids.append(torch.tensor(encodings_dict['input_ids'])) self.attn_masks.append(torch.tensor(encodings_dict['attention_mask'])) def __len__(self): return len(self.input_ids) def __getitem__(self, idx): return { 'input_ids': self.input_ids[idx], 'attn_masks': self.attn_masks[idx] } Когда мы инициализируем наш датасет ( def __init__ ), мы проходимся построчно по нашей табличке, берём оттуда тайтл и описание и оформляем их в одну текстовую последовательность вот так: 📖 форма = <bos> заголовок <sep> описание <eos> <bos> специальный токен начала предложения <sep> токен, который показывает нам и языковой модели, что здесь заканчивается заголовок и начинается описание <eos> специальный токен конца предложения (именно такие текстовые последовательности генерирует потом наша модель)

Мы делаем знакомые нам преобразования токенайзером (и внутри него энкодером). → Мы пилим слова на кусочки и присваиваем им уникальные значения, по которым можно найти вектора из чисел, задающие семантику. encodings_dict = tokenizer(form, truncation=True, max_length=max_length, padding=""max_length"") С некоторыми нововведениями: Каждая текстовая последовательность может быть разной длины (какие-то новости покороче, какие-то — длиннее), но так как все, что происходит внутри языковых моделей во время обучения, математические операции (а именно - перемножение матриц), то удобнее, чтобы все предложения были одной длины. Поэтому появляется параметр padding : ⇒ фиксируем максимальную адекватную длину нашей текстовой последовательности: у нас большинство текстовых последовательностей содержат около 130 слов, но иногда встречаются очень длинные новости. Оптимальный вариант = установить максимальную длину предложения в 150 слов. ⇒ Те последовательности, которые короче максимума, будут дополнены специальным токеном <pad> ⇒ Те последовательности, которые длиннее, будут обрезаны В результате все текстовые последовательности будут максимально короткими и информативными, и, что очень важно, одной длины!

На выход у нас формируется словарь encodings_dict , где по ключу 'input_ids' лежат те самые, уникальные значения для кусочков, из которых мы можем составить текстовую последовательность в машинном переводе: self.input_ids.append(torch.tensor(encodings_dict['input_ids'])) self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))

Но еще там есть ключ 'attention_mask' : его задача — показать, где заканчивается смысловое предложение и начинаются токены подгонки под размер ( <pad> ). Это вектор, в котором хранятся значения 1 и 0, обозначающие: 1 — модель, обрати внимание на это слово, оно важное и 0 — не смотри, это просто пустота.

Запускаем код — получаем сформированный датасет: train_dataset = myDataset(data, tokenizer) Интересный факт Вот этот кусок в цикле for , проходящему по твоему датафрейму построчно — единственная часть, где тебе нужно что-то изменить под другую задачу. title = data['title'][i] description = data['text'][i] if type(data['text'][i]) == str else '' form = '<bos>'+ title + '<sep>' + description + '<eos>' Например, у тебя просто куски текста-четверостишья и твоя задача научить модель генерировать их. Тогда в твоём датафрейме построчно будут лежать такие куски стихотворений в колонке quatrain . А твой код будет выглядеть так: text = data['quatrain'][i] form = '<bos>' + text + '<eos>' А всё остальное останется неизменным!

Пилим наш огромный датасет на кусочки, которые наша языковая модель будет поэтапно обрабатывать: data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) Задаём параметры дообучения (можно просто скопировать и поиграться) training_args = TrainingArguments( output_dir=f'{my_path}Checkouts', #The output directory overwrite_output_dir = True, #overwrite the content of the output directory num_train_epochs = 10, # number of training epochs per_device_train_batch_size = 3, # batch size for training per_device_eval_batch_size = 3, # batch size for evaluation warmup_steps = 100,# number of warmup steps for learning rate scheduler gradient_accumulation_steps = 1, # to make ""virtual"" batch size larger save_steps = 3000 ) Тут просто детали нашего обучения: количество эпох, сколько статей одновременно мы рассматриваем при обучении и оценке качества обучения + тебе нужно периодически сохранять свою модель, чтобы, если вдруг всё упадет, тебе не приходилось начинать заново, а была возможность начать там, где закончил. Если твоя видеокарта вдруг не справляется (выдает ошибку), то стоит уменьшить батч) А этот код просто надо копирнуть, сюда ты подтягиваешь ключевые штуки для языковой модели: trainer = Trainer( model=model, args=training_args, data_collator=data_collator, train_dataset=train_dataset, # Optimizer and lr scheduler optimizers = (torch.optim.AdamW(model.parameters(),lr=1e-5),None) ) Самое сложное: обучаем — это может занять мнооооооого времени 🥲 trainer.train() И после обучения обязательно сохраняем модель: tokenizer.save_vocabulary(f'{my_path}tokenizer') trainer.save_model(f'{my_path}model_with_summary') А дальше ты можешь её импортнуть вот так: tokenizer = GPT2Tokenizer.from_pretrained(f'{my_path}tokenizer') model = GPT2LMHeadModel.from_pretrained(f'{my_path}model_with_summary').to(DEVICE) И добавить всё то, что было в статье про генерацию, чтобы посмотреть, что же ты там наобучал. Не всё получится с первого раза, но мы в тебя верим!

Милая напоминашка, если ты потеряшка: целиком код с обучением и генерацией на COLAB

Автор статьи: @anyaschenikova"'https://habrastorage.org/getpro/habr/upload_files/a59/803/4eb/a598034eb9e3d97dd6eb33788dc386f0.png'"['https://habrastorage.org/r/w1560/getpro/habr/upload_files/a59/803/4eb/a598034eb9e3d97dd6eb33788dc386f0.png', 'https://habrastorage.org/getpro/habr/upload_files/a59/803/4eb/a598034eb9e3d97dd6eb33788dc386f0.png', 'https://mc.yandex.ru/watch/24049213']"
8'720004'Обзор уязвимостей DACL'Автор статьи: Александр Колесников Вирусный аналитик В этой статье рассмотрим последние уявзимости, которые были найдены за последние несколько лет. Основной критерий отбора уязвимостей — работа с...'https://habr.com/ru/post/720004/'"Автор статьи: Александр Колесников Вирусный аналитик

В этой статье рассмотрим последние уявзимости, которые были найдены за последние несколько лет. Основной критерий отбора уязвимостей — работа с DACL, что можно делать с системой, если подобные уязвимости попадаются в операционной системе. В качестве исследуемых будем использовать CVE-2022-25365 и CVE-2021-4533.

DACL

В операционной системе Windows, согласно документации, существует минимальная единица проставления привилегий — ACE(Access Contorl Entity). По одному эти сущности не существуют и их обычно используют совместно. Набор ACE называется Access Control List или просто ACL. ACL может быть 2х типов:

SACL

DACL

Если не вникать в детали, то первый используется для того, чтобы следить за состоянием объектов в ОС и давать возможность составлять подробные логи работы системы, то второй это как раз и есть те самые привилегии, которые требуются для выполнения операций или доступа к ресурсам. Оба списка находятся в токене пользователя или ресурса, к которму пользователь или приложение пытается получить доступ. Как эти самые списки увидеть? Данные по ACL можно собирать проще всего 2-мя способами:

Обратиться к get-ACL командлету Запросить данные из LDAP

В первом случае это будет работать как локально на системе, так и в инфраструктуре под управлением Windows AD, а второй вариант работает только в Active Directory.

Попробуем получить данные токена для директории:

Get-Acl C:\Windows | FL

Как видно из описания на картинке, данные скомпонованы по группам и есть описание, какие именно права есть у конкретной группы. По сути это уже разобранные данные ACL списка, но нам сойдет и так. То же самое можно выполнять и для просмотра данных о файлах и ключах реестра. Вообще информацию по токену можно достать из чего угодно, главное — чтобы у вас был создан объект powershell , к которому можно применить командлет. Ну и конечно, это должен быть объект, который считается securable, в Windows это всё, что имеет название.

Далее в общем-то нет никакой магии, но если среди данных из картинки будут те группы пользователей, в которых может появиться пользователь системы, то это можно использовать как примитив для атаки. Результатом атаки может быть:

повышение привилегий,

удаление/перемещение произвольных файлов в системе.

Алгоритм действий при этом достаточно прост:

Нужно найти ресурс, который нужно измениить. Если брать обобщенные подходы, то обычно стараются перезаписать файлы, которые используются для запуска команд другими пользователями. Необходимо обнаружить приложение, которое содержит уязвимость, позволяющую менять адрес обрабатываемого объекта. Определить, что нам нужно — командная строка от имени другого пользователя или просто удаленный файл.

Первый вопрос перекрывается задачей, если смотреть на примеры нашей статьи CVE-2022-25365 и CVE-2021-4533, то становится ясно, что тактика действий должна решать общую задачу — выполнение команд на наибольшем количестве систем.

Второй вопрос — процедура достаточно длительная, решается через изучение приложения, обычно рассматриваются следующие проблемы:

работа с файловой системой и проставляемые привилиегии на файлы и директории;

работа с механизмом pipe, которые располагаются в системе.

Во всех случаях важно, какие привилегии устанавливаются на конечный ресурс.

Следить за этой информацией можно через инструменты:

CVE-2022-25365

Уязвимость, которая была обнаружена в Docker Desktop приложении, основная проблема, которая была использована для атаки — это неверно проставленные привилегии для pipe, которые были использованы приложением. Итог — эскалация привилегий.

Если выкладывать основные действия согласно алгоритму, который приведен выше в статье, то эксплуатация уязвимости разбита на несколько частей.

Поиск механизма работы с файловой системой или с системой пердачи данных между процессами. Идентификация опасных ACL разрешений (опасность определяется в зависимости от задач приложения). Запуск функционала, который позволяет обращаться к объекту с опасными ACL.

С помощью инструмента мониторинга pipe, которые исользует приложение было выявлено, что pipe с именем ""dockerBackendV2"" позволяет обычным пользователям читать и писать в него. Кстати, именованные pipe могут использоваться для доступа к системе и удаленно.

Следующий этап атаки заключается в использовании метода, который был описан ZDI. Если коротко, то любые действия с файлами, которые могут быть проконтроллированы атакующим, могут быть использованы для эскалачии привелегий. Для этого используется механизм отката MSI.

Подробный разбор можно найти тут.

CVE-2021-4533

Одна из уязвимостей, которая была найдена в приложении от компании AVAST Free Antivirus. В этом случае проблема позволяла работать с сервисом приложения. Более того, можно было просто от имени процесса сервиса запустить любую команду в системе. Это было возможно из-за того, что любой пользователь мог запустить OpenProcess функцию и получить любые данные о процессе, включая его токен.

.\accesschk64.exe -p aswEngSrv.exe -nobanner [4704] aswEngSrv.exe RW Everyone RW NT AUTHORITY\ANONYMOUS LOGON RW APPLICATION PACKAGE AUTHORITY\ALL APPLICATION PACKAGES RW NT AUTHORITY\SYSTEM

Это тоже является достаточно серьезной уязвимостью и наравне с теми примерами, которые были рассмотрены выше — позволяет повышать привелегии.

Если так же проецировать находку на алгоритм из первой части этой статьи, то последовательность действий следующая:

Получаем данные о доступе к процессу сервиса. Запускаем процедуру копирования токена (примеры можно найти в сети). Открываем командную строку или работаем с файловой системой.

Таким образом, уязвимости с обычными, на первых взгляд, привилегиями могут быть использованы для достаточно разных атак. От вывода из строя приложений и ОС до выполнения команд из-под любого пользователя в системе. В качестве дополнительного чтения также рекомендую ознакомиться с этой статьей, там есть описание еще дополнительных методов эскалации привелегий, в том числе и за счет неверно проставленного DACL.

В заключение приглашаю всех желающих на открытое занятие «Acl атаки в Windows AD». На нем мы изучим основные наборы привилегий, которые позволяют проводить атаки на AD. Записаться можно на странице курса «Пентест. Практика тестирования на проникновение»."'https://habrastorage.org/getpro/habr/upload_files/67f/77e/90c/67f77e90cf7f3ed0c6ed8694dfe05f27.png'"['https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/avatars/b9f/baf/5f9/b9fbaf5f96ae52973706a0716bd9216e.jpg', 'https://habrastorage.org/r/w32/getpro/habr/avatars/b9f/baf/5f9/b9fbaf5f96ae52973706a0716bd9216e.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/6ca/83b/ac3/6ca83bac31712baf0ffbf7e4dca5d2e2.jpeg', 'https://habrastorage.org/getpro/habr/company/2d5/0ed/b57/2d50edb57cf45fa07cc4f39f53b78395.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/e71/7f3/026/e717f30265271748f12d344233bf8754.png', 'https://habrastorage.org/getpro/habr/upload_files/67f/77e/90c/67f77e90cf7f3ed0c6ed8694dfe05f27.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/67f/77e/90c/67f77e90cf7f3ed0c6ed8694dfe05f27.png']"
9'720000'Подготовка шаблона приложения на Typescript с Nest, Nuxt 3 и Docker'Решил описать свой подход построения окружения на Typescript с Nest на бекенде, Nuxt (SPA) на фронтенде. Все заворачивается в один docker-образ и запускается как standalone приложение c nginx,...'https://habr.com/ru/post/720000/'"Решил описать свой подход построения окружения на Typescript с Nest на бекенде, Nuxt (SPA) на фронтенде. Все заворачивается в один docker-образ и запускается как standalone приложение c nginx, healthcheck’ами, тестами и ш…широкой сферой применения.

Делал это в качестве фундамента для будущих проектов или с целью изучения Nest, Nuxt 3 с composable функциями. Можно использовать это как инструкцию к настройке подобной архитектуры, можно взять за основу код с github.

Архитектура проекта

Шаблон приложения поставляется в виде одного docker-образа, в котором установлен nest + nginx и собраны backend и frontend .

Схема архитектуры

Файловая структура

Для начала опишу из как выглядит архитектура проекта.

└── application/ ├── backend/ │ └── NEST приложение ├── frontend/ │ └── NUXT приложение ├── docker/ │ └── nginx/ │ └── conf.conf ├── .dockerignore ├── .gitignore ├── docker-compose.yml ├── Dockerfile └── readme.md

backend — стандартное nest приложение с добавленным serve-static модулем.

frontend — стандартное nuxt приложение с добавленным и настроенной связью с backend

docker — папка с конфигами, которые пойдут в docker образ (в текущей версии только nginx)

Dockerfile — указания по сборке докер-образа

docker-compose.yml — файл для запуска проекта

Весь проект доступен на github, его можно склонировать, запустить командой docker-compose up -d (подробнее про запуск написал в конце статьи) и запустить готовый к расширению шаблон приложения. Ниже я описал что именно изменено в стартовых приложениях и каким образом настроена связь между ними

В этом шаблоне нет базы данных и каких-либо других сторонних зависимостей, чтобы не ограничивать набор компонентов для дальнейшей разработки.

Процесс обработки запросов

В качестве сервера, принимающего запросы используется Nginx. Он раздает статику собранного frontend приложения и перенаправляет запрос на бекенд, если URL запроса начинается на /api

Таким образом может быть 2 типа запроса.

Статический:

И запрос к API:

Подготовка Backend сервиса

За основу взят стартовый набор nest:

$ npm i -g @nestjs/cli $ nest new backend

Дальше необходимо сделать некоторые доработки. Первым делом в main.ts прописываем порт по умолчанию на 3001, добавляем префикс /api . Таким образом main.ts обретает следующий вид:

import { NestFactory } from '@nestjs/core'; import { AppModule } from './app.module'; async function bootstrap() { const app = await NestFactory.create(AppModule); const port = process.env.APP_PORT || 3001; app.setGlobalPrefix('api'); app.enableCors(); await app.listen(port); } bootstrap();

Настройка static директории

В папку static будет переноситься статичный html/js/css бандл с nuxt приложением и потом раздаваться как статичный сайт при запуске проекта без nginx.

Да, при прочих равных, стоит запускать проект с nginx и для этого не нужно переносить в папку static ничего. Но на то это и бойлер, что я заранее не знаю как он будет и где запускаться. Может быть, в каких то ситуациях, при малых нагрузках, будет достаточно запуска чистого Nest.

Для того, чтобы nest раздавал статику достаточно подключить модуль serve-static внутрь AppModule

import { Module } from '@nestjs/common'; import { AppController } from './app.controller'; import { AppService } from './app.service'; import { ServeStaticModule } from '@nestjs/serve-static'; import * as path from 'path'; @Module({ imports: [ ServeStaticModule.forRoot({ rootPath: path.join(__dirname, '..', 'static'), serveRoot: '/', exclude: ['/api*'], }), ], controllers: [AppController], providers: [AppService], }) export class AppModule {}

Обратите внимание на блок exclude: ['/api*'] . Это нужно для того, чтобы статика раздавалась на всех ссылках, кроме /api — при запуске проекта по пути /api будет размещаться само nest приложение.

В саму папку static размещаем .gitignore с двумя строчками

* !index.html

И index.html, который будет использоваться только при разработке и при сборке конечного docker-образа в эту папку будет складываться html/js/css интерфейса.

Небольшое отступление по поводу префикса к api

В nest можно реализовать префикс /api двумя способами:

в каждом контроллере приписывать /api в @Controller('/api/controller-route')

прописать на уровне nest приложения глобальный префикс

Я в своем шаблоне использую второй способ. Для его реализации нужно сделать следующее:

Прописать в main.ts строчку app.setGlobalPrefix('api'); Поправить e2e тест, чтобы в нем тоже создавалось приложение с префиксом и поправить сами тесты.

Поскольку я стараюсь разрабатывать через e2e тесты и тестов в проектах может быть очень много, я сразу выношу в отдельную функцию создание тестового приложения:

export async function createTestingApp() { return ( await Test.createTestingModule({ imports: [AppModule], }).compile() ) .createNestApplication() .setGlobalPrefix('api'); }

Дальше я уже, в тестах, использую эту функцию вместо штатной инициализации приложения:

import { createTestingApp } from './utils/create-testing-app'; // ..... beforeEach(async () => { app = await createTestingApp(); await app.init(); });

Настройка тестового окружения

Я уже немного затронул тестовое окружение в предыдущем блоке, но по тестам я сделал еще небольшие изменения.

удалил стандартный spec файл у контроллера, т.к. сам предпочитаю e2e тесты и узкие тесты пишу в редких случаях

поменял формат jest.e2e.config.json на js , тк, зачастую, в проектах приходится добавлять динамические конфигурации и IDE js формат считывает сразу.

поправил базовый тест с указанием /api в самих тестах

Подготовка Frontend

В качестве фронта берется Nuxt 3 и ставится через официальную команду

yarn create nuxt-app frontend

Важный момент: я не буду использовать Nuxt с SSR, т.к. у меня планируется чисто SPA подход (когда браузер загружает целиком весь код к себе и дальше уже рендерит интерфейс).

Да, SSR классно и здорово, но считаю его уместным в проектах с необходимостью поддерживать SEO или если необходимо часть логики отображения скрыть от пользователя (чтобы не показывать какие-то переменные окружения).

В любом случае, при необходимости, данный стартовый набор можно “переобуть” на работу с SSR. Что бы выключить SSR режим надо в nuxt.config.ts указать ssr: false

Пара слов про Options и Composition

Если вы давно знакомы с Vue, то вы должны знать, что раньше все компоненты можно было делать vue компоненты только через Options подход (создавать объект с полями data, computed и тд). Сейчас появился подход через setup функцию и мне до конца не ясны прелести этого подхода.

Я же остановился, пока что, на подходе через options и постепенно внедряю compose функции в небольших проектах. В текущем наборе я выбрал Composition подход, т.к. тут функционала почти нет и заодно можно попробовать.

Подключение NuxtPage

Изначально в App.vue не проставлен NuxtPage компонент и, следовательно, маршрутизация через файлы в pages работать не будет. Поэтому необходимо App.Vue привести к следующему виду:

<template> <div> <NuxtPage /> </div> </template>

После чего каждый файл в папке pages/ будет открываться по одноименной ссылке в браузере. Подробнее можно прочитать здесь.

Коннектор к API

Для реализации бизнес-логики во Vue 3 разработчиками можно использовать Composable функции. Раньше я всегда делал подобные вещи в виде отдельного плагина с подстановкой хедера авторизации + указания baseUrl из env переменной.

Сейчас я сделаю по-современному через создание своей composable функции, расширяющей useFetch . В Nuxt composables создаются автоматически, создав файл в папке composables .

// frontend/composables/api.ts import { UseFetchOptions } from '#app'; import { NitroFetchRequest } from 'nitropack'; import { KeyOfRes } from 'nuxt/dist/app/composables/asyncData'; export function useApiRequest<T>( request: NitroFetchRequest, opts?: | UseFetchOptions<T extends void ? unknown : T, (res: T extends void ? unknown : T) => T extends void ? unknown : T, KeyOfRes<(res: T extends void ? unknown : T) => T extends void ? unknown : T>> | undefined ) { const config = useRuntimeConfig(); return useFetch(request, {baseURL: config.public.baseURL, ...opts}); }

Чтобы конструкция config.public.baseURL работала, необходимо расширить nuxt.config.ts следующим образом:

export default defineNuxtConfig({ ssr: false, runtimeConfig: { public: { baseURL: process.env.API_URL || 'http://localhost:3001/', }, }, })

И теперь, по умолчанию, baseURL будет равен http://localhost:3001/ , чтобы, при разработке, стучаться в отдельно запущенный Nest. При сборке буду менять его на /api .

Пример использования API вызова

В качестве примера я оставил в компонент, который делает вызов в /api/test и проставляет в разметку все состояния запроса:

<template> <div> <template v-if=""pending""> Loading </template> <template v-else> <template v-if=""data""> Api result: {{ data }} </template> <template v-else-if=""error""> Api ERROR: {{ error }} </template> <button @click=""refresh()"">refresh</button> </template> </div> </template> <script setup> import { useApiRequest } from '../composables/api' const { data, pending, error, refresh } = useApiRequest('/api/test') </script>

Подготовка Docker-образа и docker-compose.yml

В своем личном блоге я писал и снимал про это видео, что небольшие проекты я разворачиваю достаточно топорным способом:

подготовить docker образ

подготовить docker-compose

развернуть на сервере nginx-proxy c acme-companion

запускать проект обычным docker-compose up -d и наслаждаться рабочим продуктом

Да, это конечно не Kubernetes и не супер отказоустойчивая архитектура. Но такой подход позволяет на VPS на 400р в месяц запустить десяток подобных проектов для личного использования.

Основная идея сборки состоит из следущих этапов:

Собрать frontend (html, js, css) Собрать backend Подсунуть в backend файлы из frontend в папку static Собрать nginx образ, который будет разбирать траффик на статику и логику

Dockerfile с multi-stage build

В проекте я использую node 16 на базе образа alpine. Поэтому начинаем Dockerfile со строчек

FROM node:16-alpine as base-builder WORKDIR /app

Для начала нужно собрать frontend — подтянуть зависимости, собрать html, js, css.

FROM base-builder as build_fe WORKDIR /app COPY ./frontend/package.json ./frontend/yarn.lock* ./ RUN yarn install ADD ./frontend ./ RUN yarn generate

По итогу в этом промежуточном образе у нас будет собранный frontend в папке /app/dist

Далее собираем backend

FROM base-builder as build_be WORKDIR /app COPY ./backend/package.json ./backend/yarn.lock* ./ RUN yarn install ADD ./backend ./ RUN yarn build

И получаем промежуточный образ только с backend . Теперь осталось собрать воедино в следующий промежуточный образ, который будет на 3001 порту слушать все запросы:

FROM node:16-alpine as finalNode WORKDIR /app COPY --from=build_be /app /app COPY --from=build_fe /app/dist /app/static CMD yarn start

Я до конца не определился в необходимости этого этапа и, честно говоря, его можно и не делать. У нас, в итоге, получается backend, который умеет также отдавать статику приложения — то есть полностью самостоятельно рабочий docker-образ с приложением, который может работать без nginx. Но именно в рамках текущей статьи эта возможность не используется

Теперь осталось собрать ту часть, которая будет с nginx :

FROM nginx:alpine as finalNginx WORKDIR /usr/share/nginx/html RUN rm -rf ./* COPY --from=finalNode /app/static . COPY ./docker/nginx/conf.conf /etc/nginx/conf.d/default.conf CMD [""nginx"", ""-g"", ""daemon off;""]

Также надо не забыть положить файл конфигурации nginx по указанном пути:

# docker/nginx/conf.conf server { listen 80 default_server; root /usr/share/nginx/html; client_max_body_size 20M; location / { root /usr/share/nginx/html; index index.html index.htm; try_files $uri $uri/ /index.html; } location /api { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_pass http://node:3001; } }

Теперь мы, в рамках одного Dockerfile получили полную сборку всего, что нужно для работы приложения.

Итоговый Dockerfile можно посмотреть в github репозитории.

Подготовка docker-compose.yml

Как я писал выше, я запускаю подобные проекты на сервере, используя nginx-proxy . Так что, первым делом, в конце файла надо объявить сеть reverse-proxy , через которую будет идти подключение из внешнего мира к моему контейнеру с nginx .

version: ""3.8"" services: # тут будут сервисы networks: reverse-proxy: external: name: reverse-proxy back: driver: bridge

Также я добавил сеть back — это изолированная сеть, через которую между собой будут общаться nginx и backend .

Теперь опишем как мы будем запускать наш образ с той частью, которая отвечает за backend :

node: build: context: . target: finalNode networks: - back expose: - 3001 restart: always environment: - APP_PORT=3001 healthcheck: test: wget --no-verbose --tries=1 --spider <http://localhost:3001> || exit 1 timeout: 3s interval: 3s retries: 10

По порядку о каждом параметре:

build context — что будет являться текущей директорией при сборке Dockerfile target — какую часть multi-stage build нужно запускать в этом месте. В данном случае мы указываем, что собирать нужно все до finalNode

networks тут мы указываем только back, т.к. во внешний мир контейнер ходить не будет и нужен только доступ от nginx к этому контейнеру.

expose этот пункт открывает доступ другим контейнерам в сети по перечисленным портам. В данном случае мы сообщаем, что в сети back контейнеры могут подключаться на 3001 порт

restart: always сообщаем, что этот контейнер надо перезапускать всегда. Даже после перезапуска сервера проект будет запущен будет работать до тех пор пока не выключим его командой docker-compose down

environment передача переменных окружения в сам процесс node в нашем случае только указываем порт, на котором мы хотим, чтобы backend был запущен

healthcheck прекрасный инструмент для контроля работоспособности контейнера test — команда, от которой мы ожидаем exit-code = 0 (какие есть еще можно прочитать здесь) timeout — время, которое может выполняться команда. Если команда зависла на больший срок, то проверка считается не пройденой interval — с какой частотой стоит выполнять команду, чтобы быть уверенным, что контейнер работает retries — после скольких неудачных ответов сервер помечается “нерабочим”.



Теперь добавим блок с запуском nginx :

nginx: build: context: . target: finalNginx networks: - reverse-proxy - back expose: - 80 restart: always depends_on: node: condition: service_healthy environment: - VIRTUAL_HOST=${DOMAIN} - VIRTUAL_PORT=80 - LETSENCRYPT_HOST=${DOMAIN} - LETSENCRYPT_EMAIL=test@test.ru

Подробнее:

build тоже самое, что в node сервисе, только указан другой target , т.к. нам нужно получить ту часть, которая связана с nginx

networks тут теперь 2 сети: reverse-proxy сеть, через которую будет доступ от контейнера nginx-proxy back та сеть, в которой есть контейнер node чтобы можно было пересылать запросы ему

expose сообщаем всем в сетях, что в этот контейнер можно стучаться на 80 порт. Это нужно nginx-proxy для обработки запросов

restart аналогично сервису node

depends_on тут мы указываем от каких сервисов мы зависим если это не указать, то nginx будет запускаться вместе с остальными и может получиться ситуация, в которой node еще не запущен, а nginx уже готов принимать запросы, что нехорошо поэтому мы указываем, что зависит от node сервиса зависимость можно считать удовлетворенной только когда сервис прошел свой healthcheck (как раз блок condition )

environment тут мы указываем переменные окружения, которые нужны для работы nginx-proxy : VIRTUAL_HOST название домена доступа к приложению VIRTUAL_PORT порт, на котором запущено приложение в контейнере LETSENCRYPT_HOST тот же самый домен но уже для создания https сертификата LETSENCRYPT_EMAIL электронная почта, куда писать о том, что скоро сертификат будет просрочен тут используется внешняя переменная окружения ${DOMAIN} и она будет записаться из файла .env который будет лежать рядом с docker-compose.yml файлом (подробнее тут).



Конечный вариант файла также находится в github репозитории.

Дополнительные моменты в подготовке окружения

В корне проекта я создал файл .dockerignore чтобы, во время сборки, не перекачивать в контекст лишнего:

#.dockerignore .idea .git **/.nuxt **/dist **/.output **/node_modules **/.env

Также создал .env.example в качестве файла-примера:

DOMAIN=domain.ru

Запуск приложения

Подготовка сервера

Разумеется на сервере должен уже стоять Docker. Если нет, то установите его по официальной инструкции.

Далее необходимо на сервере запустить nginx-proxy и лучше это делать в отдельном месте на том-же сервере (инструкция здесь, но если нужно, то напишите в комментариях и дополню эту инструкцию здесь).

Запуск самого приложения

Запускается все это приложение очень простым образом:

Клонируем исходники Прописываем DOMAIN в .env файл в корне проекта Запускаем командой docker-compose up -d

Одной командой этот запуск можно сделать следующей командой:

DOMAIN=domain.ru && echo DOMAIN=$DOMAIN > .env && docker-compose up -d --build

Важно: заменить domain.ru на свой домен, который уже направлен на сервер, где мы запускаем сервис.

Обновление версии приложения

Если нужно обновить исходники до последней версии, то можно выполнить следующую команду:

git fetch && git reset --hard origin/master && docker-compose up -d --build

И проект обновится и запустит обновленную версию на домене.

Небольшое заключение

В конечном итоге получился вариант шаблона приложения на Nuxt + Nest который дальше можно расширять. Он крайне пуст — нет БД, авторизации и прочих базовых вещей. Разумеется в наших проектах есть разные шаблоны приложений, но я решил начать с описания самого базового варианта, который дальше можно развивать куда угодно.

Если подобный формат полезен и интересен для дальнейшего описания, то в следующих статьях опишу подобный стартовый набор с базой данных (Postgres) и авторизацией (JWT). Также есть мысль описать процесс подготовки и настройки ansible для подобных проектов.

Также в своем личном блоге в рубрике разработка пишу разные обучающие статьи и делюсь опытом на своем Youtube канале и Telegram.

Благодарю за внимание."'https://habrastorage.org/getpro/habr/upload_files/8d3/e66/88a/8d3e6688a6dfb96db7ff054a3a1cea45.jpg'"['https://habrastorage.org/r/w1560/getpro/habr/upload_files/e3e/5ee/4fc/e3e5ee4fcc7dcf00dd9aa5f5e022d902.png', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/8d3/e66/88a/8d3e6688a6dfb96db7ff054a3a1cea45.jpg', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/upload_files/8d3/e66/88a/8d3e6688a6dfb96db7ff054a3a1cea45.jpg', 'https://habrastorage.org/r/w32/getpro/habr/avatars/026/a3e/3f0/026a3e3f02754906f0ef69ba812bd251.jpg', 'https://habrastorage.org/getpro/habr/upload_files/ba0/3ae/379/ba03ae3799c53203984f05802f881dfa.gif', 'https://habrastorage.org/getpro/habr/upload_files/d83/783/bf9/d83783bf964a69bb0a279dec153b2882.gif', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/898/5fc/4ab/8985fc4abefa4722ca22b8e0a67a1779.png', 'https://habrastorage.org/getpro/habr/avatars/026/a3e/3f0/026a3e3f02754906f0ef69ba812bd251.jpg']"
10'719664'Один плагин, чтоб править всеми. Как разработать сканер на Flutter для 3 платформ и ускорить ввод данных в 2 раза'Привет! Меня зовут Сергей, я Flutter-разработчик SimbirSoft. В этой статье хочу поделиться интересным платформоспецифичным кейсом для мобильных устройств и ТСД. Нам с командой удалось сократить...'https://habr.com/ru/post/719664/'"Привет! Меня зовут Сергей, я Flutter-разработчик SimbirSoft. В этой статье хочу поделиться интересным платформоспецифичным кейсом для мобильных устройств и ТСД. Нам с командой удалось сократить затраты на разработку и ускорить процесс ввода данных в 2 раза.

Клиент располагает крупными товарными складами, на которых сотрудники используют сканеры 1-D/2-D кодов — это смартфоны на iOS, Android, а также терминалы сбора данных с установленным Flutter-приложением для сборки заказов. Нашей задачей стало обновить плагин сканера, не привлекая отдельные команды для разных платформ.

Очевидно, что данная функциональность сильно полагается на платформу, и Flutter из коробки не умеет работать с ТСД. Как мы решили эту задачу, расскажу по порядку, а в конце поделюсь результатами тестов и ссылкой на исходный код. Спойлер: по сравнению с ручным вводом штрихкодов скорость выросла в 13,4 раза, а с предыдущей версией сканера в 2 раза.

Подобный кейс применим везде, где требуется сканировать 1-D/2-D коды в большом количестве. Поэтому материал будет полезен разработчикам кроссплатформенных приложений для решения подобных задач, а также их заказчикам.

Вводные данные

Представим большой склад, с которого ежедневно отправляются сотни заказов. Чтобы вести учет в системе по каждому наименованию в собранном заказе, сотрудник склада сканирует штрихкоды товаров.

Для этого сотрудник использует специальное мобильное приложение на Android и iOS со сканером, написанным на Flutter. Сканер не работает в режиме потока, а вызывается по запросу, и каждое отдельное сканирование занимает 2 секунды (это в идеальном случае). Кажется, что немного, но за весь день приходится обрабатывать тысячи таких кодов.

Владелец склада планирует расширение бизнеса, он хотел бы отправлять больше заказов и тратить на распознавание штрихкодов меньше времени. Еще для части работников он решает приобрести ТСД (терминал сбора данных) под управлением Android.

Мы оценили требования и пришли к выводу, что требуется написать новый плагин для всех используемых устройств, который будет считывать штрихкоды потоково, не требуя открытия нового экрана. За счет уменьшения количества инициализаций и закрытий сканера/камеры удастся сократить общее время, затрачиваемое на сканирование.

Наша задача — написать новый плагин со следующими фичами:

Он должен работать на Android, iOS и выбранных моделях ТСД (Android). Возможность отсканировать несколько 1-D/2-D кодов с одного экрана. Для не ТСД сканирование должно осуществляться с помощью распознавания изображения с камеры устройства (Android/iOS).

Становится понятно, что для этой задачи не подойдут стандартные средства Dart/Flutter или существующие pub-пакеты. Поэтому принимаем решение написать свой плагин, и нам на помощь приходит встроенный в Flutter механизм взаимодействия с кастомным платформоспецифичным кодом — Platform Channels!

Что такое Platform Channels и как его использовать

Platform Channels — механизм, позволяющий из dart-кода вызывать нативный код.

Flutter использует гибкую систему, которая позволяет вызывать специфичные для платформы API на языке, который работает непосредственно с этими API:

Kotlin или Java на Android

Swift или Objective-C на iOS

C++ на Windows

Objective-C на macOS

C на Linux

Сообщения передаются между клиентом (UI) и хостом (платформой) с использованием каналов платформы, как показано на этой схеме:

Схема передачи Flutter сообщений с Flutter‑приложения на хост‑платформу. Источник

Сообщения и ответы передаются асинхронно, чтобы обеспечить отзывчивость пользовательского интерфейса. На стороне клиента MethodChannel позволяет отправлять сообщения, соответствующие вызовам методов. Со стороны платформы MethodChannel на Android ( MethodChannelAndroid ) и FlutterMethodChannel на iOS ( MethodChanneliOS ) позволяют принимать вызовы метода и отправлять результат обратно. Эти классы позволяют разрабатывать плагин платформы с очень небольшим количеством бойлерплейта.

Создадим наш plugin, запустив следующую команду в терминале:

flutter create --org com.example --template=plugin --platforms=android -a kotlin example_barcode_scanner

Это создает проект плагина в папке «example_barcode_scanner» со следующим содержимым:

Dart API для плагина

lib/example_barcode_scanner.dart

Реализация API плагина в Kotlin для конкретной платформы Android

android/src/main/java/com/example/hello/ExampleBarcodeScannerPlugin.kt

Приложение Flutter, которое зависит от плагина и иллюстрирует, как его использовать

example/

Вынос платформоспецифичного кода в отдельный плагин позволит нам повторно использовать наш сканер в других приложениях. Подробнее о создании плагинов можно прочесть в документации .

Реализация плагина на Dart

Приступим же к реализации нашего плагина!

В качестве первого шага нам потребуется определить PlatformChannel API нашего плагина. Для этого создайте .dart файл вне lib проекта:

/// описание API, генерируемого pigeon для платформы @HostApi() abstract class ScanHostApi { /// запуск сканера @async StartScanResult startScan(); /// остановка сканера @async void stopScan(); } /// описание API, геренрируемого pigeon для Flutter-приложения @FlutterApi() abstract class ScanFlutterApi { /// метод, вызываемый платформой для передачи результата сканирования void onScan(String data); }

Опишем модели, используемые для передачи данных по PlatformChannel:

— Тип реализации сканера, это потребуется в дальнейшем для выбора реализации UI под соответствующий тип сканера:

/// тип реализации сканера enum ScannerType { /// ТСД tsd, /// камера camera }

— Модель свойств камеры для корректного отображения preview с камеры:

/// модель свойств камеры class CameraProperties { const CameraProperties( this.textureId, this.aspectRatio, this.width, this.height, ); /// id текстуры для передачи изображения final int textureId; /// соотношение сторон final double aspectRatio; final int width; final int height; }

— Модель результата метода startScan:

/// модель результата запуска сканера class StartScanResult { const StartScanResult( this.scannerType, this.cameraProperties, ); /// тип сканера final ScannerType scannerType; /// свойства камеры /// согласно контракту, если тип сканера [ScannerType.tsd], то данное поле буде null. final CameraProperties? cameraProperties; }

Далее запустим pigeon для генерации Dart и платформенного кода API для типобезопасного взаимодействия по PlatformChannel:

flutter pub run pigeon \

--input pigeons/scan_api.dart \

--dart_out lib/scan_api.dart \

--java_out android/src/main/kotlin/com/example/example_barcode_scanner/Pigeon.java \

--java_package ""com.example.example_barcode_scanner""

Следующим шагом создадим интерфейс сервиса для взаимодействия со сканером:

abstract class ExampleBarcodeScannerService extends ScanFlutterApi { static ExampleBarcodeScannerService? _instance; static ExampleBarcodeScannerService get instance { if (_instance != null) { return _instance!; } _instance = ExampleBarcodeScannerServiceImpl(); return _instance!; } /// поток результатов сканирования Stream<String> get scanResultStream; /// метод запуска сервиса сканирования /// в [onScan] передается результат успешного сканирования /// /// возвращает [StartScanResult] с информацией о сканере Future<StartScanResult> startScan(); /// метод остановки сервиса сканирования Future<void> stopScan(); }

Можно заметить что ExampleBarcodeScannerService наследует ScanFlutterApi. Это сделано для того чтобы экземпляр реализации ExampleBarcodeScannerService мог принимать события от платформы.

ExampleBarcodeScannerServiceImpl() { // важно вызвать чтобы зарегистрировать экземпляр // плагина для получения сообщений по PlatformChannel ScanFlutterApi.setup(this); }

5. Для вызовов методов платформы потребуется создать экземпляр ScanHostApi:

final ScanHostApi _scanHostApi = ScanHostApi();

Таким образом, при вызове startScan/stopScan мы будем просто проксировать вызовы платформе:

@override Future<StartScanResult> startScan() { // вызываем метод платформы для запуска сканера return _scanHostApi.startScan(); } @override Future<void> stopScan() { // вызываем метод платформы для остановки сканера return _scanHostApi.stopScan(); }

6. А платформа, в свою очередь, будет проактивно вызывать метод onScan при распознании штрихкода:

@override void onScan(String data) { _scanResultStreamController.add(data); }

Реализация плагина на Kotlin

От реализации интерфейса на Dart перейдем к реализации плагина на Android:

/** Класс плагина сканера * реализует [Pigeon.ScanHostApi] для получения событий по PlatformChannel * @property [flutterApi] - экземпляр [Pigeon.ScanFlutterApi] для вызова api Flutter приложения * */ class ExampleBarcodeScannerPlugin : FlutterPlugin, ActivityAware, Pigeon.ScanHostApi {

ExampleBarcodeScannerPlugin помимо FlutterPlugin будет также реализовывать ActivityAware, чтобы наш плагин получал уведомления о состоянии activity через данные методы:

override fun onAttachedToActivity( activityBinding: ActivityPluginBinding ) {...} override fun onDetachedFromActivityForConfigChanges() {...} override fun onReattachedToActivityForConfigChanges( activityBindingScanner: ActivityPluginBinding ) {...} override fun onDetachedFromActivity() {...}

Но стоит отдельно отметить, что onAttachedToActivity вызывается строго после onAttachedToEngine из FlutterPlugin.

А интерфейс Pigeon.ScanHostApi был сгенерирован pigeon, и содержит методы, вызываемые Flutter через PlatformChannel.

Чтобы экземпляр плагина мог получать события по PlatformChannel, важно вызвать setup из Pigeon.ScanHostApi:

override fun onAttachedToEngine( @NonNull flutterPluginBinding: FlutterPlugin.FlutterPluginBinding ) { // важно вызвать, чтобы зарегистрировать экземпляр // плагина для получения сообщений по PlatformChannel Pigeon.ScanHostApi.setup(flutterPluginBinding.binaryMessenger, this)

Далее определим интерфейс сканера, общий для камеры и ТСД:

/** * Интерфейс сканера * */ interface Scanner { fun onActivityAttach(activity: Activity) fun onActivityDetach(activity: Activity) /** * метод запуска сканера * @param onData - callback вызываемый при распозновании штрихкода * @param onComplete - callback вызываемый при завершении запуска зканера * */ fun startScan( onData: (String) -> Unit, onComplete: (Pigeon.StartScanResult) -> Unit, ) /** * Метод остановки сканера * */ fun stopScan() }

onActivityAttach и onActivityDetach нужны для того чтобы мы могли иметь доступ к activity внутри сканера.

Реализация сканера

От интерфейса перейдем к реализации в виде сканера, использующего заднюю камеру устройства для распознания штрихкодов.

В startScan получаем processCameraProvider и создаем preview:

val cameraProviderFuture = ProcessCameraProvider.getInstance(activity) cameraProviderFuture.addListener( { val cameraProvider: ProcessCameraProvider = cameraProviderFuture.get() processCameraProvider = cameraProvider val preview = Preview.Builder().build()

Далее биндим userCase preview к cameraProvider:

try { cameraProvider.unbindAll() camera = cameraProvider.bindToLifecycle( activity as LifecycleOwner, CameraSelector.DEFAULT_BACK_CAMERA, preview, imageAnalysis )

Связываем текстуру с preview:

preview.setSurfaceProvider { request -> val reqRes = request.resolution // связываем текстуру и превью val surfaceTexture = textureEntry.surfaceTexture() surfaceTexture.setDefaultBufferSize(reqRes.width, reqRes.height) request.provideSurface(Surface(surfaceTexture), mainThreadExecutor) {}

Формируем результат со свойствами камеры и отправляем его через onComplete в Flutter:

// формируем результат val cameraProperties = Pigeon.CameraProperties.Builder() .setAspectRatio(reqRes.height.toDouble() / reqRes.width.toDouble()) .setHeight(reqRes.height.toLong()).setWidth(reqRes.width.toLong()) .setTextureId(textureEntry.id()).build() val startScanResult = Pigeon.StartScanResult.Builder() .setScannerType(Pigeon.ScannerType.CAMERA) .setCameraProperties(cameraProperties).build() // отправляем результат через вызов onComplete onComplete(startScanResult)

Далее в ExampleBarcodeScannerPlugin создадим экземпляр CameraScanner как дефолтный сканер, так как большинство девайсов обладают камерой:

override fun onAttachedToActivity(activityBinding: ActivityPluginBinding) { activity = activityBinding.activity val deviceInfo = ""${Build.MANUFACTURER} ${Build.MODEL} ${Build.DEVICE}"" Log.i(""ExampleBarcodeScanner"", deviceInfo) scanner = when { // TODO здесь можно добавить создание объекта реализации [Scanner] под конкретный ТСД else -> textureRegistry?.let { textureRegistry -> return@let CameraScanner(textureRegistry) } }

А метод startScan ExampleBarcodeScannerPlugin примет следующий вид:

override fun startScan(result: Pigeon.Result<Pigeon.StartScanResult>?) { val scanner = this.scanner if (scanner == null) { result?.error(Exception(""Scanner not running"")) return } scanner.startScan(onData = { data -> ContextCompat.getMainExecutor(activity).execute { Log.i(""ExampleBarcodeScanner"", ""data: $data"") flutterApi?.onScan(data) {} } }, onComplete = { result?.success(it) }) }

Важно вызывать методы FlutterApi только из главного потока, так как они обращаются к PlatformChannel.

Но данный сканер еще не умеет распознавать изображения. Чтобы исправить досадный недостаток, напишем свой класс, реализующий ImageAnalysis.Analyzer:

/** * класс анализатора изображения для распознавания EAN-13 и EAN-8 штрихкодов */ class MlKitCodeAnalyzer( private val barcodeListener: SuccessListener, ) : ImageAnalysis.Analyzer { private val scanner = BarcodeScanning.getClient( defaultOptions() ) private fun defaultOptions() = BarcodeScannerOptions.Builder().setBarcodeFormats( Barcode.FORMAT_EAN_13, Barcode.FORMAT_EAN_8, ).build() @SuppressLint(""UnsafeExperimentalUsageError"") override fun analyze(image: ImageProxy) {...} }

Данный анализатор использует mlKit для распознания штрихкодов:

@SuppressLint(""UnsafeExperimentalUsageError"") override fun analyze(image: ImageProxy) { val mediaImage = image.image ?: return val mlImage = InputImage.fromMediaImage(mediaImage, image.imageInfo.rotationDegrees) val currentTimestamp = System.currentTimeMillis() scanner.process(mlImage).addOnSuccessListener { barcodes -> barcodes.firstOrNull()?.let { it.rawValue?.let(barcodeListener) } }.addOnCompleteListener { // Позволяет производить сканирование раз в секунду CoroutineScope(Dispatchers.IO).launch { delay(1000 - (System.currentTimeMillis() - currentTimestamp)) image.close() } } }

Подключим наш анализатор к камере. Для этого нам надо создать ImageAnalysis и указать в качестве Analyzer экземпляр нашего анализатора.

val imageAnalysis = ImageAnalysis.Builder().build() val analyzer: ImageAnalysis.Analyzer = MlKitCodeAnalyzer( barcodeListener = onData, )

Также важно выставить backpressure-стратегию. STRATEGY_KEEP_ONLY_LATEST в нашем случае подходит идеально, так как модель распознания кодов из mlKit работает весьма шустро:

val imageAnalysis = ImageAnalysis.Builder() .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST).build()

Затем биндим наш imageAnalysis к cameraProvider:

cameraProvider.unbindAll() camera = cameraProvider.bindToLifecycle( activity as LifecycleOwner, CameraSelector.DEFAULT_BACK_CAMERA, preview, imageAnalysis )

Результат и выводы

Пройдя все этапы работы, запускаем небольшое демоприложение с использованием нашего плагина и наслаждаемся результатом. Нативная часть успешно распознает EAN-9 и EAN-13 штрихкоды, передает изображение и результат сканирования во Flutter-приложение.

Какой профит мы предоставили клиенту и его бизнесу?

Экономия времени = денег. За счет обновления сканера мы смогли уменьшить время, затрачиваемое пользователем на ввод 1-D/2-D кода. Чтобы убедиться в этом, проведем синтетический тест: При работе старой версии сканера на чтение 20 штрихкодов ушло 39 секунд 475 миллисекунд. После обновления приложения при потоковом чтении процесс занял 20 секунд 086 миллисекунд. Скорость увеличилась в 2 раза. Еще один тест мы провели, чтобы сравнить работу обновленного сканера с ручным вводом. На ручной ввод 20 штрихкодов ушло 4 минуты, 30 секунд и 11 миллисекунд. Скорость увеличилась в 13,4 раза. Кажется, разница небольшая. Но если у вас тысячи товаров на складе, а время сотрудника стоит денег, это заметный рост скорости работы. Универсальность. Решение делает возможным использование и поддержку специфичных устройств, таких как ТСД. Для этого надо создать свою реализацию интерфейса Scanner и подставить ее в момент определения устройства: override fun onAttachedToActivity(activityBinding: ActivityPluginBinding) { activity = activityBinding.activity val deviceInfo = ""${Build.MANUFACTURER} ${Build.MODEL} ${Build.DEVICE}"" Log.i(""ExampleBarcodeScanner"", deviceInfo) scanner = when { // TODO здесь можно добавить создание объекта реализации [Scanner] под конкретный ТСД else -> textureRegistry?.let { textureRegistry -> return@let CameraScanner(textureRegistry) } } try { activity?.let { activity -> scanner?.onActivityAttach(activity) } } catch (e: Throwable) { Log.e(""ExampleBarcodeScanner"", deviceInfo, e) } }

Снижение влияния человеческого фактора. Мы нивелируем риск ошибки, когда сотрудник склада может ввести неверный штрихкод. Но важно отметить, что желательно оставить пользователю опцию ввести данные вручную, так как код может быть поврежден и его нельзя считать устройством.

Типобезопасность. Использование Pigeon устраняет необходимость сопоставления строк между хостом и клиентом для имен и типов данных сообщений. Сгенерированный код читается и гарантирует отсутствие конфликтов между несколькими клиентами разных версий. Поддерживаемыми языками являются Objective-C, Java, Kotlin и Swift (с взаимодействием Objective-C). Поддержка крупных 2-D кодов. При небольшой доработке возможна обработка 2-D кодов, содержащих большой объем информации, невозможной для ввода человеком (QR-код, DataMatrix и т.д.).

Отмечу, что для написания такого плагина нужны компетенции не только в Flutter, но и в iOS/Аndroid. Если таковых нет, то попросите помощи у своих коллег из соответствующих направлений. Они сделают свою платформенную магию, а вам лишь останется «примотать» их код к Flutter-приложению. Это требует несравнимо меньше затрат, чем работа отдельных команд для каждой платформы.

Тем, кто хочет изучить исходный код подробнее, добро пожаловать на GitHub .

Спасибо за внимание!"'https://habrastorage.org/getpro/habr/upload_files/b90/39a/18f/b9039a18fd488c2a9e9feb69f22c7b5d.png'"['https://habrastorage.org/r/w1560/getpro/habr/upload_files/783/25a/f17/78325af17a5f3b099e98e87f4facce9a.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/20a/d9a/a24/20ad9aa2427bff3765bf2204bc1c6052.png', 'https://habrastorage.org/getpro/habr/upload_files/b90/39a/18f/b9039a18fd488c2a9e9feb69f22c7b5d.png', 'https://habrastorage.org/getpro/habr/company/da7/fca/ca8/da7fcaca8b145a0f223a8d22b6097f5b.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w32/getpro/habr/avatars/873/25e/5e8/87325e5e8ffc436438c86d53ec87ffc2.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/0ae/8c0/b73/0ae8c0b7323b6cb2bc4d359385c276a6.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/876/dfc/f9f/876dfcf9f72b74de7deb853ae26ef765.png', 'https://habrastorage.org/getpro/habr/avatars/873/25e/5e8/87325e5e8ffc436438c86d53ec87ffc2.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/2bd/622/854/2bd622854776c6863db7b4c964b39210.png']"
11'719994'Управление инфраструктурой с помощью terragrunt (terraform) и gitlab ci'В этом посте: Использование terraform модулей Организуем структуру каталогов с terraform модулями для terragrunt согласно вашей инфраструктуре Создание/Обновление/Удаление инфраструктуры одной...'https://habr.com/ru/post/719994/'"В этом посте:

Использование terraform модулей

Организуем структуру каталогов с terraform модулями для terragrunt согласно вашей инфраструктуре

Создание/Обновление/Удаление инфраструктуры одной terragrunt командой

Настройка в gitlab ci для запуска и сохранения terraform lock и state в gitlab

Бекап terraform state из gitlab

Terraform — это инструмент для управления инфраструктурой в облаке с использованием описания в виде кода.

Позволяет автоматизировать процесс развертывания и изменения инфраструктуры в облаке, предоставляя более надежное и прозрачное управление инфраструктурой.

Terragrunt — это обертка для Terraform, позволяющая решать проблемы, связанные с масштабированием и переиспользованием кода для настройки инфраструктуры.

Он позволяет повторно использовать конфигурационные параметры и поддерживает многоуровневые конфигурации и зависимости.

GitLab CI (Continuous Integration) — это система интеграции продолжения, которая позволяет пользователям автоматизировать процессы сборки, тестирования и деплоя для проектов, использующих репозиторий GitLab.

Он позволяет разработчикам начать проект быстро и просто, а также иметь возможность использовать настраиваемые пайплайны для автоматизации процессов сборки и деплоя.

Использование terraform модулей

Terraform модули — это предварительно настроенные компоненты Terraform, которые можно использовать для создания и управления различными инфраструктурными компонентами.

Они позволяют вам разбивать сложные конфигурации на повторно используемые компоненты, которые можно использовать для быстрого создания различных инфраструктурных решений.

Посты про terraform модули:

Будут использоваться модули:

Организуем структуру каталогов с terraform модулями для terragrunt согласно вашей инфраструктуре

Можно использовать terraform модули с чистым terraform, но c terragrunt это будет удобнее:

Код будет компактнее, а значит читабельнее

Структура каталогов отражает то, как организована ваша инфраструктура.

Можно создать/обновить/удалить инфраструктуру одной командой

Можно периодически запускать plan и проверять изменилось что-нибудь

Посты про terragrunt:

Будет рассматриваться репозиторий https://gitlab.com/anton_patsev/gitlab_ci_terragrunt

Структура каталогов этого репозитория минимальная в целях объяснения принципов работы terragrunt:

. ├── dev │ ├── env.hcl │ └── group1 │ ├── dns │ │ └── terragrunt.hcl │ ├── group.hcl │ └── vpc-address │ └── terragrunt.hcl ├── README.md └── terragrunt.hcl

Обычно инфраструктуру разделяют на какие-нибудь группы:

dev/uat/prod

vpc1/vpc2

другие варианты

Пример структуры каталогов от самого terragrunt — https://github.com/gruntwork-io/terragrunt-infrastructure-live-example

Рассмотрим HCL конфиги

Рассмотрим корневой terragrunt.hcl.

Корневой terragrunt.hcl нужен для задания общих переменных, параметров.

# корневой terragrunt.hcl locals { # переменные общие для всего проекта env_vars = read_terragrunt_config(find_in_parent_folders(""env.hcl"")) # Функция read_terragrunt_config() используется для чтения конфига env.hcl (group.hcl), group_vars = read_terragrunt_config(find_in_parent_folders(""group.hcl"")) # найденного с помощью функции find_in_parent_folders() в этой директории или вышележащих директориях до корневой директории. project_id = ""43542597"" # project_id в gitlab env = ""dev"" # Переменная, которая описывает директорию dev как dev окружение в этом проекте. username = ""anton_patsev"" # ваш login в gitlab string_path_relative_to_include = join(""."", [replace(""${path_relative_to_include()}"", ""/"", ""-""), ""tfstate""]) # получение из пути dev/group1/dns получить имя tfstate: dev-group1-dns.tfstate } inputs = merge({ # merge в terragrunt используется для объединения входных параметров из нескольких источников. # Это позволяет использовать общие параметры для нескольких конфигураций Terraform, # а также позволяет переопределять параметры для конкретной конфигурации. cloud_id = local.env_vars.locals.cloud_id folder_id = local.env_vars.locals.folder_id network_id = local.group_vars.locals.network_id }) remote_state { # В backend.tf определяется где сохраняется terraform state. В данном случае сохраняется в http сервисе. backend = ""http"" # Параметры для сохранения terraform state config = { address = ""https://gitlab.com/api/v4/projects/${local.project_id}/terraform/state/${local.string_path_relative_to_include}"" lock_address = ""https://gitlab.com/api/v4/projects/${local.project_id}/terraform/state/${local.string_path_relative_to_include}/lock"" unlock_address = ""https://gitlab.com/api/v4/projects/${local.project_id}/terraform/state/${local.string_path_relative_to_include}/lock"" username = local.username # Login доступа к http сервису (в данном случае gitlab) # Пароль доступа берется из переменной окружения lock_method = ""POST"" unlock_method = ""DELETE"" retry_wait_min = 5 } generate = { path = ""backend.tf"" # Создается файл backend.tf в каждом скачанном terraform модуле if_exists = ""overwrite_terragrunt"" } }

Рассмотрим env.hcl.

В директории dev в конфиге env.hcl задаются параметры для dev окружения, например id folder или другие.

locals { cloud_id = ""b1gvct0b630bbm7i7v90"" # cloud-patsevanton folder_id = ""b1g972v94kscfi3qmfmh"" # default }

Рассмотрим group.hcl.

В директории group1 в конфиге group.hcl задаются параметры для обособленной группы (group1), например network_id или другие.

locals { network_id = ""enprkje8ae9b74e0himb"" # default }

Рассмотрим terragrunt.hcl для вызова определенного terraform модуля. В качестве примера возьмем terragrunt.hcl из директории dns.

В директории group1 присутствуют 2 директории: dns и vpc-address. В каждой директории присутствует terragrunt.hcl.

terraform { # Ссылка на terraform модуль и его тег или ветку source = ""github.com/patsevanton/terraform-yandex-dns.git//.?ref=main"" } include { # Этот блок кода используется для поиска и загрузки конфигурационных параметров из родительских папок. # Это позволяет использовать один и тот же код для конфигурации нескольких подсистем. path = find_in_parent_folders() } # Указывается зависимость текущего кода от vpc-address dependency ""vpc-address"" { config_path = ""../vpc-address"" # Указываем где искать vpc-address # Mock_outputs_allowed_terraform_commands используется для того, # чтобы позволить Terragrunt выполнять имитацию команд Terraform для проверки ваших конфигураций без изменения любого реального состояния. # Это позволяет проверять и отлаживать ваши конфигурации до того, как вы будете применять их на самом деле. mock_outputs_allowed_terraform_commands = [""init"", ""validate"", ""plan""] # При выполнении terragrunt init/validate/plan в переменную external_ipv4_address будет подставлено фейковое значение mock_outputs = { external_ipv4_address = ""fake_external_ipv4_address"" } } # Параметры, которые могут быть переданы в terraform модуль. inputs = { description = ""grafana"" zone = ""apatsev.org.ru."" name = ""apatsev-org-ru"" public = true recordset = [ { name = ""grafana1.apatsev.org.ru."" type = ""A"" ttl = 600 data = [dependency.vpc-address.outputs.external_ipv4_address] }, ] }

Запуск terragrunt в каждой директории

cd dev/group1/dns terragrunt apply cd .. cd vpc-address terragrunt apply

Создание/Обновление/Удаление инфраструктуры одной terragrunt командой

Можно создать/удалить инфраструктуру одной командой terragrunt run-all :

cd dev terragrunt run-all apply -auto-approve --terragrunt-non-interactive

Если вы хотите отладить и создать инфраструктуру вручную, необходимо закомментировать remote_state в корневом terragrunt.hcl

Настройка в gitlab ci для запуска и сохранения terraform lock и state в gitlab

Создайте Project access token с именем GITLAB_ACCESS_TOKEN согласно инструкции: Project access tokens

Чтобы передать Personal Access Token в переменную окружения GitLab откройте ваш проект, затем откройте Settings —> CI/CD и разверните Variables.

Создайте новую переменную окружения GITLAB_ACCESS_TOKEN и в качестве ее значения укажите содержимое Personal Access Token.

Для безопасности отметьте созданную переменную как protected.

Так же создайте новую переменную окружения YC_TOKEN и в качестве ее значения укажите содержимое OAuth-токена согласно инструкции: OAuth-токен

Для безопасности лучше использовать IAM-токен сервисного аккаунта.

Бекап terraform state из gitlab

curl --header ""Content-Type: application/vnd.api+json"" --header ""Authorization: Bearer glpat-xxxx"" \ https://gitlab.com/api/v4/projects/43542597/terraform/state/dev-group1-dns.tfstate

Где 43542597 — id проекта, dev-group1-dns.tfstate — название terraform state

Планы"'https://habr.com/share/publication/719994/d240416ce8b878202d5835d458e83c9e/'"['https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w32/getpro/habr/avatars/e18/935/57e/e1893557eeaacf388b0e596d910014c8.jpg', 'https://habr.com/share/publication/719994/d240416ce8b878202d5835d458e83c9e/', 'https://habrastorage.org/getpro/habr/avatars/e18/935/57e/e1893557eeaacf388b0e596d910014c8.jpg']"
12'719982'[Перевод] В 2022 году хакеры взламывали T-Mobile более 100 раз'В течение 2022 года три группы киберпреступников заявили, что смогли получить доступ к внутренним сетям гиганта связи T‑Mobile более чем 100 раз. В каждом случае цель...'https://habr.com/ru/post/719982/'"В течение 2022 года три группы киберпреступников заявили, что смогли получить доступ к внутренним сетям гиганта связи T‑Mobile более чем 100 раз. В каждом случае цель злоумышленников была одной и той же: выманить у сотрудников T‑Mobile доступ к внутренним инструментам компании. Получив доступ, киберпреступники могли перенаправлять текстовые сообщения и телефонные звонки любого пользователя T‑ Mobile на другое устройство.

Приведенная выше информация основана на анализе чатов Telegram трех различных групп киберпреступников. Каждая из этих групп, согласно исследованиям KrebsOnSecurity, является особенно активной и эффективной в «подмене SIM‑карт», что включает в себя временный захват контроля над номером мобильного телефона жертвы.

Все три организации, занимающиеся подменными симками, по прежнему активно работают в 2023 году и ведут бизнес в открытых каналах Telegram. Сами группы и каналы KrebsOnSecurity не озвучивает, чтобы преступники не мигрировали на закрытые серверы, где добывать информацию об их деятельности будет сложнее.

Каждая киберпреступная организация рекламирует получение доступа к системам T‑Mobile схожим образом. Например, любая возможность подмены SIM‑карты объявляется кратким сообщением «Tmobile up!» или «Tmo up!« для участников канала. Кроме того в объявлениях указывают цену за один запрос на подмену SIM‑карты, имя человека, который принимает платеж, и информацию о целевом абоненте.

Клиенту, который приобретает услуги подмены SIM‑карты, нужно указать номер телефона жертвы и серийный номер, привязанный к новой SIM‑карте, которая будет использоваться для получения текстовых сообщений и телефонных звонков с украденного телефонного номера.

Первоначально цель проекта безопасников состояла в том, чтобы подсчитать, сколько раз каждая организация запрашивала доступ к T‑Mobile в течение 2022 года. Это делалось путём каталогизации сообщений «Tmo up!» в обратном порядке с 31 декабря 2022 г.

Но к тому времени, когда они добрались до постов, сделанных в середине мая 2022 года, составлять график по оставшейся части показалось ненужным. Подсчёт показал, что за последние семь с половиной месяцев 2022 года эти группы в совокупности подавали запросы о замене SIM‑карты в T‑Mobile в 104 разных дня — часто несколько групп заявляли о получении доступа в одни и те же дни.

104 дня во второй половине 2022 года, в течение которых различные известные группы подмены SIM-карт требовали доступ к инструментам сотрудников T-Mobile.

KrebsOnSecurity поделился информацией, собранной в рамках своего расследования, с T‑Mobile. Компания отказалась подтвердить или опровергнуть любое из заявленных вторжений.

«Мы постоянно ведём с этим борьбу», — сообщили её представители в заявлении. «Мы продолжали внедрять усовершенствования, обеспечивающие дополнительную защиту от несанкционированного доступа, в том числе усовершенствование средств управления многофакторной аутентификации, укрепление среды, ограничение доступа к данным, приложениям или службам и многое другое. Мы также сосредоточены на сборе данных об угрозах, подобных тем, которыми вы поделились, чтобы преумножить наши усилия».

Tmo up!

Хотя киберпреступники периодически предлагают услуги подмены SIM‑карт и для других операторов мобильной связи, включая AT&T, Verizon и более мелких операторов, такие предложения появляются гораздо реже, чем по T‑Mobile. И предложения по другим операторам значительно дороже.

Цена подмены SIM‑карты клиентов T‑Mobile во второй половине 2022 года варьировались от 1000 до 1500 $, в то время как замена SIM‑карты AT&T и Verizon стоила, как минимум, в два раза больше.

KrebsOnSecurity не известно, были ли реальными инциденты с подменой SIM‑карты, о которых заявляли мошенники. Тем не менее, подавляющее большинство рекламных объявлений о подмене SIM‑карты T‑Mobile, о которых говорится в статье, имели две общие черты, которые отличали их от случайных объявлений о подмене SIM‑карт в Telegram.

Во‑первых, в сообщениях предлагалось использовать взаимно доверенного «посредника» или поставщика услуг эскроу (чтобы обезопасить одну из сторон от мошенничества). Более того, киберпреступники, которые размещали объявления о возможности подмены SIM‑карты, обычно делали это ежедневно или почти ежедневно — часто анонсировали свои предстоящие предложения за несколько часов до публикации сообщения «Tmo up!».

Если бы мошенники реально не имели доступа, если бы они брали с клиентов деньги, но не оказывали заявляемую услугу подмены симки, это было бы сразу заметно. Как минимум, из ответов более опытных и серьезных киберпреступников в том же чате.

В Telegram много людей, утверждающих, что у них есть доступ к подмене SIM‑карт крупных телекоммуникационных компаний, но значительная часть таких предложений — обычное мошенничество, и лтаких персонажей быстро выявляют и банят (в лучшем случае) .

Одна из групп, публиковавших благонадёжные сообщения «Tmo up!» о возможности подмены SIM‑карты для клиентов T‑Mobile, также стабильно писала «Tmo down!», сообщая, что их заявленный доступ к инструментам сотрудников T‑Mobile был обнаружен и отозван мобильным гигантом.

Обзор временных меток сообщений «Tmo up» и «Tmo down» показывает, что, хотя заявленный доступ к инструментам сотрудников обычно длился менее часа, в некоторых случаях он оставался незамеченным в течение нескольких часов или даже дней.

Инструменты Tmo

Как эти группы подмены SIM‑карт получают доступ к сети T‑Mobile? Среди болтовни в их каналах Telegram часто встречаются объявления о том, что им срочно требуется «обзвонщик», то есть специалист по социальной инженерии, который будет убеждать сотрудников мобильного оператора переходить на фишинговый сайт и вводить свои учетные данные.

Эллисон Никсон — главный научный сотрудник нью‑йоркской фирмы по кибербезопасности Unit 221B — говорит, что мошенники обычно звонят сотрудникам на мобильный, притворяясь кем‑то из ИТ‑отдела компании, а затем пытаются заставить человека на другом конце провода посетить фишинговый сайт, который имитирует страницу авторизации сотрудника.

Никсон утверждает, что многие специалисты по безопасности склонны сбрасывать со счетов угрозу голосовых фишинговых атак, считая их «низкотехнологичными» и «маловероятными» угрозами. Сама Эллисон не считает их низкотехнологичными. В наше время фишинг — сложная схема с большим количеством участников. Есть звонящий, у которого на другом конце провода находится сотрудник мобильного оператора. Есть человек, управляющий набором для фишинга. Очень важно быстро раскрутить сотрудника, чтобы охранные компании не успели засечь мошенников. Затем нужно заманить сотрудника на фишинговый сайт и украсть его учетные данные.

Кроме того, обычно в схеме участвует ещё один сообщник. Он использует украденные учетные данные для входа в сервисы сотрудников мобильного оператора. Этот человек также должен заставить своё устройство пройти «Posture checks» (оценка надёжности сети на предмет нивелирования рисков), форму аутентификации устройства, которую некоторые компании используют для того, чтобы убедиться, что каждый вход в систему происходит только с телефонов или ноутбуков, выданных компанией.

Для начинающих преступников с небольшим опытом мошеннических звонков в каналах Telegram доступно множество расшифровок звонков, на примере которых можно обучиться выдавать себя за ИТ‑специалиста в целевой компании и грамотно реагировать на отказ или скептический настрой со стороны сотрудника. Вот фрагмент одного из таких руководств, который недавно появился на одном из каналов по подмене SIM‑карт:

— «Здравствуйте, это Джеймс, звоню из ИТ‑отдела Metro, как поживаете?» — Всё хорошо, как сам? — У меня всё отлично, спасибо, что спросил. Я звоню по поводу заявки, которую мы получили от вас на прошлой неделе. Там говорится, что у вас, ребята, были проблемы с сетевым подключением, которые мешали работе [Microsoft] Edge, не позволяя войти в систему или время от времени выкидывая из сессии. Заявка не обновлялась с момента её создания, поэтому я звоню узнать, актуальна ли ещё проблема…».

Tmo down!

Упомянутые выше данные TMO UP в сочетании с комментариями самих мошенников показывают, что, хотя доступ к инструментам T‑Mobile в середине 2022 года сохранялся несколько часов подряд, частота и продолжительность сессий начали неуклонно снижаться.

T-Mobile отказался обсуждать потенциальные способы борьбы с этими вторжениями в прошлом году. Однако в конце октября 2022 года одна из групп мошенников начала жаловаться на то, что T-Mobile, должно быть, что-то предпринял. Фишинговый доступ к инструментам сотрудников прекращался сразу же после того, как они его получали.

Одна группа даже заподозрила, что служба безопасности T-Mobile начала отслеживать их чаты.

Действительно, временные метки сообщений TMO UP/TMO DOWN одной из групп показывают, что их заявленный доступ часто ограничивался менее чем 15 минутами в течение ноября и декабря 2022 года.

Какой бы ни была причина, график выше ясно показывает, что частота доступа к T-Mobile значительно снизилась во всех трех группах по подмене SIM-карт в последние недели 2022 года.

Ключи безопасности

T‑Mobile US сообщила о доходах в размере почти 80 миллиардов долларов в прошлом году. В настоящее время в ней работает более 71 000 человек в США, и любой из них может стать мишенью для мошенников.

T‑Mobile отказался отвечать на вопросы о том, как он может усилить аутентификацию сотрудников. Но Николас Уивер, исследователь и преподаватель Международного института компьютерных наук Калифорнийского университета в Беркли, считает, что T‑Mobile и все основные поставщики услуг беспроводной связи должны требовать от сотрудников использования физических ключей безопасности при входе в ресурсы компании, потому что они гарантированно блокируют этот вид атаки.

Устройство U2F производства Yubikey.

Наиболее часто используемые ключи безопасности — это недорогие USB‑устройства. Ключ безопасности является формой многофакторной аутентификации U2F (универсальный второй фактор), которая позволяет входить в систему, просто вставив USB‑ключ и нажав кнопку на устройстве. Ключ работает без каких‑либо специальных драйверов.

Устройства U2F для многофакторной аутентификации хороши тем, что даже если сотрудник попытается пройти по фишинговой ссылке на фейковый сайт, системы компании просто откажутся запрашивать ключ безопасности, если пользователь не зашёл на легитимный сайт компании. Попытка входа будет неудачной. Таким образом, второй фактор не может быть скомпрометирован ни по телефону, ни через интернет.

Роль несовершеннолетних в подмене SIM-карт

Преступные группировки склонны вербовать подростков для выполнения отдельных этапов работы. Мошенники часто рекламируют низкоуровневые вакансии в Roblox, Minecraft, других онлайн‑играх.

Они вербуют детей, потому что те наивны, от них можно получить больше, к тому же у них есть юридическая защита, которой нет у людей старше 18 лет. Даже когда несовершеннолетних арестовывают за пособничество в подмене SIM‑карт, как правило, после освобождения они возвращаются к этой деятельности.

В январе 2023 года T‑Mobile сообщила, что «злоумышленник» украл записи примерно о 37 миллионах текущих клиентов, включая их имена, платёжные адреса, адреса электронной почты, номера телефонов, даты рождения и номера учётных записей T‑Mobile.

В августе 2021 года T‑Mobile признала, что хакеры украли имена, даты рождения, номера социального страхования и информацию о водительских правах/удостоверениях личности более 40 миллионов текущих, бывших или потенциальных клиентов, которые подали в компанию заявку на кредит. Об этом нарушении стало известно после того, как хакер начал продавать записи на форуме.

На фоне таких грандиозных взломов любой ущерб от непрерывных атак групп подмены SIM‑карт может показаться незначительным. Но это большая ошибка. Да, подменить могут всего лишь несколько десятков или сотен SIM‑карт в день, но при этом можно выбрать любого клиента из всей базы. То есть да, объём мал, но проблема в прицельности. Группы мошенников могут выявлять состоятельных людей, которым реально есть что терять.

Ещё одна причина, по которой защитники кибербезопасности игнорируют угрозу со стороны этих групп, в том, что мошенники представляются низкоквалифицированными, начинающими хакерами. Может, они и правда технически не слишком подкованы, но когда вы купаетесь в украденной криптовалюте на миллионы, вы можете купить профессионализм. Мошенники, занимающиеся подменой SIM‑карт не столько крадут сценарии других, сколько нанимают людей, чтобы те писали сценарии для них. И им всё равно, кто делает работу, пока они могут красть деньги.

Что ещё интересного есть в блоге Cloud4Y

→ Информационная безопасность и глупость: необычные примеры

→ NAS за шапку сухарей

→ Взлом Hyundai Tucson, часть 1, часть 2

→ Столетний язык программирования — какой он

→ 50 самых интересных клавиатур из частной коллекции"'https://habrastorage.org/getpro/habr/upload_files/2fc/aa5/43b/2fcaa543bd47167e2df8fe8a606cb5e0.png'"['https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/dbc/0a4/a6b/dbc0a4a6b66fd634f21c1736e298355a.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/46e/7cc/0d5/46e7cc0d5cb418a4324809936be69909.png', 'https://habrastorage.org/getpro/habr/upload_files/2fc/aa5/43b/2fcaa543bd47167e2df8fe8a606cb5e0.png', 'https://habrastorage.org/getpro/habr/company/9d5/c09/f08/9d5c09f082a1df6cef1fe1ec3acdd0b9.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/285/02a/6b5/28502a6b5d7b3ccbfdec2edc787fe30a.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/44a/463/dc5/44a463dc544b9d7a169bd10f4bf57c5d.png', 'https://habrastorage.org/getpro/habr/avatars/c2d/d0d/c05/c2dd0dc05834aebeb86bb76e35caeaab.png', 'https://habrastorage.org/r/w32/getpro/habr/avatars/c2d/d0d/c05/c2dd0dc05834aebeb86bb76e35caeaab.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/c7d/020/15a/c7d02015a46abb58a0fa00bb17989f7e.png']"
13'719988'Как решать реальные задачи при помощи ChatGPT'Всем привет! На связи Андрей Нестеров, я занимаюсь компьютерным зрением в  применении к мобильным приложениям в компании Friflex .  Мы создаем продукты по оцифровке спорта, которые...'https://habr.com/ru/post/719988/'"Всем привет! На связи Андрей Нестеров, я занимаюсь компьютерным зрением в применении к мобильным приложениям в компании Friflex .

Мы создаем продукты по оцифровке спорта, которые позволяют распознавать спортивные события и анализировать их.

В этой статье я приведу простейший пример использования ChatGPT в машинном обучении, предложу несколько способов, которые помогут обращаться к ChatGPT для решения реальных задач, а также подчеркну моменты, в которых она совершенно неприменима.

Многие из нас видели большое количество новостей и различных статей про ChatGPT – нейросеть, с которой можно разговаривать, задавать вопросы, а также просить ее помочь в написании кода. Уже больше месяца я пробую использовать ее для рабочих задач и чем глубже изучаю, тем больше открываю ее скрытые возможности.

И так как дисклеймер «данная статья была написана полностью при помощи ChatGPT» больше не является чем-то новым, то по традиции, дадим слово самой ChatGPT рассказать о себе:

«Я – ChatGPT, крупнейшая модель языкового обучения, созданная OpenAI для предоставления ответов на разнообразные вопросы и участия в разговорах на различные темы. Я обучена на огромном количестве текстовых данных и могу генерировать ответы, похожие на человеческие, используя техники обработки естественного языка».

Далее обращаться мы будем к ней на английском языке. Так проще задавать вопросы, связанные с конкретной имплементацией кода.

Построение основ

Начнем наш путь к решению реальных задач при помощи ChatGPT с простейшего примера построения обучающего пайплайна для задачи классификации изображений. В данном случае, мы хотим получить основу, простейший каркас пайплайна, который далее будем усовершенствовать. В качестве примера возьмем стандартную задачу MNIST. Первым шагом попросим ChatGPT создать базовую модель для классификации изображений:

“Construct a CNN model for the MNIST task”

Теперь, когда модель у нас готова, запросим создать пример обучающего пайплайна.

“Give me a short baseline example of tensorflow data loader for image classification task using MNIST as an example”

В конце попросим ее подготовить графики обучения.

“Create plots for training losses and accuracies”

Так в несколько шагов, используя только ChatGPT, мы получили основу обучающего пайплайна на примере MNIST датасета. Теперь пора приступать к задачам, более приближенным к реальности.

Улучшение ответов

В большинстве случаев ChatGPT отлично справляется со стандартными запросами, но как добиться качественного ответа на более сложные вопросы?

Итеративные вопросы

ChatGPT может отвечать на последовательно заданные вопросы, ориентируясь на предыдущий разговор в контексте одного диалога. В том случае, когда вы не до конца уверены, какой ответ хотите получить или какой конкретно вопрос задать, можно последовательно подступать к желаемому результату.

Рассмотрим данную технику на одном примере. Допустим, на определенном этапе обучения мы хотим сохранить состояние оптимизатора, чтобы использовать его для другой нейросетевой модели.

“How to save the state of an optimizer in a keras model?”

Здесь мы можем увидеть стандартный ответ. Однако, в этой задаче стоит цель получить именно состояние оптимизатора, а не всей модели.

Поэтому, чтобы получить нужный для нас ответ, зададим уточняющий вопрос:

“Can I save only the state of the optimizer?”

Результат выглядит уже намного ближе к желаемому. В нашем случае, когда все обучение проходит на Tensorflow, сделаем последнюю итерацию:

“Yes, the same but in tensorflow/keras”

Именно то что и было нужно!

Детализированные запросы

ChatGPT использует большую часть всех ключевых слов, которые вы ей даете. Чем точнее запрос и чем больше в нем содержится контекста – тем больше вероятность получить качественный ответ. Этот способ может отлично подойти, когда вам понятен желаемый результат и методы его получения.

Вспомним наш пример готового бейзлайна, который мы получили в начале. Теперь мы хотим добавить аугментации в процесс обучения. Исходим из того, что мы знаем библиотеку, которую хотим использовать – Albumentations. А также, мы знаем примерный вид решения и методы, которые будут использоваться. В таком случае зададим как можно более конкретный вопрос ChatGPT:

“Create a tensorflow data loader for image classification task, MNIST as an example. Prepare a class called Transformation that gets a list of albumentations, uses __call__ to process a sample with tf.numpy_function”

Получаем отличную обертку для аугментаций и создания Tensorflow датасета, который можем использовать в реальных задачах.

Корректировка ошибок

Несмотря на отличные результаты ChatGPT при точных запросах и при итеративном уточнении, данная нейросеть довольно часто выдаёт ответы, содержащие довольно очевидные ошибки. Такие проблемы вполне поддаются корректировке. В некоторых случаях достаточно сгенерировать выдачу заново. На один и тот же запрос ChatGPT может выдавать различные ответы. Какие-то из них будут лучше прочих. Также можно прямо спрашивать о том, что могло пойти не так. Вбивая в чат ошибку выдачи, ChatGPT подтягивает предыдущий разговор и старается исправиться в нужном направлении.

Например, на один из запросов, с которым мы уже сталкивались, ChatGPT выдала следующее:

“Provide an example of how to use albumentations on MNIST dataset with tensorflow”

Здесь мы получаем очевидную ошибку:

ModuleNotFoundError: No module named 'albumentations.tensorflow'

В данном случае ChatGPT совместила методы из PyTorch’а и Tensorflow и придумала albumentations.tensorflow.ToTensorV2. Задав тот же вопрос, на второй раз мы получаем рабочий вариант без ошибок.



Однако в некоторых случаях ни один из подходов совершенно не работает.

Допустим, мы хотим оптимизировать модель, используя метод прунинга.

“Give me a script for pruning a tensorflow keras model”

Как и в одном из примеров, мы получили код, в котором присутствуют несуществующие методы. При этом, если попробовать итеративно улучшить ответ, получится бесконечный цикл ошибок:

Q:“tf.keras.callbacks has no PruningCallback”

A:“My apologies, you are correct. As of my knowledge cutoff (2021), the PruningCallback was not available in the tf.keras.callbacks module. Instead, you can use the tf.keras.mixed_precision module, which provides a pruning API for both weight and activation pruning.”

…

A:“Sure, here's an example of pruning a TensorFlow Keras model using the MNIST dataset and tf.keras.optimizers.schedules.PolynomialDecay:”

Q:(обнаруживая, что в ответе не существует таких аргументов) I think the arguments in PolynomialDecay are wrong here

A:“You're correct, I apologize for the mistake. Here is the corrected version of the code using the correct arguments for tf.keras.optimizers.schedules.PolynomialDecay:”

Q:“This code gives me this error: RuntimeError: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.”

A:“... Here is an updated version of the code to include the compile step:”

Q:“ValueError: Index out of range using input dim 0; input has only 0 dims for '{{node sequential_1/prune_low_magnitude_dense_2/strided_slice}}”

…

Заключение

В данной статье мы рассмотрели пример построения основ для задачи машинного обучения, исследовали несколько способов обращения с ChatGPT для улучшения результатов выдачи и взглянули на ошибки, которые может выдавать ChatGPT, а после узнали, как с ними работать.

Несмотря на впечатляющий прогресс языковых моделей, ChatGPT не может заменить человека. Она часто совершает ошибки, выдает правдоподобные, но ложные ответы, а иногда совсем не может справиться с задачей. Однако для человека, который чувствует себя уверенно в своей области, этот инструмент может отлично пригодиться для улучшения продуктивности. Совмещая интуицию опытного специалиста и понимание, как работает данная нейросеть, можно:

ускорить свою разработку;

получать быстрые готовые варианты для прототипирования;

добиваться правильного решения сложной проблемы через череду уточняющих запросов.

Используете ли вы ChatGPT в своих задачах? Делитесь вашим опытом в комментариях👇🏻"'https://habrastorage.org/getpro/habr/upload_files/c73/543/5e5/c735435e5c64142440d81f66d27d1ff9.jpg'"['https://habrastorage.org/getpro/habr/upload_files/c73/543/5e5/c735435e5c64142440d81f66d27d1ff9.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/bf6/08d/9fc/bf608d9fc3e6ae3204a5302d5067c9cb.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/1b3/9b8/d9a/1b39b8d9a136531e209323316612c1a9.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/c73/543/5e5/c735435e5c64142440d81f66d27d1ff9.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/74e/554/772/74e554772d711220d18507b8c26fb26d.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/25f/9a8/e81/25f9a8e81726f3e002eace21ce7e52f5.png', 'https://habrastorage.org/getpro/habr/avatars/a99/f15/0b1/a99f150b1a27e7b4cadf4b150ba4a9c4.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/304/803/133/304803133db63375f491a532dfb10855.png', 'https://habrastorage.org/getpro/habr/company/1f4/d70/bc5/1f4d70bc5985fdea306078fe16d2dfbe.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/de1/ce6/7d7/de1ce67d77dadb4b383d683d0a7cb5e9.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/541/e9f/992/541e9f9920a3e6c3ceef92d30ae36157.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/ad0/f08/007/ad0f08007b43677d5d89053d91fb11d6.png', 'https://habrastorage.org/r/w32/getpro/habr/avatars/a99/f15/0b1/a99f150b1a27e7b4cadf4b150ba4a9c4.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/756/f0a/312/756f0a312260d40c9bc5ca698d110f3a.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/dc7/6ea/1df/dc76ea1df1fff16239b121a4c3eaf7ed.png']"
14'719976'ABAP: Как выгрузить данные в MS Word с помощью XSLT-трансформации за 5 простых шагов'Привет, Хабр! Меня зовут Дарья Чувашова, я — руководитель группы отделения SAP-разработки. В процессе моей проектной деятельности мне приходилось сталкиваться с задачами выгрузки...'https://habr.com/ru/post/719976/'"Привет, Хабр! Меня зовут Дарья Чувашова, я — руководитель группы отделения SAP-разработки. В процессе моей проектной деятельности мне приходилось сталкиваться с задачами выгрузки документов в .doc формат и делать это нужно было быстро. При этом эти документы могли быть с совершенно разным форматированием, кучей таблиц, реквизитов и т. д. В SAP для выгрузки в форматы pdf и excel есть удобные инструменты, возможность работать с формулярами и графическими редакторами форм. Для работы с форматом.doc инструментов меньше. В этой статье я расскажу о быстром и самом простом способе выгрузить документ любой сложности.

Почему я решила написать этот «how‑to»? Как я упомянула, задачи по выгрузке файлов в.doc мне приходилось выполнять часто. В какой‑то момент я собрала все лайфхаки и советы по ускорению работы в один материал, а сейчас хочу поделиться им с хабровской аудиторией. Надеюсь, для коллег записи будут полезными. Описанный вариант решения имеет свои особенности, поэтому я постараюсь на примерах продемонстрировать некоторые «узкие» моменты.

Пошаговая инструкция решения вопроса

Шаг 0

В первую очередь нам нужно вспомнить, что.doc формат — это по сути тот же самый XML формат. Поэтому проще всего будет подготовить шаблон в MS Word в нужном формате. Важно заполнить все реквизиты тестовыми данными для примера, это значительно упростит нам жизнь в последующих действиях.

В качестве примера рассмотрим вот такой документ «Счёт‑фактура» в MS Word:

Этот и другие используемые здесь примеры были взяты из открытого источника: https://glavkniga.ru/situations/k505106

Шаг 1

Шаблон необходимо заполнить тестовыми примерами, чтобы проверить, что при заполнении ничего не съезжает, и все реквизиты остаются на месте:

Шаг 2

Так как документ .doc по своей сути – XML, нам ничего не мешает сохранить документ в формате XML: Файл - Сохранить как. Выбираем расширение .xml

Примечание: для большинства задач вполне достаточно формата.doc, он поддерживает ограничения редактирования, элементы управления и т. п.

Для того чтобы открыть данный файл, мне удобно использовать программу Altova XML Spy. Скорее всего нам потребуется проанализировать содержимое, а в данной программе выполнять анализ файла очень удобно за счёт подсветки синтаксиса. Вы, конечно, можете использовать любой другой редактор.

Открываем свой XML, видим примерно такую картину:

После применения команды PrettyPrinter:

Шаг 3

Переходим в SAP. В своём пакете разработки создадим Преобразование:

Выберем трансформацию XSLT:

Видим следующую картину:

Для того, чтобы наша трансформация верно работала, необходимо указать следующий код между тегами <xsl:template match=""/""> </xsl:template>:

<xsl:processing-instruction name=""mso-application"" progid=""Word.Document""> <xsl:text progid=""Word.Document""/> </xsl:processing-instruction>

Теперь можно смело вставить весь XML‑код ниже из нашего документа:

Визуально просматриваю данный XML‑код, обнаруживаю, что часть тегов подсвечивается, как текст:

Вижу, что это произошло из‑за кавычек в наименовании компании (Company), смело их удаляю:

Теперь пытаемся активировать трансформацию. В 90% случаях активация пройдёт успешно.

Но если у вас появятся подобные ошибки,

Предлагаю стереть данные коды, так как они не имеют никакого смысла для генерации документа из SAP.

Удаляем:

После удаления всех кодов трансформация успешно активируется.

Шаг 4

Переходим в программу. Для вызова трансформации и выгрузки файла привожу для примера такой код:

Данный код максимально облегчён для простоты восприятия и предельной наглядности.

После запуска программы в папке C:\TEMP сохранится файл точно в таком же виде, как наш подготовленный шаблон:

При открытии файла может возникнуть следующая ошибка:

Для того, чтобы от неё избавиться, переходим в трансформацию и ищем /word/settings.xml

Избавиться от ошибки мне помогло удаление всего блока <pkg:part … </pkg:part>. Это не повлияло на работоспособность, и файл стал открываться нормально. Без подсветки синтаксиса тяжело искать закрывающий тег, поэтому имеет смысл снова воспользоваться программой Altova XML Spy (в данной программе вы можете удалить лишний код, а затем вставить новую версию в нашу трансформацию).

Удаляем и активируем, проверяем, что ошибка ушла и с файлом всё в порядке.

Шаг 5

Переходим к выгрузке данных из контекста. Начнём с самого простого: выгрузим данные в поле «Продавец»:

Контекст представляет собой структуру c данными, например, вот такую:

Её мы заполняем и подаём в трансформацию как контекст. Далее копируем из файла, заполненного в качестве примера, текст из реквизита «Продавец» и ищем это место в нашей XML:

Вместо данного текста вставляем:

Не забываем указать нужную структуру контекста и сделать выборку данных. Для примера прописываю хардкодом наименование продавца:

Результат трансформации:

Остальные реквизиты заполняем аналогично.

Шаг 6

Как видим, заполненный на Шаге 1 пример нам помогает выполнять быструю навигацию по XML и искать нужные места для доработки.

Отдельную сложность может представлять собой заполнение табличных данных. В структуре контекста имеем вложенную таблицу с данными T_INVOICE. Для вывода данных используем цикл for each. Начнём с 1 строки 1 столбца. Ищем поиском пример «Яблоки» и вставляем код, приведённый чуть ниже.

Теги описания таблицы довольно понятны: <w:tc> </w:tc> — стоблец, <w:tr </w:tr> — соответственно строка, ну и сам текст <w:t> </w:t>.

Если мы хотим каждую строку таблицы из контекста выводить в новую строку таблицы, то цикл ставим перед тегом строки и закрываем после окончания описания строки:

Конец цикла будет обозначен тут:

Так как таблица большая, окончание цикла будет через 400 строк, поэтому удобно воспользоваться опять же программой с подсветкой тегов, таким образом выводим все необходимые элементы таблицы.

Результат:

Видим, что строка автоматически добавилась. Так как нам не нужны старые данные из примера, удалим эти строки из таблицы. Ищем так же по тегам.

В идеале можно было бы в самом шаблоне оставить лишь одну строку для заполнения, тогда лишних действий по удалению колонок не пришлось бы делать. Но я хочу показать неидеальный случай.

Если необходимо вывести данные из таблицы контекста не в каждой строке таблицы, а текстом с переносом, то можем воспользоваться тегом переноса строки <w:br/>, например,

Получим вот такой результат:

Ещё немного полезных рекомендаций

Мы разобрали основные шаги, как сделать выгрузку любого реквизита и заполнить таблицу. При этом необязательно думать о размере шрифта или форматировании, достаточно изначально выстроить необходимые настройки и правки в исходном документе.

Что ещё записано в моих заметках?

Как поменять шрифт быстро, если он перестал подходить? Допустим, мы желаем заменить Arial на Calibri. Для этого в трансформации выполняем поиск Arial — «Заменить все» и вставляем название нового шрифта Calibri.

Как сделать защиту листа и позволить редактировать лишь некоторые реквизиты в выгруженном файле?



Для этого нужно в исходном файле на 1 шаге настроить защиту листа, тогда кодирующие эту операцию теги будут отражены в нашей трансформации.

Примеры исходного кода из статьи можно увидеть в репозитории github.

Данной информации должно быть достаточно, чтобы сделать выгрузку практически любого документа быстро и эффективно."'https://habrastorage.org/getpro/habr/upload_files/3a8/053/f9d/3a8053f9dc85dbc504d2fbd497598e11.jpg'"['https://habrastorage.org/getpro/habr/company/e5e/a0a/be4/e5ea0abe46837719ec89cb7bd367a50d.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/618/9fc/d7a/6189fcd7a6dbebcc58e6283821a64106.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/747/b05/9e8/747b059e80758f5ee8c7d818c2f60353.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/2df/f40/2a0/2dff402a05ac9744d388d31914ca6b3a.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/573/8fc/adb/5738fcadb373bc1b41db9975fef734de.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/a8f/fdf/979/a8ffdf979ed39c24f8113bfa2b7b85ad.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/e71/4e4/b39/e714e4b394efb59a77c6ce03f24aa805.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/ff5/d4d/dbb/ff5d4ddbb2a38c85aa304761e00ad7c6.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/3ca/260/143/3ca260143767d45b9d891fc8835d5984.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/906/601/56c/90660156cc79aea2498d3ad9156a6de8.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/fa2/2e4/e2d/fa22e4e2d5a498eefb4422edb5efb193.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/0aa/1a0/ee0/0aa1a0ee0cb40828e453d6448de1492e.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/237/f6a/83b/237f6a83b5605037a92bed708c005cab.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/3f6/85c/809/3f685c80947c90feee0d870a35b6fdd1.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/2d5/0bc/e9a/2d50bce9af0304210727e798bab1f0ae.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/655/95d/5b4/65595d5b498963ba9661d85ab629f407.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/080/ff1/c04/080ff1c0430c8ece0ea6ea45769e94f3.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/f40/f1a/8e8/f40f1a8e87503c530c52c649f4b3a716.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/fa2/b66/a28/fa2b66a2870f06158e1862f04529305e.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/f86/4d9/50a/f864d950a0d5f0bac375b8ad4330bc09.png', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/c4b/228/1ca/c4b2281ca51b7c8381fe7b7e454ba63c.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/b75/c76/0e6/b75c760e62e9db8ae36b205885bf1d1f.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/9a6/780/d90/9a6780d9050020e0a3d3b7c41deb1b32.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/71f/13d/a25/71f13da25195f8a0b3b1ff73518c2ef4.png', 'https://habrastorage.org/getpro/habr/upload_files/3a8/053/f9d/3a8053f9dc85dbc504d2fbd497598e11.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/25d/fc9/b20/25dfc9b20be427f8e0db6a24b7b1dd4b.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/2cc/dc3/a95/2ccdc3a9546b54f1763732573af962ec.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/028/d1d/65c/028d1d65c0fd3d4cfb0c81a304e6fb67.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/132/8aa/2a8/1328aa2a8360dd3eda55dff4abda1f6e.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/e3d/160/82e/e3d16082e68d894b8f4a5f8cdf3d46d9.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/724/19c/07f/72419c07f010624fe6274788b7c08361.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/f8e/815/563/f8e815563d5b9f111229b2a04dc0d333.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/4a2/b68/495/4a2b68495dedc15afc8d1fa7a596f11e.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/d44/4fe/abb/d444feabb2eaf7c2e6ed5db223469b38.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/867/9aa/629/8679aa629c30a8fbecf5ce8d7aa89ce4.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/d24/f6c/ac0/d24f6cac032d628c354bbcab1e4029ef.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/da3/e68/d7e/da3e68d7e33cefd499ac8b87a17c302b.png']"
15'719828'Запуск на Product Hunt: Как мы заработали первые доллары с ChatGPT'Привет✌ Меня зовут Юра Ребрик , и в этой статье я расскажу вам о своем опыте создания пет-проекта, который мы запустили на Product Hunt и заработали свои первые доллары онлайн. Это мы. Все просто....'https://habr.com/ru/post/719828/'"Привет✌ Меня зовут Юра Ребрик, и в этой статье я расскажу вам о своем опыте создания пет-проекта, который мы запустили на Product Hunt и заработали свои первые доллары онлайн.

Это мы. Все просто.

Cmd J – это расширение для Google Chrome, которое позволяет использовать ChatGPT на любой вкладке без лишних копирований и вставок. Например, при написании блог-поста можно просто выделить параграф, попросить сократить его, нажать ""enter"" и вуаля! Готово.

Идея

С тех пор как появился ChatGPT, я активно использую его для выполнения различных задач. Например, переписываю им свои имейлы, чтобы они звучали более естественно, а также задаю ему вопросы вместо гугла.

Очевидной проблемой для меня было то, что нужно было написать черновик письма, скопировать и вставить его на страницу chat.openai.com, написать свой запрос, получить ответ и затем снова скопировать и вставить его обратно. Это начинает раздражать, если у вас таких писем много.

Представьте себе следующее: вы написали письмо, нажали Cmd+J, выбрали «Улучшить текст», а затем нажали enter – и у вас уже есть классное письмо без ошибок. Кайф!

Создать расширение для Google Chrome, которое легко установить и использовать, типа Spotlight или Raycast для Mac. Именно таков был наш план!

Я пишу письмо используя Cmd J.

Разработка

Я сам отлично умею писать код, но моя экспертиза в основном в бэкенд разработке и Machine Learning. Поэтому я решил найти кого-нибудь, кто специализируется на фронтенд разработке, чтобы мы могли за пару дней собрать первую версию продукта.

К счастью, у меня есть личный блог в Телеграм с несколькими тысячами подписчиков, где я поделился этой идеей. Один из моих друзей, Даниил, её заценил, и мы взялись за работу.

Фронтенд

Мы потратили несколько дней на сборку первой версии приложения, используя JavaScript и пакет cmdk, который предоставляет React компонент с красивым командным меню.

Как новичку в JavaScript и React, мне было интересно прокачать фронтенд скиллы, работая с более опытным разработчиком.

Я был потрясен хаосом, который творится в вебе при работе с текстом, который выделил пользователь. Нет универсального решения, которое позволило бы заменить этот текст на любом сайте. Например, если вы хотите, чтобы это работало в Google Docs, то вам придется написать отдельный код под него.

Кроме того, в React интерфейс и логика часто тесно связаны, что затрудняет распределение задач между разработчиками в небольших проектах. Удобно, когда один разработчик работает над пользовательским интерфейсом, а другой - над бэкендом. В нашей ситуации задачи относительно небольшие и часто перекрываются. В результате разрешение конфликтов в процессе разработки может занять больше времени, чем если бы один разработчик выполнил все задачи самостоятельно.

Бэкенд

Вначале у нас не было бэкэнда, потому что мы могли вызывать API OpenAI напрямую из клиента, что позволяет избежать дополнительной задержки.

Однако, с этим подходом есть несколько проблем. Во-первых, пользователи часто должны входить в свой аккаунт OpenAI, чтобы обновить сессию. Во-вторых, ChatGPT иногда просто недоступен или пользователь может превысить лимит запросов.

Поэтому мы решили ввести платные тарифы, чтобы пользователям можно было не регистрироваться в OpenAI. Мы создали аккаунт на Gumroad для приема платежей, и я накидал бэкэнд на Vercel, который проверяет подписку пользователя и лимиты, а затем перенаправляет его запрос в OpenAI, используя наш API-ключ.

Наша Gumroad страница с платными планами.

Первые пользователи

Когда мы разработали приложение, я опубликовал об этом твит. Не могу сказать, что твит стал вирусным, но мы получили 10 тыс. просмотров в первый день.

Мой первый твит про Cmd J.

Кроме того, я написал статью о том, как использовать приложение, и опубликовал её на хабре и в нескольких других сообществах. Именно так мы и получили первую тысячу установок.

Мы получили первых 1000 пользователей.

Многие люди начали сообщать нам о багах, некоторые из которых были критическими. Неудивительно! Поэтому мы решили их исправить перед запуском на Product Hunt.

Product Hunt

Подготовка

Когда баги были исправлены, мы начали готовиться к запуску на Product Hunt. Мы выбрали вторник для нашего запуска, потому что это популярный день, когда много людей посещают площадку.

Я должен предупредить вас, что вторник является очень конкурентным днём. Это означает, что получить значок «продукт дня» будет сложнее. Если вы хотите получить значок, лучше всего запустить свой продукт в пятницу или выходные дни.

Совет: если вам важен только значок «продукт дня», запускайтесь в выходные дни.

За несколько дней перед запуском я сделал следующее:

Подготовил посты для социальных сетей, таких как Telegram, LinkedIn и Twitter;

Попросил своих друзей, у которых есть собственные каналы в Telegram, помочь мне, поделившись моим постом со своими подписчиками;

Составил список более чем из 100 друзей, которым может быть интересен наш продукт, и кто сможет поддержать наш запуск.

Я слышал, что некоторые команды тратят недели на подготовку к запуску продукта. Я без понятия, что они там делают. Нужно нарисовать красивые картинки в Figma и сделать то, что я упомянул выше. Обычно это занимает 1–2 дня, не больше.

Совет: если вы хотите получить много лайков и бейдж, то хорошей идеей будет написать статью о вашем приложении и попросить людей поддержать вас на Product Hunt. Опубликуйте ее в день запуска. Если ваша статья будет полезна для читателей, они, вероятно, вам помогут.

Многие команды ищут хантера, чтобы он разместил их продукт. Однако я не уверен, что это того стоит. Я уже запускал продукт раньше без хантера и все равно брал продукт дня. Но если вы уже знаете хорошего хантера, который может опубликовать ваше приложение, то почему бы и нет?

Запуск

Если нравится пост, то сделай приятно автору – поставь ❤

Если вы хотите крутой запуск, то очень важно запуститься ровно в 12:01 ночи PST и получить несколько десятков голосов, чтобы ваш продукт сразу поднялся в топ и начал привлекать органику.

Когда все стартовало, мы быстро набрали лайков и встали на первое место. Следующие несколько часов я потратил на ответы на комментарии.

Совет: количество комментариев влияет на общую оценку вашего продукта, поэтому попросите людей оставлять комментарии, если им понравилось ваше приложение.

К сожалению, позже мы стали вторыми. Первое и третье места имели много лайков, но очень мало комментариев, что показалось подозрительным. Моя первая мысль была о том, что они покупали голоса. Немного позже у них начали появляться и комментарии. Но в какой-то момент я заметил, что некоторые люди публикуют комментарии о своём продукте на нашей странице. Как такое вообще возможно?! Это были боты.

Боремся с ботами на Product Hunt.

К счастью, Product Hunt разобрались с этим беспорядком, и третье место получило всего около 150 голосов, а первое место - всего на 45 голосов больше, чем мы. Таким образом, мы заняли второе место с 600 голосами 🎉

Это было трудно. Ненавижу этих чертовых читеров.

Стоит упомянуть, что в тот день сервера OpenAI были недоступны, поэтому некоторое время наше приложение нельзя было использовать 😬

Наше приложение заняло второе место на Product Hunt.

Результаты

Получить второе место хорошо, но что мы имеем на самом деле? Давайте посмотрим на числа, которые мы получили в конце дня:

Просмотры страницы в Chrome Web Store: 3717;

Установки плагина: 1184;

Просмотры страницы Gumroad: 555;

Платные подписки: всего 5.

Необходимо отметить, что это не совсем воронка, поскольку пользователи могут попадать на страницу Gumroad по прямой ссылке или с лендинга.

Мы получили 1184 установки в день запуска.

Однако не нужно быть экспертом, чтобы заметить маленькое количество покупок. У меня есть три гипотезы, почему так произошло:

У нас нет пейвола внутри самого приложения; Бесплатная версия и так достаточно хороша; Многие люди не знакомы с Gumroad, и он их отпугивает.

Нам стоило направить людей сразу на Chrome Web Store вместо лендинга, а затем конвертировать их в платных пользователей уже внутри приложения.

Наши 5 платных подписок 🥲

Итоги

В целом, это был крутой опыт создания продукта, которым я сам постоянно пользуюсь каждый день. И, наконец, JavaScript и Node.js меня больше не отпугивают!

Если вы планируете запустить свой продукт, то я советую использовать Vercel для написания бэкенда, потому что это действительно очень удобно. Это проще чем Firebase или разворачивание своей машины на Digital Ocean.

Также не тратьте слишком много времени на подготовку запуска на Product Hunt: подготовьте посты для социальных сетях и соберите большой список друзей, которые заценят то, что вы делаете. Ну и не забудьте запланировать запуск точно в 12:01 PST.

Удачи!"'https://habrastorage.org/getpro/habr/upload_files/1ad/01a/08a/1ad01a08a51f50f87df33b98f6626ab4.jpg'"['https://habrastorage.org/r/w780q1/getpro/habr/upload_files/260/216/66a/26021666ab4e9f1bf5e18358af5381c2.jpeg', 'https://habrastorage.org/getpro/habr/upload_files/1ad/01a/08a/1ad01a08a51f50f87df33b98f6626ab4.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/c2f/2f7/21e/c2f2f721e0be85a0795a8d25299c7ef3.jpeg', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/695/8b2/1d4/6958b21d4816ba0ea3d781195b16b59c.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/202/060/767/20206076784c5248291cbc5ddbf243be.jpeg', 'https://habrastorage.org/getpro/habr/avatars/5d1/a2c/dcb/5d1a2cdcb963f0734420ee508a81cf70.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/74d/22f/2eb/74d22f2ebec8ff7c6b1a961c9c33b571.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/0b4/4fa/1a5/0b44fa1a559afac0dd7ab1b2f742b44e.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/e67/d96/0cd/e67d960cdeaf08123e1ecd7b6778c2dc.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/b81/0ee/21a/b810ee21a8879893a6008a429f326659.jpeg', 'https://habrastorage.org/r/w32/getpro/habr/avatars/5d1/a2c/dcb/5d1a2cdcb963f0734420ee508a81cf70.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/9cf/545/46f/9cf54546fe476371de6bd5ad0f098ff7.jpeg']"
16'719520'Как геймифицировать аренду серверов со скидкой, связав сайт с внутренней панелью администрирования'Полгода назад в Selectel мы запустили интересный механизм продвижения серверов по скидке, которая растет на глазах потенциального покупателя. Назвали его аукционом — в честь голландского аукциона,...'https://habr.com/ru/post/719520/'"Зачем мы запустили аукцион?

Хотели более гибко управлять сдачей в аренду определенных конфигураций

Искали возможности для снижения цен на сервисы

Нужно было подогревать интерес к конфигурациям с более старым «железом»

В итоге в пределах одной группы конфигураций «железо» отличается по поколениям EL11-SSD:



Intel® Xeon® E-2236 (3.4 ГГц, 6 ядер),

32 ГБ DDR4,

2 × 480 ГБ SSD SATA.

EL42-NVMe:



Intel Xeon E-2386G (3.5 ГГц, 6 ядер),

32 ГБ DDR4,

2 × 960 ГБ SSD NVMe.



Как это работает?

Голландский аукцион на снижение (Dutch auction) — это тип аукциона, где аукционер начинает с высокой цены и постепенно снижает ее, пока не найдется покупатель, готовый купить товар по указанной цене.

Что «под капотом»?

Синхронизация с панелью администрирования

Вывод данных об аукционе в Telegram-канале

Обработка данных с лендинга в панели администрирования

Как забрать желаемый сервер с максимальной скидкой: лайфхаки

Планы по развитию автоматизации продукта

Конкурс: При написании статьи я не смог пройти мимо хайпа вокруг ChatGPT и сгенерил несколько абзацев этого текста с помощью нейросети. Первому, кто угадает в комментариях хотя бы один из этих абзацев, мы начислим бонусные 2 000 рублей на баланс Selectel. Их можно потратить на любую из услуг компании. Пишите ваши варианты в комментариях.

Полгода назад в Selectel мы запустили интересный механизм продвижения серверов по скидке, которая растет на глазах потенциального покупателя. Назвали его аукционом — в честь голландского аукциона, работающего по формату снижения цены относительно изначальной. Я Сергей Ковалёв, категорийный менеджер Selectel, расскажу, зачем мы запустили такую штуку и как она работает «под капотом». А еще дам пару советов, как дождаться максимальной скидки и не упустить желанный сервер.Для самых внимательных — конкурс, в котором можно выиграть бонусные баллы на счет в Selectel, или мерч (если вам не нужны бонусы). Аукцион выделенных серверов — сервис, с помощью которого можно в несколько кликов арендовать нужную конфигурацию с выгодной скидкой. Это пример стратегии win-win: активность полезна и клиентам (они получают сервер с отличной скидкой), и нам как провайдеру инфраструктуры. Перечислю несколько причин, по которым мы придумали аукцион. Если неинтересно, переходите к блоку «Что под капотом?» У нас более 20 000 клиентов — больших и маленьких. Одни могут за один раз арендовать более 100 серверов, а через несколько месяцев, по окончании проекта, сдать их. Другие заказывают серверы под задачи с учетом сезонности, появления нового железа, изменения потребностей и рынка.Иногда случается дисбаланс, и остатки по конфигурациям становятся неоптимальными для нас (кстати, ранее я писал, как мы контролируем количество комплектующих на складе). Мы хотим не допускать таких перекосов, быстро управлять нашим предложением и утилизацией оборудования, в том числе с помощью скидок.При этом просто дать постоянную общую скидку на все серверы мы не могли — могли навредить экономике бизнеса. Нужен был именно менее ригидный механизм. В этом плане то, что мы назвали аукционом, казалось подходящим форматом. Клиенты получали справедливую на данный момент цену на сервер и позитивные эмоции от совершения удачной, выгодной покупки. Кто-то, возможно, даже испытывает небольшой азарт, охотясь за самыми низкими ценами.В Selectel много моделей выделенных серверов — сейчас можно выбирать из более 100 готовых конфигураций. Это и доступные серверы на десктопных CPU, и серверы для баз данных с высокочастотными процессорами, и многоядерные процессоры для виртуализации. Даже серверы с ARM-чипами и 128 ядрами.Когда на рынке появляется новое поколение процессоров, мы стараемся сделать все, чтобы оно как можно быстрее появилось в наших дата-центрах. С появлением новых CPU пополняется список готовых конфигов. Так, после выхода следующего поколения процессоров Intel® Xeon® E мы обновили предложение, попутно добавив в сервер быстрые NVMe-диски.В общем, наше новое «железо» стало конкурировать со старым. Нужно было поддерживать интерес к классным конфигурациям с менее современными комплектующими (например, с процессором предыдущей генерации).Все новое — хорошо забытое старое. Несколько лет назад в Selectel мы уже пытались реализовать подобную акцию — тогда механика аукциона немного отличалась. Случилась пандемия и кризис чипов — от продолжения акции решили отказаться. К слову, сама идея аукциона серверов не уникальна: например, такую механику давно использует Hetzner.Спустя несколько лет решили вернуться к идее только в обновленном виде — появились ресурсы на его развитие. С точки зрения механики выбрали самый прозрачный и понятный для клиента вид аукциона — голландский.Для пользователей аукцион выглядит так .На странице мы видим постоянно снижающуюся цену на ряд серверов и время до окончания аукциона. После нажатия на кнопку «Заказать сейчас» клиент сразу же получает сервер со скидкой. При этом цена на него фиксируется, а итоговая скидка не ограничена по сроку — клиент будет платить сниженную цену за сервер столько времени, сколько этот сервер будет ему нужен.Такой формат аукциона отличается от известного многим формата английского аукциона, где торги начинаются с низкой цены и увеличиваются по мере того, как каждый участник предлагает более высокую цену. Аукцион продолжается до тех пор, пока один участник не предложит самую высокую цену и не уйдет с приобретенным товаром победителем. Вариант с английским аукционом показался нам более сложным в реализации и менее прозрачным для аудитории.В голландском аукционе нет ставок на повышение, ожиданий результатов и прямой борьбы с другими участниками. К сожалению, многие не знакомы с этим форматом аукциона, поэтому некоторым пользователям механизм казался непонятным. Пришлось немного изменить интерфейс и «подсветить» условия покупки серверов.Каждый день мы анализируем статистику продаж и нам известны конфигурации выделенных серверов, которые мы бы хотели задействовать в аукционе. Из списка таких конфигов создаются лоты. На данный момент все лоты добавляются вручную в нашу корпоративную панель администрирования серверами, которую мы называем Seido.Основная часть «расписания» аукционов формируется на несколько дней вперед, однако есть и «горячие замены» — в таком случае лот добавляется внепланово. Когда они случаются? Например, появилось большое число доступных серверов или, наоборот, клиенты заказали все доступные лоты на аукционе. Это делается для того, чтобы предложение всегда было сформировано для пользователей, а также учитывало наши интересы.В нем указываются параметры, необходимые для старта аукциона. Дата окончания аукциона рассчитывается автоматически, в зависимости от стартовой и максимальной скидок, шага и периодичности их применения. Все лоты на аукционах запускаются с ненулевой стартовой скидкой. Канал ведется автоматически с помощью бота, который взаимодействует с API административной панели.После появления лота на аукционе приходит уведомление в Telegram:Также в канале мы уведомляем пользователей о лотах, которые скоро будут сняты с аукциона:Помимо того, что данные выводятся из внутренней системы на лендинг, существует и обратная взаимосвязь. Любой посетитель лендинга может предложить свой вариант конфигурации на аукцион. Для этого оставьте информацию о желаемом сервере в любом удобном для вас формате: ссылку на конфиг на сайте Selectel, название конфигурации или перечень основных характеристик сервера.Заполненная карта-предложение появляется в панели администрирования Seido. Далее мы рассматриваем возможность добавления на аукцион предложенный сервер и, если она есть, добавляем его в течение ближайших дней.Так как аукцион действует уже почти полгода, нам удалось сделать некие статистически подтвержденные выводы о взаимодействии с ним пользователей. На основе этих цифр можно сформулировать некоторые рекомендации для успешного участия в аукционе.Посмотрим на статистику:— Данные о средней скидке и времени выкупа могут подсказать, на каком отрезке аукциона лот, на который вы положили глаз, может увести ваш «конкурент».— Чтобы не следить за страницей аукциона так же маниакально, как за положением своего брокерского счета, подпишитесь на Telegram-канал . Там вы будете получать уведомления о новых лотах, а также о конфигурациях, которые скоро будут сняты с аукциона. Последнее означает, что там накопилась приличная скидка, а сервер до сих пор никто не забрал.Кстати, лоты, которые доживают последние часы на сайты, подсвечиваются:— Пользуйтесь формой «обратной связи», где можно порекомендовать свой конфиг. Это действительно рабочий инструмент для продвижения своего сервера на аукцион. Например, если мы увидим большой спрос на какой-нибудь сервер c GPU, он обязательно появится среди лотов.Сейчас мы добавляем около 20-25 лотов в день, и это занимает время. В планах автоматизация, в рамках которой мы будет только указывать признак конфигураций, по которому отфильтруем возможность их участия в аукционе, и максимальную скидку. Далее алгоритм и великий рандом сами будут формировать расписание аукциона и добавлять лоты.При этом останется возможность ручного управления — например, для оперативного пополнения конфигураций из заявок клиентов.В остальном мы стараемся делать аукцион все более понятным и простым для использования. Поэтому большинство фич направлено именно на это. Например, сейчас думаем о том, как сделать уведомление подписчиков о новых лотах более гибким."'https://habrastorage.org/webt/oc/wk/_c/ocwk_cejmgl5keqnqfxnmu3z9ke.png'"['https://habrastorage.org/r/w1560/webt/1s/ba/37/1sba37a3r-zi73zreblg005mmqo.png', 'https://habrastorage.org/r/w1560/webt/u6/ls/bd/u6lsbdcg9pnieg0okh-ru3hey0g.png', 'https://habrastorage.org/r/w1560/webt/eg/zt/94/egzt94g6blcnpzsehd4xzdoo4g0.png', 'https://habrastorage.org/webt/4h/fa/r7/4hfar71xd5ykrbkaltfvs9_pt84.gif', 'https://habrastorage.org/getpro/habr/company/66a/f7d/039/66af7d03979b6d18654293d8f1e72837.png', 'https://habrastorage.org/r/w1560/webt/1p/ct/xk/1pctxkataqhufi_mwvxkeiynl6w.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w1560/webt/-n/xu/gq/-nxugqmr1-owllnxrpueyxrgrmy.png', 'https://habrastorage.org/r/w1560/webt/xt/qt/1m/xtqt1meszcb3oeiecmki8b6qlda.png', 'https://habrastorage.org/r/w1560/webt/zq/ne/9m/zqne9mfipwqpppot2-hi_5iqnf4.png', 'https://habrastorage.org/getpro/habr/avatars/277/5f0/87b/2775f087b1c04e1e756b8f68fea09858.jpg', 'https://habrastorage.org/r/w1560/webt/mk/gk/fg/mkgkfgjokyy4dz_skhsfsqalsv8.png', 'https://habrastorage.org/r/w1560/webt/oc/wk/_c/ocwk_cejmgl5keqnqfxnmu3z9ke.png', 'https://habrastorage.org/r/w32/getpro/habr/avatars/277/5f0/87b/2775f087b1c04e1e756b8f68fea09858.jpg', 'https://habrastorage.org/webt/oc/wk/_c/ocwk_cejmgl5keqnqfxnmu3z9ke.png']"
17'719864'[recovery mode] «Берите ложки и начинайте дудеть!» — как нейросеть справляется с реальными заказами на бирже копирайтинга'Мы взяли 8 заказов на бирже копирайтинга eTxT и отдали их в работу нейросети. Рассказываю, что из этого получилось и способен ли искусственный интеллект заменить живых авторов. Привет, меня зовут...'https://habr.com/ru/post/719864/'"Мы взяли 8 заказов на бирже копирайтинга eTxT и отдали их в работу нейросети. Рассказываю, что из этого получилось и способен ли искусственный интеллект заменить живых авторов.

Привет, меня зовут Паша Молянов, я руковожу контент-агентством «Сделаем». Когда несколько месяцев назад начался нейросетевой хайп, многие копирайтеры напряглись: стало страшно потерять работу из-за бездушной и практически бесплатной машины. Особенно напряглись копирайтеры из дешевого сегмента, потому что в каждом обсуждении будущего нейросетей на них ставили крест в самую первую очередь.

Мы решили на практике проверить, может ли нейросеть заменить копирайтеров, которые работают на бирже и берут там заказы по цене до 100 рублей за тысячу символов. Сдули пыль с одного из наших древних аккаунтов на бирже eTxT, разослали заявки, получили 8 заказов и начали тестировать.

Первый заказ: много возни — и 40 рублей в кассе

Сначала я решил попробовать нейросеть на чем-то совсем простом. Взял в работу текст на 1000 символов про ложки. Те, которые музыкальный инструмент.

Да, 40 рублей за текст — это не исключительно дешевый заказ, а вполне в местном рынке

Я написал короткую ТЗшку для нейросети и попросил ее написать статью. Результат был печальный. Во-первых, на русском языке chatGPT пишет очень медленно, примерно как я бы сам печатал этот текст.

Во-вторых, в тексте было несколько очень странных фраз: «ложки использовались в качестве инструмента для танцев», «игра музыки», «в древности их (ложки) исполняли мужчины». Но моя самая любимая часть звучит так: «Они были украшены различными орнаментами, такими как колье, вышивка или гравировка». Копирайтеры с бирж, конечно, тоже допускают разные перлы, но это уже перебор =)

Тексты нейросетей можем показать только картинками, чтобы не снижать уникальность

Тогда я поставил задачу написать статью на английском языке — так нейросетка работает шустрее. Потом закинул текст в DeepL, но результат снова не понравился: и деревянные ложки по версии нейросети делаются из металла, и формулировки местами странные. Особенно в конце: «Берите ложки и начинайте дудеть!».

На этом моменте мы уже почти расстроились: ожидания были, что сейчас как посдаем заказы на бирже, как заработаем 40 рублей за минуту, как напишем классную статью на VC! А нам нейросеть предлагает подудеть в ложки.

Решили сделать третью финальную попытку: предложить нейросетке взять информацию из Википедии.

Снова прогнал английский текст через DeepL и отправил его заказчику. Статью приняли практически сразу, честно говоря, сомневаюсь, что заказчик ее вообще читал — скорее всего, просто посмотрел на уникальность в 97% и автоматически принял работу.

В сумме мы провозились где-то час, и заработали 40 рублей. С одной стороны, написать такой текст самостоятельно столько бы и заняло. С другой стороны, мы же тут вроде как высокие технологии тестируем. Чтобы дальше процесс пошел быстрее, мы купили платную подписку на chatGPT за 20 долларов. Нейросетка стала писать быстро даже на русском, а еще сервис перестал блокироваться из-за перегруза серверов.

Вторая попытка — и идеальный результат почти с первого раза

Мы вооружились Pro-версией chatGPT и взялся за следующий заказ: описание услуги «Разработка проекта планировки территории». Заказчику нужен был текст на 1000 символов, платят 25 рублей.

Эта тема уже намного сложнее, чем ложки — обычному копирайтеру час понадобится только на то, чтобы переварить название услуги. И еще пару часов, чтобы написать этот текст, даже если делать рерайтинг из одного источника.

Но нейросеть не привередливая, поэтому мы просто скормили ей тезисы из ТЗ. На первый взгляд, получилось адекватно — но, с другой стороны, тут столько терминов, что сложно понять, не бред ли это на самом деле =)

По ТЗ от заказчика нужно было еще приложить к тексту список документов, необходимых в рамках проекта. Нейросеть не сделала этого в первом варианте текста, поэтому я отдельно попросил ее дать перечень этих документов. Она дала, мы добавили его к основному тексту и отнесли заказчику.

Он его принял без доработок. Надеемся, список этих документов нормальный, а то не хочется, чтобы кто-то из-за нашего эксперимента попал впросак. Но оставим эту ответственность заказчику.

Список документов от нейросети. Если кто-то разбирается в теме, напишите в комментах, насколько он правдивый =)

Так работать понравилось намного больше, чем с первым заказом. Во-первых, не было ощущения, что я бы мог по-быстрому написать этот текст лучше — потому что тема жуть какая профильная и сложная. Во-вторых, на всю работу ушло минут 10-15. Но даже так: 25 рублей за 15 минут — это как работа за 100 рублей в час. Такие вот биржи копирайтинга: даже искуственный интеллект тут сталкивается с суровой действительностью и впахивает за 16к в месяц.

chatGPT и объективизация женщин

Следующий лот — статья «Как подобрать одежду для офиса». 3000 символов, 75 рублей. При первой попытке нейросеть написала заметку на ~1000 символов вместо статьи. Но зато chatGPT поделился своим видением идеальной коллеги-женщины. В Твиттере нейросеть зашеймили бы за объективизацию ↓

«В офисе женщины должны выглядеть красиво, но при этом не вызывать неудобства или расстройство коллег и клиентов» chatGPT

Вторая попытка уже была намного лучше — правда, нейросети очень понравилось слово «профессионально», поэтому она употребила его 9 раз. А еще она изобрела новое слово: «кассуальный». Его я вручную поменял на «кежуал» или «повседневный», а повторы слова «профессионально» трогать не стал. Работу приняли с первого раза, +75 рублей 💸

Полный текст статьи → https://tinyurl.com/3jkhjehu

Немного закусились с нейросетью

В следующей статье нужно было ответить на вопрос «Можно ли оформить авто на человека без прав» на 4к символов за 80 рублей. Сначала мы по старой схеме скормили нейросети ТЗ, но вместо статьи получил ответ: «Нет, оформить машину на человека без прав нельзя, это незаконно».

Пришлось поспорить с нейросетью, чтобы она все же соизволила выдать текст =)

Но даже согласившись написать статью, в тексте нейросеть то и дело вставляла что-то вроде: «Оформить машину без прав нужно так-то и так-то — но лучше все-таки пойти и сначала получить права».

Этот текст заказчик отправил на доработку два раза, но к нейросети тут ноль претензий. Сначала не прошли по уникальности: получилось 87%, а надо было 95%. Но текст подразумевает кучу названий инстанций и документов, которые по определению не могут быть уникальными.

Потом мы исковеркали текст самостоятельно, чтобы довести его до этих 95%, но заказчик снова вернул его на доработку из-за стилистических ошибок. В третий раз пробовать уже не стали.

Еще два текста мы сдали без особых приключений: дали нейросети ТЗ, получили результат, местами подправили итоговый текст и отправили заказчикам. В конечном итоге, заработали 390 рублей. Pro-подписку за 1500 рублей не отбили даже близко =) Но в последних статьях мы набрали неплохой темп: на написание одной статьи уходило не больше получаса. Человеку написать за это время статью на 3-5 тысяч символов нереально.

С какими заказами нейросеть не справилась

Нейросети не удалось выполнить два заказа с биржи: рерайтинг текста о промышленных светодиодных светильниках и статью про оформление источников в дипломной работе.

С рерайтингом не было ничего интересного: я просто отдал нейросети исходный текст, попросил его переписать. Она переписала, но с уникальностью в 57%. Похоже, писать тексты с нуля для нее проще, чем уникализировать уже готовые.

А вот над статья про оформление источников была веселой. Нейросеть смешно подбирала примеры. В статье нужно было показать, как можно оформить книги в списке литературы — и нейросеть стала изобретать несуществующи книги: «Введение в когнитивную психологию» Бергмана, «Психология и педагогика развития» Крылова и даже «Работа с метафорами в когнитивно-поведенческой терапии», которую вместе написали некие Голдфрид М., Хаймовиц Н., Мэршалл Р. Д. & Шапиро Д.

Я погуглил, этих людей не существует =)

Но примеры выглядели очень реалистично, а для обоснования способов оформления источников нейросеть сослалась на реальный документ: ГОСТ Р 7.0.5-2008 «Библиографическая запись. Библиографическое описание».

Но заказчик с биржи закончил эту высокотехнологичную сказку: оставил к тексту комментарии, что оформление источников приводится неправильное и вообще оно регламентируется другим ГОСТом. Увы.

А еще мы попробовали сдать тексты от нейросети клиентам нашего агентства

Мы взяли ТЗ на посты в соцсети от двух наших клиентов — сервис K50 и банк «Открытие» — и отдали их в работу нейросети. Потом спросили клиентов, что они думают про эти тексты и приняли ли бы они их, если бы мы принесли их на полном серьезе.

Для K50 нужно было написать короткий пост на тему «Как понять, стоит ли вкладываться в контекстную рекламу». Вот, что получилось ↓

«Если бы прислали такой текст, я бы точно почувствовала неладное и подумала, что писал автор-новичок. Что-то нашел в интернете, отрерайтил, чтобы читалось нормально, а проверить факты и задать вопросы поленился. Но вариант не безнадежный. Например, мне нравится композиция и то, что в каждому пункту есть примеры. Еще это «запомните» в конце — звучит прям уверенно, можно взять на заметку. Как драфт, который можно доработать и получить нормальный текст — пойдет. Но как готовый пост для канала, который печется о своей репутации, — конечно слабовато». Дарья Комаричева, K50

Для банка «Открытие» нужно было написать пост на тему «Кому полезен брачный договор и как его заключить». Нейросеть выдала такой текст ↓

«В целом, текст нейросети читабельный, но я такое не принял бы. Во-первых, ритм текста «майонезный». Такое чувство, будто читаешь очень кривой перевод зарубежной статьи, в которой потерялись все акценты и ритмичность абзацев. Во-вторых, есть ошибки в фактуре. Например, нейросеть пишет, что договор должен быть «нотариально удостоверен, подписан обеими сторонами и заверен свидетелями». Какими свидетелями? Понятыми? Думаю, пришли вы мне такое, мы потратили бы очень много времени на доработку текста». Артем Рощупка, «Открытие»

Что в итоге

Мы взяли 8 заказов на бирже, из которых совсем плохо получились 2. Еще один заказ мы не сдали из-за чересчур сложных требований по уникальности.

1. Нейросеть пишет не хуже дешевых копирайтеров. Местами она выдает ляпы, но в целом тексты получаются складный и логичный — если не знать, как они создавались, можно подумать, что их писал живой человек.

Особенно полезной нейросеть выглядит для текстов на сложные темы — промышленные светильники, планировка территории и тому подобные. С ними она справляется так же быстро, как и с простыми темами вроде офисной одежды или ложек.

2. С помощью нейросети можно зарабатывать на бирже. Особенно, если получится поставить процесс на поток: взяли 10 заказов, быстренько составили ТЗ и сгенерировали текст, сдали заказ. Скорее всего, сдавать 10 из 10 заказов не получится — но вполне может получаться 7 из 10. Если статьи будут стоить по 100 рублей за штуку, это 700 рублей за 2-3 часа работы — на биржах мало кто столько зарабатывает.

3. С доработками текста у нейросети проблемы. Она не умеет менять небольшие детали в тексте или добавлять объемы в уже готовую работу — вместо этого просто переписывает текст с нуля. Это подбешивает: например, нейросеть может выдать хороший текст, но повторять много раз одно и то же слово. И при доработке она перепишет его полностью вплоть до структуры, а не просто заменит повторы на синонимы.

4. Кажется, в ближайшей перспективе дешевый сегмент копирайтинга себя изживет. Заказчики, которые покупают тексты за 50-100-200 рублей, могут генерировать примерно те же тексты с помощью нейросети за меньшие деньги и, самое главное, с меньшим количеством возни.

Ведь сейчас нужно: составить ТЗ, создать заказ, выставить цену, дождаться заявок, выбрать копирайтера, дождаться, пока он сделает заказ, потом еще и может понадобиться отправить его на доработку. С нейросетью проще: закинул ТЗ → получил результат → перегенерировал пару раз при необходимости → готово.

Биржи не умрут прямо сейчас: вряд ли много контент-менеджеров про них вообще знают — а даже если знают, чтобы пользоваться chatGPT в России, нужно много танцев с бубном.

Но пройдет год — и наверняка появятся аналоги. Ну, или россияне научатся легко обходить географические ограничения =) Сами же нейросети станут совершеннее, и научатся писать более сложные тексты с меньшим количеством косяков.

5. Нормальный копирайтинг пока в безопасности. Пока возможностей нейросети хватает лишь на то, чтобы оформлять в складный текст информацию, которая лежит на поверхности. Текст смотрится нормально (особенно, если не вчитываться), но не более того.

Если же речь идет о сложных материалах, где нужно подбирать правильные формулировки, доставать информацию из экспертов, думать о целях бизнеса и использовать интересные заходы и форматы текста, то нейросеть вряд ли поможет.

Подписывайтесь на мой Телеграм-канал, где я делюсь опытом в контент-маркетинге, диджитале и управлении агентством. Посты пока пишу сам → @molyanov_blog"'https://habrastorage.org/getpro/habr/upload_files/9ad/633/4e9/9ad6334e94a0768e715d3aa1df03ded0.jpg'"['https://habrastorage.org/getpro/habr/upload_files/ad4/2f6/d48/ad42f6d4840d5e82d4784e05262ec933.gif', 'https://habrastorage.org/getpro/habr/avatars/2ea/37a/857/2ea37a857805bc05145559dd68d9a4c6.jpg', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/upload_files/5c9/7a1/155/5c97a11556fb99f0a4b6c347789dc0a1.JPG', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/890/f3f/4ef/890f3f4ef21f445b8555eb88af8e190b.jpg', 'https://habrastorage.org/getpro/habr/upload_files/9ad/633/4e9/9ad6334e94a0768e715d3aa1df03ded0.jpg', 'https://habrastorage.org/r/w32/getpro/habr/avatars/2ea/37a/857/2ea37a857805bc05145559dd68d9a4c6.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/a53/c87/5b1/a53c875b1105051dcc73f12e2e02593f.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/13f/8e0/5eb/13f8e05ebf57b37a1f7cb277899f4171.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/923/523/3cc/9235233cc226da14a59e8438e500c7de.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/204/8e6/a50/2048e6a501ef639feeb76d05443df30a.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/ede/508/081/ede50808102e701861b2f51eacbbe0e2.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/9ad/633/4e9/9ad6334e94a0768e715d3aa1df03ded0.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/d53/f58/5cb/d53f585cb492ddb1c55d651f255de2f2.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/6bb/245/a16/6bb245a16c0db91bb6d2dd9281a23b75.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/f35/bf2/fbc/f35bf2fbc886dce5a60d3a15f9cea25d.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/870/ecf/656/870ecf6568bf857b737f18f068f78160.jpg']"
18'714840'PostgreSQL под капотом. Часть 4. Цикл бэкэнда'Приветствую. Продолжаем изучать работу PostgreSQL под капотом. Сегодня рассмотрим работу главного цикла бэкэнда. Продолжаем с места где остановились  — прямо перед главным циклом. Файл...'https://habr.com/ru/post/714840/'"Приветствую.

Продолжаем изучать работу PostgreSQL под капотом. Сегодня рассмотрим работу главного цикла бэкэнда. Продолжаем с места где остановились — прямо перед главным циклом. Файл входной точки располагается в src/backend/tcop/postgres.c

Обработчик исключений

Для обработки исключений используется setjmp

Перед началом цикла настраивается обработчик исключений и сохраняется точка стека вызовов для возврата при исключениях.

Обработка исключений работает следующим образом:

Сбрасываются флаги (например, DoingCommandRead)

Сбрасываются таймауты

Откатывается текущая транзакция

Библиотека libpq откатывается

Сбрасываются слоты репликации

Контекст памяти сбрасывается

Пользователю отправляется сообщение об ошибке

if (sigsetjmp(local_sigjmp_buf, 1) != 0) { // Обработка исключения } // Сохранение стека для возврата PG_exception_stack = &local_sigjmp_buf; // ... for (;;) { // Главный цикл }

setjmp setjmp — стандартная C библиотека позволяющая делать нелокальные переходы по стеку. Что это значит? Это значит, что можно вернуться вверх по стеку к нужной функции, в отличие от goto, где переход возможен только в пределах текущей фукнции. Работу с ней можно описать следующим кодом: jmp_buf *global_buf; void doSomeStaff(); // Всегда будет возвращать 1 int main() { jmp_buf buf; if (setjmp(&buf) != 0) { printf(""Вернулись обратно!"") return 1; } global_buf = &buf; doSomeStaff(); return 0; } void doSomeStaff() { printf(""Внутри doSomeStaff""); longjmp(global_buf, 1); } В начале создается переменная типа jmp_buf, хранящая контекст стека, для восстановления. Создаются локальная, для восстановления в случае неполадок, и глобальная, для доступа к адресу возврата остальных функций, переменные. После вызывается setjmp, которому передается созданная переменная. Когда нужно будет к ней вернуться, вызывается longjmp, которому передается та же переменная и код, который будет возвращен setjmp. Замечание: setjmp при первом вызове возвращает 0, а в остальных случаях, возвращаться будут коды переданные longjmp. Таким образом, мы можем вечно получать 0 из setjmp, если будем передавать код 0 в longjmp С помощью setjmp/longjmp можно создать свою систему обработки исключений — SJLJ. Существуют и другие способы обработки исключений, например, SEH или DWARF. Их краткое описание можно найти здесь. Как можно реализовать свой обработчик исключений можно посмотреть в этой статье P. S. существуют sigsetjmp/siglongjmp функции. Они работают аналогично, но позволяют учитывать сигналы. Например, сохранять маску игнорирования. Именно этот вариант используется в Postgres, т.к. в нем много работы с сигналами.

Обработка исключений в Postgres В Postgres есть своя инфраструктура для обработки ошибок. В src/include/utils/elog.h определяются макросы для try/catch/finally конструкций: #define PG_TRY() \ do { \ sigjmp_buf *_save_exception_stack = PG_exception_stack; \ ErrorContextCallback *_save_context_stack = error_context_stack; \ sigjmp_buf _local_sigjmp_buf; \ bool _do_rethrow = false; \ if (sigsetjmp(_local_sigjmp_buf, 0) == 0) \ { \ PG_exception_stack = &_local_sigjmp_buf #define PG_CATCH() \ } \ else \ { \ PG_exception_stack = _save_exception_stack; \ error_context_stack = _save_context_stack #define PG_FINALLY() \ } \ else \ _do_rethrow = true; \ { \ PG_exception_stack = _save_exception_stack; \ error_context_stack = _save_context_stack #define PG_END_TRY() \ } \ if (_do_rethrow) \ PG_RE_THROW(); \ PG_exception_stack = _save_exception_stack; \ error_context_stack = _save_context_stack; \ } while (0) Для хранения информации о возврате из‑за исключений используется глобальная переменная PG_exception_stack (src/backend/utils/error/elog.c), которая инициализируется перед началом главного цикла. Использовать try/catch/finally можно так PG_TRY(); { // Ошибка if (!IsValidValue(1)) { ereport(ERROR, (errcode(ERRCODE_SOME_ERRCODE), errmsg(""value is not valid""))); } } PG_CATCH(); { GlobalValue = old_value; } PG_END_TRY(); // =================== PG_TRY(); { fwrite(""Hello, world!"", sizeof(char), 13, handle) } PG_FINALLY(); { fclose(handle); } PG_END_TRY(); Исключение эмулируется путем логирования с уровнем ERROR Но одновременно нельзя использовать PG_CATCH и PG_FINALLY: после разворачивания макросов создается if/else конструкция, причем if — PG_TRY, а else — есть в PG_CATCH и в PG_FINALLY — при использовании catch/finally вместе получится if с 2 else.

После настройки обработчика исключений, мы входим в цикл.

Инициализация/сброс переменных

В начале каждой итерации настраиваются локальные переменные и окружение.

Перед началом принятия и обработки пакета инициализируются переменные:

MessageContext — контекст памяти при обработке входящего сообщения.

input_message — хранение строки входящего запроса.

StringInfo - хранение строк переменной длины Для хранения строк переменной длины существует свой тип данных — StringInfo. Объявлен в src/include/lib/stringinfo.h typedef struct StringInfoData { // Буфер, хранящий строку char *data; // Длина строки int len; // Максимально возможная длина строки, // т.к. память была аллоцирована через palloc int maxlen; // Переменная для нужд других функций. // Например, отслеживание текущий позиции считывания int cursor; } StringInfoData; typedef StringInfoData *StringInfo; Этот тип используется во всем коде проекта: Буфер plain строки запроса.

Выставление названия приложения.

Выполнение запросов внутри приложения (например, для обновления материализованного приложения). И в других местах Например, при выполнении EXPLAIN результат записывается в переменную StringInfo (src/backend/commands/explain.c): // 1584 строка if (es->format == EXPLAIN_FORMAT_TEXT) { appendStringInfo(es->str, "" (cost=%.2f..%.2f rows=%.0f width=%d)"", plan->startup_cost, plan->total_cost, plan->plan_rows, plan->plan_width); }

Сигнализация клиенту о готовности принимать запрос

Пакет ReadyForQuery бэкэнд отправляет, когда хочет уведомить фронтэнд о готовности принимать запросы. Чтобы понимать, нужно ли отправлять этот пакет используется флаг send_ready_for_query.

После инициализации (на первом шаге) он будет выставлен, но затем может быть снят

В начале каждой итерации проверяется этот флаг. Если он установлен:

Отправляется статистика сборщику статистики. Обновляется заголовок программы соответствующим образом. Инициализируются таймеры таймаутов. Отправляются измененные GUC настройки. Отправляется ReadyForQuery пакет. Флаг send_ready_for_query снимается.

Для первых 3 шагов поведение/параметры зависят от состояния транзакции:

Простой в транзакции

Прерванная транзакция

Остальные

Модуль xact - управление транзакциями За управление транзакциями отвечает модуль xact — src/backend/access/transam/xact.c. Состояние транзакции описывается структурой TransactionStateData typedef struct TransactionStateData { FullTransactionId fullTransactionId; /* my FullTransactionId */ SubTransactionId subTransactionId; /* my subxact ID */ char *name; /* savepoint name, if any */ int savepointLevel; /* savepoint level */ TransState state; /* low-level state */ TBlockState blockState; /* high-level state */ int nestingLevel; /* transaction nesting depth */ int gucNestLevel; /* GUC context nesting depth */ MemoryContext curTransactionContext; /* my xact-lifetime context */ ResourceOwner curTransactionOwner; /* my query resources */ TransactionId *childXids; /* subcommitted child XIDs, in XID order */ int nChildXids; /* # of subcommitted child XIDs */ int maxChildXids; /* allocated size of childXids[] */ Oid prevUser; /* previous CurrentUserId setting */ int prevSecContext; /* previous SecurityRestrictionContext */ bool prevXactReadOnly; /* entry-time xact r/o state */ bool startedInRecovery; /* did we start in recovery? */ bool didLogXid; /* has xid been included in WAL record? */ int parallelModeLevel; /* Enter/ExitParallelMode counter */ bool chain; /* start a new block after this one */ bool assigned; /* assigned to top-level XID */ struct TransactionStateData *parent; /* back link to parent */ } TransactionStateData; Верхнеуровневая транзакция хранится в переменной TopTransactionStateData, а текущая в CurrentTransactionState /* * CurrentTransactionState always points to the current transaction state * block. It will point to TopTransactionStateData when not in a * transaction at all, or when in a top-level transaction. */ static TransactionStateData TopTransactionStateData = { .state = TRANS_DEFAULT, .blockState = TBLOCK_DEFAULT, .assigned = false, }; static TransactionState CurrentTransactionState = &TopTransactionStateData; Состояние транзакции описывается 2 перечислениями: TBlockState - общее описание состояние транзакции без деталей /* * transaction states - transaction state from server perspective */ typedef enum TransState { TRANS_DEFAULT, /* idle */ TRANS_START, /* transaction starting */ TRANS_INPROGRESS, /* inside a valid transaction */ TRANS_COMMIT, /* commit in progress */ TRANS_ABORT, /* abort in progress */ TRANS_PREPARE /* prepare in progress */ } TransState; TransState - детализированное состояние транзакции /* * transaction block states - transaction state of client queries * * Note: the subtransaction states are used only for non-topmost * transactions; the others appear only in the topmost transaction. */ typedef enum TBlockState { /* not-in-transaction-block states */ TBLOCK_DEFAULT, /* idle */ TBLOCK_STARTED, /* running single-query transaction */ /* transaction block states */ TBLOCK_BEGIN, /* starting transaction block */ TBLOCK_INPROGRESS, /* live transaction */ TBLOCK_IMPLICIT_INPROGRESS, /* live transaction after implicit BEGIN */ TBLOCK_PARALLEL_INPROGRESS, /* live transaction inside parallel worker */ TBLOCK_END, /* COMMIT received */ TBLOCK_ABORT, /* failed xact, awaiting ROLLBACK */ TBLOCK_ABORT_END, /* failed xact, ROLLBACK received */ TBLOCK_ABORT_PENDING, /* live xact, ROLLBACK received */ TBLOCK_PREPARE, /* live xact, PREPARE received */ /* subtransaction states */ TBLOCK_SUBBEGIN, /* starting a subtransaction */ TBLOCK_SUBINPROGRESS, /* live subtransaction */ TBLOCK_SUBRELEASE, /* RELEASE received */ TBLOCK_SUBCOMMIT, /* COMMIT received while TBLOCK_SUBINPROGRESS */ TBLOCK_SUBABORT, /* failed subxact, awaiting ROLLBACK */ TBLOCK_SUBABORT_END, /* failed subxact, ROLLBACK received */ TBLOCK_SUBABORT_PENDING, /* live subxact, ROLLBACK received */ TBLOCK_SUBRESTART, /* live subxact, ROLLBACK TO received */ TBLOCK_SUBABORT_RESTART /* failed subxact, ROLLBACK TO received */ } TBlockState; Для управления транзакцией используются функции: StartTransactionCommand — старт транзакции CommitTransactionCommand — коммит транзакции AbortCurrentTransaction — прервать транзакцию При отправке клиенту ReadyForQuery пакета было сказано, что первые шаги выполняются в зависимости от состояния транзакции. Конкретно, использовались функции: IsTransactionOrTransactionBlock — Находимся ли мы в транзакции bool IsTransactionOrTransactionBlock(void) { TransactionState s = CurrentTransactionState; if (s->blockState == TBLOCK_DEFAULT) return false; return true; } IsAbortedTransactionBlockState — текущая транзакция прервана bool IsAbortedTransactionBlockState(void) { TransactionState s = CurrentTransactionState; if (s->blockState == TBLOCK_ABORT || s->blockState == TBLOCK_SUBABORT) return true; return false; } Сами первые 3 шага выглядят следующим образом: if (IsAbortedTransactionBlockState()) { set_ps_display(""idle in transaction (aborted)""); pgstat_report_activity(STATE_IDLEINTRANSACTION_ABORTED, NULL); /* Start the idle-in-transaction timer */ if (IdleInTransactionSessionTimeout > 0) { idle_in_transaction_timeout_enabled = true; enable_timeout_after(IDLE_IN_TRANSACTION_SESSION_TIMEOUT, IdleInTransactionSessionTimeout); } } else if (IsTransactionOrTransactionBlock()) { set_ps_display(""idle in transaction""); pgstat_report_activity(STATE_IDLEINTRANSACTION, NULL); /* Start the idle-in-transaction timer */ if (IdleInTransactionSessionTimeout > 0) { idle_in_transaction_timeout_enabled = true; enable_timeout_after(IDLE_IN_TRANSACTION_SESSION_TIMEOUT, IdleInTransactionSessionTimeout); } } else { /* * Process incoming notifies (including self-notifies), if * any, and send relevant messages to the client. Doing it * here helps ensure stable behavior in tests: if any notifies * were received during the just-finished transaction, they'll * be seen by the client before ReadyForQuery is. */ if (notifyInterruptPending) ProcessNotifyInterrupt(false); pgstat_report_stat(false); set_ps_display(""idle""); pgstat_report_activity(STATE_IDLE, NULL); /* Start the idle-session timer */ if (IdleSessionTimeout > 0) { idle_session_timeout_enabled = true; enable_timeout_after(IDLE_SESSION_TIMEOUT, IdleSessionTimeout); } }

Чтение запроса

Так как во время чтения и парсинга пришедшего пакета могут прийти асинхронные сигналы. Чтобы учитывать текущее состояние принятия пакета при обработке сигналов, существует флаг DoingCommandRead. Перед началом получения пакета этот флаг выставляется, а после затирается.

Дальше начинается само чтение запроса.

Для получения запроса используется функция ReadCommand. При работе через сокеты внутри происходит делегирование выполнения функции SocketBackend.

Вначале читается первый байт. На его основании определяются максимальный размер буфера записи и тип режима работы: Simple Query или Extended Query режимы.

Когда чтение началось отменить запрос нельзя, поэтому процесс чтения текущего запроса оборачивается макросами HOLD_CANCEL_INTERRUPTS и RESUME_CANCEL_INTERRUPTS. Грубо говоря, они размечают область, при входе в которую не нужно выполнять команды на отмену запросов.

/* ---------------- * SocketBackend() Is called for frontend-backend connections * * Returns the message type code, and loads message body data into inBuf. * * EOF is returned if the connection is lost. * ---------------- */ static int SocketBackend(StringInfo inBuf) { int qtype; int maxmsglen; /* * Get message type code from the frontend. */ HOLD_CANCEL_INTERRUPTS(); pq_startmsgread(); qtype = pq_getbyte(); if (qtype == EOF) /* frontend disconnected */ { if (IsTransactionState()) ereport(COMMERROR, (errcode(ERRCODE_CONNECTION_FAILURE), errmsg(""unexpected EOF on client connection with an open transaction""))); else { /* * Can't send DEBUG log messages to client at this point. Since * we're disconnecting right away, we don't need to restore * whereToSendOutput. */ whereToSendOutput = DestNone; ereport(DEBUG1, (errcode(ERRCODE_CONNECTION_DOES_NOT_EXIST), errmsg_internal(""unexpected EOF on client connection""))); } return qtype; } /* * Validate message type code before trying to read body; if we have lost * sync, better to say ""command unknown"" than to run out of memory because * we used garbage as a length word. We can also select a type-dependent * limit on what a sane length word could be. (The limit could be chosen * more granularly, but it's not clear it's worth fussing over.) * * This also gives us a place to set the doing_extended_query_message flag * as soon as possible. */ switch (qtype) { case 'Q': /* simple query */ maxmsglen = PQ_LARGE_MESSAGE_LIMIT; doing_extended_query_message = false; break; case 'F': /* fastpath function call */ maxmsglen = PQ_LARGE_MESSAGE_LIMIT; doing_extended_query_message = false; break; case 'X': /* terminate */ maxmsglen = PQ_SMALL_MESSAGE_LIMIT; doing_extended_query_message = false; ignore_till_sync = false; break; case 'B': /* bind */ case 'P': /* parse */ maxmsglen = PQ_LARGE_MESSAGE_LIMIT; doing_extended_query_message = true; break; case 'C': /* close */ case 'D': /* describe */ case 'E': /* execute */ case 'H': /* flush */ maxmsglen = PQ_SMALL_MESSAGE_LIMIT; doing_extended_query_message = true; break; case 'S': /* sync */ maxmsglen = PQ_SMALL_MESSAGE_LIMIT; /* stop any active skip-till-Sync */ ignore_till_sync = false; /* mark not-extended, so that a new error doesn't begin skip */ doing_extended_query_message = false; break; case 'd': /* copy data */ maxmsglen = PQ_LARGE_MESSAGE_LIMIT; doing_extended_query_message = false; break; case 'c': /* copy done */ case 'f': /* copy fail */ maxmsglen = PQ_SMALL_MESSAGE_LIMIT; doing_extended_query_message = false; break; default: /* * Otherwise we got garbage from the frontend. We treat this as * fatal because we have probably lost message boundary sync, and * there's no good way to recover. */ ereport(FATAL, (errcode(ERRCODE_PROTOCOL_VIOLATION), errmsg(""invalid frontend message type %d"", qtype))); maxmsglen = 0; /* keep compiler quiet */ break; } /* * In protocol version 3, all frontend messages have a length word next * after the type code; we can read the message contents independently of * the type. */ if (pq_getmessage(inBuf, maxmsglen)) return EOF; /* suitable message already logged */ RESUME_CANCEL_INTERRUPTS(); return qtype; }

Получение запроса от интерактивного бэкэнда В случае, если был запущен интерактивный бэкэнд, то строка запроса получается не из сокета, а от пользователя напрямую. В этом случае в функции ReadCommand вместо SocketBackend вызывается InteractiveBackend. Внутри эта функция последовательно читает байты из STDIN и добавляет его в результирующую строку, пока не получит сегмент разделителя запросов (EOF, новая строка, двойная новая строка) static int InteractiveBackend(StringInfo inBuf) { int c; /* character read from getc() */ /* * display a prompt and obtain input from the user */ printf(""backend> ""); fflush(stdout); resetStringInfo(inBuf); /* * Read characters until EOF or the appropriate delimiter is seen. */ while ((c = interactive_getc()) != EOF) { if (c == '

') { if (UseSemiNewlineNewline) { /* * In -j mode, semicolon followed by two newlines ends the * command; otherwise treat newline as regular character. */ if (inBuf->len > 1 && inBuf->data[inBuf->len - 1] == '

' && inBuf->data[inBuf->len - 2] == ';') { /* might as well drop the second newline */ break; } } else { /* * In plain mode, newline ends the command unless preceded by * backslash. */ if (inBuf->len > 0 && inBuf->data[inBuf->len - 1] == '\\') { /* discard backslash from inBuf */ inBuf->data[--inBuf->len] = '\0'; /* discard newline too */ continue; } else { /* keep the newline character, but end the command */ appendStringInfoChar(inBuf, '

'); break; } } } /* Not newline, or newline treated as regular character */ appendStringInfoChar(inBuf, (char) c); } /* No input before EOF signal means time to quit. */ if (c == EOF && inBuf->len == 0) return EOF; /* * otherwise we have a user query so process it. */ /* Add '\0' to make it look the same as message case. */ appendStringInfoChar(inBuf, (char) '\0'); /* * if the query echo flag was given, print the query.. */ if (EchoQuery) printf(""statement: %s

"", inBuf->data); fflush(stdout); return 'Q'; }

После завершения принятия сообщения сбрасываются таймауты ожиданий:

IDLE_IN_TRANSACTION_SESSION_TIMEOUT — таймаут ожидания в транзакции;

IDLE_SESSION_TIMEOUT — таймаут ожидания в сессии.

Проверка внешних прерываний (сигналы)

Во время чтения команды от пользователя была заблокирована обработка сигналов. Теперь мы можем проверить наличие пришедших сигналов. За обработку сигналов отвечает функция ProcessInterrupts().

Проверка услования необходимости обработки сигналов Для увеличения производительности делается предположение, что во время чтения пришедшего пакета не возникло никаких сигналов. Для проверки существования ожидающих обработки сигналов используется макрос INTERRUPTS_PEDNDING_CONDITION (src/include/miscadmin.h) /* Test whether an interrupt is pending */ #ifndef WIN32 #define INTERRUPTS_PENDING_CONDITION() \ (unlikely(InterruptPending)) #else #define INTERRUPTS_PENDING_CONDITION() \ (unlikely(UNBLOCKED_SIGNAL_QUEUE()) ? pgwin32_dispatch_queued_signals() : 0, \ unlikely(InterruptPending)) #endif Можно заметить, что условия помечены атрибутом unlikely. Он сигнализирует компилятору о том, что какое‑либо условие скорее всего не будет выполнено. Это позволяет компилятору генерироать более оптимизированный код. unlikely — тоже макрос. Вместе с likely пределяется в src/include/c.h /* * Hints to the compiler about the likelihood of a branch. Both likely() and * unlikely() return the boolean value of the contained expression. * * These should only be used sparingly, in very hot code paths. It's very easy * to mis-estimate likelihoods. */ #if __GNUC__ >= 3 #define likely(x) __builtin_expect((x) != 0, 1) #define unlikely(x) __builtin_expect((x) != 0, 0) #else #define likely(x) ((x) != 0) #define unlikely(x) ((x) != 0) #endif

В начале ProcessInterrupts имеется проверка на нахождение в особенных секциях.

CritSectionCount — количество исполняемых критических секций. Например, во время построение Gist индекса, мы входим в такую критическую секцию (src/backend/access/gist/gistbuild.c)

IndexBuildResult * gistbuild(Relation heap, Relation index, IndexInfo *indexInfo) { // ... START_CRIT_SECTION(); GISTInitBuffer(buffer, F_LEAF); MarkBufferDirty(buffer); PageSetLSN(page, GistBuildLSN); UnlockReleaseBuffer(buffer); END_CRIT_SECTION(); // ... }

InterruptHoldoffCount — количество секций, в которых исполнение обработчиков нежелательно. Например, обработчик SIGALRM входит в такую секцию

static void handle_sig_alarm(SIGNAL_ARGS) { /* * Bump the holdoff counter, to make sure nothing we call will process * interrupts directly. No timeout handler should do that, but these * failures are hard to debug, so better be sure. */ HOLD_INTERRUPTS(); // ... RESUME_INTERRUPTS(); }

В начале ProcessInterrupts проверяется, что обе мы не находимся ни в одной из перечисленных секций.

Внутри функции проверяется, что нужно:

Завершить процесс

Проверить подключение к клиенту

Провести восстановление после конфликта

Отменить запрос

Обработать превышения таймаутов ожидания

Выполнить параллельные запросы

Вывести в лог описание текущего контекста памяти

Отдельно проверяются изменения GUC конфигурации. При их изменении, они отсылаются клиенту.

Обработка входного запроса

Когда пакет получен и все подготовки завершены, мы готовы обрабатывать запрос. На основании первого байта пакета определяются следующие шаги.

Клиент-серверный протокол Для коммуникации фронтэнда и бэкэнда используется собственный клиент‑серверный протокол. Для каждой стороны определены сообщения, которые она может послать и которыми должна отвечать на сообщения другой стороны. Например, на сообщение фронтэнда BIND, бэкэнд должен ответить BindComplete или ErrorResponse. Формат сообщений имеет следующий вид: Первый байт — тип пакета. В документации описывается типом char. Например, для Bind первый байт — «B» 4 байтное число (int32) — размер сообщения, включая само число размера, т. е. 4 + кол‑во байтов полезной нагрузки Сама полезная нагрузка сообщения — для каждого сообщения своя: может быть простой строкой, а может быть комбинацией чисел и строк Описание форматов всех сообщений описано в документации: https://www.postgresql.org/docs/15/protocol‑message‑formats.html При обычной работе обычно используются 2 режима работы: Simple Query и Extended Query Simple Query Документация: https://www.postgresql.org/docs/current/protocol‑flow.html#id-1.10.6.7.4 Первый байт — «Q». Представляет из себя простую строку — без параметров или какой‑либо защиты. При ее обработке запрос представляется простой C‑строкой (вот здесь интерполяция строк и играет злую шутку). Может содержать в себе несколько выражений, разделенных ; . Например, INSERT и SELECT могут передаться в одном сообщении. Extended Query Документация: https://www.postgresql.org/docs/current/protocol‑flow.html#PROTOCOL‑FLOW‑EXT‑QUERY Этот режим можно описать конвейером сообщений: PARSE — подготовить запрос, заранее распарсив его и создав именованное (или безымянный, хранящийся только до следующего безымянного) выражение. В SQL может быть представлен через PREPARE выражение. BIND — создать портал (именованный или безымянный), подставив в него переданные аргументы запроса. EXECUTE — получить следующую порция результатов выполнения портала (следующие строки). Больше о самом протоколе в документации: https://www.postgresql.org/docs/15/protocol.html

Simple Query

Сообщение Simple Query просто исполняется. Но если процесс — WALSender, то выполнение происходит только в случае, если сообщение репликационное.

case 'Q': /* simple query */ { const char *query_string; /* Set statement_timestamp() */ SetCurrentStatementStartTimestamp(); query_string = pq_getmsgstring(&input_message); pq_getmsgend(&input_message); if (am_walsender) { if (!exec_replication_command(query_string)) exec_simple_query(query_string); } else exec_simple_query(query_string); send_ready_for_query = true; } break;

Идентификация репликационных сообщений Для определения того, что пакет содержит запрос для синхронизации, используется функция объявленная в src/include/replication/walsender_private.h extern void replication_scanner_init(const char *query_string); extern void replication_scanner_finish(void); extern bool replication_scanner_is_replication_command(void); Но их реализации в исходном коде не найти. Она генерируется с помощью пары LEX (лексический анализатор) и YACC (парсер) LEX генерирует токены из исходного кода, а YACC создает из пришедших токенов синтаксическое дерево. Файл с лексическим анализатором для репликации располагается в src/backend/replication/repl_scanner.l. Сама функция, используемая для идентификации репликационной команды: /* * Check to see if the first token of a command is a WalSender keyword. * * To keep repl_scanner.l minimal, we don't ask it to know every construct * that the core lexer knows. Therefore, we daren't lex more than the * first token of a general SQL command. That will usually look like an * IDENT token here, although some other cases are possible. */ bool replication_scanner_is_replication_command(void) { int first_token = replication_yylex(); switch (first_token) { case K_IDENTIFY_SYSTEM: case K_BASE_BACKUP: case K_START_REPLICATION: case K_CREATE_REPLICATION_SLOT: case K_DROP_REPLICATION_SLOT: case K_TIMELINE_HISTORY: case K_SHOW: /* Yes; push back the first token so we can parse later. */ repl_pushed_back_token = first_token; return true; default: /* Nope; we don't bother to push back the token. */ return false; } }

Parse

В случае подготовленного запроса нужно не только почитать строку запроса, но и получить все параметры. Так как типы в Postgres являются объектами, то и типы параметров передаются в виде идентификаторов объектов.

case 'P': /* parse */ { const char *stmt_name; const char *query_string; int numParams; Oid *paramTypes = NULL; forbidden_in_wal_sender(firstchar); /* Set statement_timestamp() */ SetCurrentStatementStartTimestamp(); stmt_name = pq_getmsgstring(&input_message); query_string = pq_getmsgstring(&input_message); numParams = pq_getmsgint(&input_message, 2); if (numParams > 0) { paramTypes = (Oid *) palloc(numParams * sizeof(Oid)); for (int i = 0; i < numParams; i++) paramTypes[i] = pq_getmsgint(&input_message, 4); } pq_getmsgend(&input_message); exec_parse_message(query_string, stmt_name, paramTypes, numParams); } break;

Также можно заметить функцию forbidden_in_wal_sender. Эта функция инициирует ошибку, если WALSender получил недопустимую команду - он работает только в режиме Simple Query.

/* * Throw an error if we're a WAL sender process. * * This is used to forbid anything else than simple query protocol messages * in a WAL sender process. 'firstchar' specifies what kind of a forbidden * message was received, and is used to construct the error message. */ static void forbidden_in_wal_sender(char firstchar) { if (am_walsender) { if (firstchar == 'F') ereport(ERROR, (errcode(ERRCODE_PROTOCOL_VIOLATION), errmsg(""fastpath function calls not supported in a replication connection""))); else ereport(ERROR, (errcode(ERRCODE_PROTOCOL_VIOLATION), errmsg(""extended query protocol not supported in a replication connection""))); } }

Bind

Парсинг BIND пакета (как описывается в комментарии) довольно затратный. Поэтому практически вся логика вынесена в отдельную функцию.

case 'B': /* bind */ forbidden_in_wal_sender(firstchar); /* Set statement_timestamp() */ SetCurrentStatementStartTimestamp(); /* * this message is complex enough that it seems best to put * the field extraction out-of-line */ exec_bind_message(&input_message); break;

Execute

Если на вход пришел EXECUTE пакет, то из него получаются название портала для выполнения и количество строк для возврата. Они передаются самой функции выполнения.

case 'E': /* execute */ { const char *portal_name; int max_rows; forbidden_in_wal_sender(firstchar); /* Set statement_timestamp() */ SetCurrentStatementStartTimestamp(); portal_name = pq_getmsgstring(&input_message); max_rows = pq_getmsgint(&input_message, 4); pq_getmsgend(&input_message); exec_execute_message(portal_name, max_rows); } break;

Fastpath function call

В отличие от предыдущих пакетов, бизнес логика FASTPATH частично вынесена наружу. Именно: старт транзакции и переключение контекста памяти.

case 'F': /* fastpath function call */ forbidden_in_wal_sender(firstchar); /* Set statement_timestamp() */ SetCurrentStatementStartTimestamp(); /* Report query to various monitoring facilities. */ pgstat_report_activity(STATE_FASTPATH, NULL); set_ps_display(""<FASTPATH>""); /* start an xact for this function invocation */ start_xact_command(); /* * Note: we may at this point be inside an aborted * transaction. We can't throw error for that until we've * finished reading the function-call message, so * HandleFunctionRequest() must check for it after doing so. * Be careful not to do anything that assumes we're inside a * valid transaction here. */ /* switch back to message context */ MemoryContextSwitchTo(MessageContext); HandleFunctionRequest(&input_message); /* commit the function-invocation transaction */ finish_xact_command(); send_ready_for_query = true; break;

P.S. В комментарии сказано не добавлять код, предполагающий его исполнение в корректном состоянии (не прерванной транзакции), хотя этот комментарий нужно вынести из switch’а, т.к. подобное относится ко всей итерации цикла.

Close

В Расширенном запросе можно создавать порталы и именованные запросы. Они хранятся в памяти, поэтому их стоит закрывать. Для этого используется пакет CLOSE и одноименная SQL команда.

В пакете передаются тип объекта для закрытия и его название.

case 'C': /* close */ { int close_type; const char *close_target; forbidden_in_wal_sender(firstchar); close_type = pq_getmsgbyte(&input_message); close_target = pq_getmsgstring(&input_message); pq_getmsgend(&input_message); switch (close_type) { case 'S': if (close_target[0] != '\0') DropPreparedStatement(close_target, false); else { /* special-case the unnamed statement */ drop_unnamed_stmt(); } break; case 'P': { Portal portal; portal = GetPortalByName(close_target); if (PortalIsValid(portal)) PortalDrop(portal, false); } break; default: ereport(ERROR, (errcode(ERRCODE_PROTOCOL_VIOLATION), errmsg(""invalid CLOSE message subtype %d"", close_type))); break; } if (whereToSendOutput == DestRemote) pq_putemptymessage('3'); /* CloseComplete */ } break;

Describe

От бэкэнда можно получить описание порталов и именованных выражений:

Портал — описание возвращаемых данных (название, тип данных и т. д.).

Выражения — описание передаваемых параметров.

Так же как и в CLOSE, в DESCRIBE пакете передаются тип объекта для описания и его название

case 'D': /* describe */ { int describe_type; const char *describe_target; forbidden_in_wal_sender(firstchar); /* Set statement_timestamp() (needed for xact) */ SetCurrentStatementStartTimestamp(); describe_type = pq_getmsgbyte(&input_message); describe_target = pq_getmsgstring(&input_message); pq_getmsgend(&input_message); switch (describe_type) { case 'S': exec_describe_statement_message(describe_target); break; case 'P': exec_describe_portal_message(describe_target); break; default: ereport(ERROR, (errcode(ERRCODE_PROTOCOL_VIOLATION), errmsg(""invalid DESCRIBE message subtype %d"", describe_type))); break; } } break;

Flush

В процессе работы во внутреннем буфере могла накопиться информация. Для немедленного сброса буфера отправки, определена команда FLUSH

case 'H': /* flush */ pq_getmsgend(&input_message); if (whereToSendOutput == DestRemote) pq_flush(); break;

Sync

SYNC пакет используется для коммита транзакции. Логика работы простая — просто вызывается функция коммита

case 'S': /* sync */ pq_getmsgend(&input_message); finish_xact_command(); send_ready_for_query = true; break;

Terminate

Закрыть соединения можно 2 способами: оборвать соединение и послать пакет закрытия соединения. Оба этих случая учтены.

/* * 'X' means that the frontend is closing down the socket. EOF * means unexpected loss of frontend connection. Either way, * perform normal shutdown. */ case EOF: /* for the statistics collector */ pgStatSessionEndCause = DISCONNECT_CLIENT_EOF; /* FALLTHROUGH */ case 'X': /* * Reset whereToSendOutput to prevent ereport from attempting * to send any more messages to client. */ if (whereToSendOutput == DestRemote) whereToSendOutput = DestNone; /* * NOTE: if you are tempted to add more code here, DON'T! * Whatever you had in mind to do should be set up as an * on_proc_exit or on_shmem_exit callback, instead. Otherwise * it will fail to be called during other backend-shutdown * scenarios. */ proc_exit(0);

Дополнительно

Также в процессе разработки обнаружился случай, когда клиент продолжает посылать пакеты команд связанными с копированием. Такие пакеты просто игноруруются.

case 'd': /* copy data */ case 'c': /* copy done */ case 'f': /* copy fail */ /* * Ac!cept but ignore these messages, per protocol spec; we * probably got here because a COPY failed, and the frontend * is still sending data. */ break;

Также обрабатывается неизвестный тип пакета.

default: ereport(FATAL, (errcode(ERRCODE_PROTOCOL_VIOLATION), errmsg(""invalid frontend message type %d"", firstchar)));

Конец

На этом итерация заканчивается и начинается новая (если не было исключений)"'https://habrastorage.org/getpro/habr/upload_files/50f/f2d/5d4/50ff2d5d4c0c83973a42f7d0718578b7.png'"['https://habrastorage.org/getpro/habr/upload_files/50f/f2d/5d4/50ff2d5d4c0c83973a42f7d0718578b7.png', 'https://mc.yandex.ru/watch/24049213']"
19'719844'Наставничество и ревьюерство как апскилинг для мидла'Плох тот джун, который не мечтает стать мидлом. Быть самостоятельным, справляться с задачами без советов старших коллег. Но и мидл хочет расти дальше — к сеньору. К новым вызовам, новой ответственности и высокой зарплате. У многих мидлов и знаний достаточно, чтобы с работой сеньора справляться. Казалось бы, ничто не мешает перейти на следующую ступень, но не переходится. Где же выход? Не самое очевидное решение — стать наставником или ревьюером. Мы попросили действующих наставников и ревьюеров Яндекс Практикума рассказать, как пополнить их ряды, чем придётся заниматься, а главное — что им это дало и почему стоит попробовать себя в наставничестве. Если интересно, приглашаем под кат.'https://habr.com/ru/post/719844/'"Плох тот джун, который не мечтает стать мидлом. Быть самостоятельным, справляться с задачами без советов старших коллег. Но и мидл хочет расти дальше — к сеньору. К новым вызовам, новой ответственности и высокой зарплате. У многих мидлов и знаний достаточно, чтобы с работой сеньора справляться. Казалось бы, ничто не мешает перейти на следующую ступень, но не переходится. Где же выход?

Не самое очевидное решение — стать наставником или ревьюером . Мы попросили действующих наставников и ревьюеров Яндекс Практикума рассказать, как пополнить их ряды, чем придётся заниматься, а главное — что им это дало и почему стоит попробовать себя в наставничестве. Если интересно, приглашаем под кат.

Опытные специалисты зачастую надолго застревают в мидлах. Ведь от сеньора ждут не только глубоких познаний в технической части, но и софт-скиллы — умения помогать коллегам советами, объяснять, наставлять, делать code review. И если на работе мидлу негде их развивать, он окажется в тупике.

— Вам предстоит обучить новое поколение техномагов.

Однако можно прокачивать скиллы и вне работы. Например, стать наставником для студентов Яндекс Практикума, а в процессе освежить и обновить свои знания, научиться лучше общаться и объяснять. Или попрактиковаться в code review. То есть получить тот самый импульс к развитию, который может вынести разработчика на новый уровень. Слово амбассадорам очевидцам.

Я в IT более 20 лет, в Практикуме веду уже вторую когорту. Совмещать с работой легко: SLA 12 часов — большой промежуток, можно найти время для студентов. И не всегда нужно отвечать им быстро — иногда важно, чтобы они сами научились находить решения.

Когда я пришёл в первую когорту, были студенты, которые задавали примитивные вопросы. Складывалось впечатление, что они вряд ли дотянут до конца. А в итоге студенты удивили: на моих глазах ребята с самых азов вышли в лидеры курса. Меня это очень вдохновило и впечатлило: я сам за собой такой силы воли не замечал.

Я — интроверт, были проблемы с общением, так что переживал, найду ли подход к когорте (там же 50 человек!), смогу ли что-то рассказать на вебинарах. Но Школа наставников мне очень помогла. И в целом курсы Практикума — как одна большая семья: у нас есть неформальные чаты в Телеграме, чаты с наставниками. Благодаря Практикуму я стал увереннее в коммуникации с людьми.

Ян Щербатов Positive Technologies, наставник в Яндекс Практикуме на курсе «Python-разработчик»

Наставничество для мидлов в IT — ступенька в тимлидство. Возможность почувствовать: моё или не моё, нравится ли развивать софт-скиллы или больше по хардам. Это такая безопасная тестовая площадка. А ещё способ усваивать знания. Когда ты просто слушаешь лекцию, то запоминаешь примерно 5% информации из неё. Зато когда рассказываешь сам — усваиваешь 90% знаний.

Юлия Аравина Коуч IT-руководителей и команд, психолог, наставник курсов «Как управлять командой» и «Аргументация», ревьюер на курсе «Начинающие руководители», модератор на курсе «Проектные менеджеры»

Я начинала карьеру в геймдеве, сейчас работаю QA в продуктовой компании. Работу менять не хочу, но задумывалась о дополнительной частичной занятости. Яндекс Практикум мне подошёл из-за гибкого графика.

Занимаюсь дипломными проектами: проверяю, выделяю сильные и слабые стороны, указываю на ошибки, точки роста, подсказываю, что можно улучшить, отвечаю на комментарии. Сначала обращаю внимание на ребят с повторным ревью — понимаю, что они переживают и им важно скорее получить обратную связь. Затем в порядке очереди беру новых ребят.

Ревьюерство требует обширной базы знаний и не подойдёт джунам. Это роль для мидлов и сеньоров, которым не хватает опыта в наставничестве. И если ты занимаешься наймом сотрудников, ревьюерство тоже помогает. Я пообщалась с огромным количеством студентов и уже знаю, какие вопросы задавать на собеседовании, чтобы понять, есть ли у человека перспективы, хочет ли он развиваться, вкладывать ли в него наши ресурсы.

Надежда Киселёва Ревьюер в Яндекс Практикуме в команде QA

Я начинал как преподаватель в колледже, но через год ушёл в бэкенд и далее уже в iOS, сейчас работаю на Кипре. Пришёл в Практикум летом 2022-го, потому что осталась потребность делиться знаниями. Разработчики не идут преподавать из-за разницы в зарплатах, но Практикум как раз и решает эту проблему: можно работать и быть наставником. Ну и для студентов большой буст, что их наставляет реальный человек из IT.

Мы с коллегой-наставником по очереди отвечаем на вопросы, раз в две недели проводим вебинары, а «офлайн» общаемся в чатах. Нравится, что студенты инициативные: приносят какие-то идеи, которых даже в учебнике нет, и мы их обсуждаем. На работе привыкаешь решать всё одинаково, а у студентов другой образ мышления — общение с ними расширяет твои хард-скиллы и инструменты.

Рекомендую наставничество тем, кому интересно расширить кругозор и не засиживаться на одном месте. Например, я уже двух наставников привел сам со своей работы, и они оба теперь в команде сопровождения.

Иван Дмитриев iOS-Developer, наставник на курсе «iOS-разработчик»

Я — бэкендер и пишу сервисы. Всегда мечтал поработать в Яндексе и, когда в июле 2022-го увидел предложение от Яндекс Практикума на роль ревьюера, решил попробовать свои силы.

— Код-ревью заклинаний — это особое искусство.

У нас на факультете ревьюеры закреплены за определёнными студентами. Я проверяю их работы, делаю ревью каждый день. Бывает, студенты предлагают неожиданные решения — проверяю и думаю: «Вау, так тоже можно?!» Например, как-то было условие — массив делить на два, менять указатели. Студент использовал побитовый сдвиг вправо, что гораздо эффективнее обычного деления. Я никогда об этом не задумывался.

Ревьюерство помогло мне прокачать общение, навыки объяснять и давать обратную связь. Подтягиваются и «харды»: на основном месте работы я не взаимодействую с какими-то инструментами, а в Практикуме студенты сдают работы по ним, что не даёт мне подзабыть материал.

Семён Потапов Ревьюер в Яндекс Практикуме на курсе «Python-разработчик»

Я инженер по надёжности платёжных систем в Uber, работаю в Нидерландах. В Практикум пришёл по наводке друга, хотел прокачать наставничество, а ещё посмотреть, как люди без навыков из области Computer Science пробуют себя в IT. Я не просто верю, что люди с разным опытом дадут сфере IT больше возможностей развития — я считаю, что они буквально необходимы индустрии.

Когда в Практикуме собирают когорту, мы договариваемся об удобном для всех наставников времени дежурств в чатах. Договариваемся о проведении вебинаров — кто какие темы ведёт.

Моя самая любимая категория студентов — мамы в декрете: они сидят и до трёх ночи учатся. Мотивируют самоотверженные ученики, которые сделают что угодно, чтобы обрести новую профессию. Нравятся студенты, задающие сложные вопросы.

Яндекс Практикум дал возможность понять, какой формат подачи материала наиболее комфортен для студентов. Результаты этих экспериментов я использую при проведении лекций и воркшопов непосредственно в Uber.

Карен Товмасян Наставник в Яндекс Практикуме на курсе «Python-разработчик»

У меня четыре года коммерческого опыта в вёрстке. Однажды я пошёл учиться на web-факультет Яндекс Практикума, и мне предложили попробовать себя в ревьюерстве. И вот с июля 2021 года я ревьюер вёрстки, а недавно стал и инструктором.

Я нахожу ошибки и помогаю студентам писать оптимальный код в соответствии с best practices, предлагаю другие варианты решения задач, даю фидбэк. Главное — не навредить дезинформацией, чётко формулировать мысли, соблюдая tone of voice, а ещё составлять пулы статей по темам для студентов. Поначалу я много времени тратил на комменты, долго делал ревью, но с практикой это уходит, и длительность ревью сокращается раз в 5–10.

Благодаря ревьюерству я глубже погрузился в работу браузера и доступность. Из софт-скиллов прокачал тайм-менеджмент, обратную связь и коммуникацию, стал проще относиться к чужим ошибкам. А ещё избавился от синдрома самозванца и нездорового перфекционизма.

Софт-скиллы тем важнее, чем более высокую позицию ты занимаешь. Для мидл+ навыки наставничества и тактичного ревью жизненно необходимы, ведь треть дня ты занят именно этим.

Алексей Казаков Ревьюер вёрстки на web-факультете и инструктор модуля бэкенд на web+

Сменив двенадцать мест работы, решил передохнуть и восстановить навыки, сейчас работаю только в Практикуме. У меня сейчас одна когорта, потом, думаю, возьму ещё Python+ (продвинутый курс).

Есть такое название в ML — рекуррентные нейросети: направляемое обучение, когда видишь, как прогрессирует нейросеть на твоих глазах. И в Практикуме так же: смотришь, как студент ничего не знал, а теперь прогрессирует. Приятно. Меня удивляют наши студенты: один ездит по стране и налаживает электростанции, другой — учитель физики, третий был пилотом. Была студентка, учительница английского и испанского, мы с ней общались на английском. Раньше думал, что я социопат и мизантроп, а оказалось, мне нравится со студентами работать. Научился объяснять даже людям с нулевым уровнем.

Кирилл Быков Наставник в Яндекс Практикуме на курсе «Python-разработчик»

Я восемь лет занимался анализом данных, работал продуктовым аналитиком, вёл телеграм-канал и блог про аналитику. Когда в Яндекс Практикуме в 2019 году набирали первую команду сопровождения курса по анализу данных, мне предложили пройти тестовое на вакансию наставника. Менеджер продукта «Аналитик данных» на собеседовании рассказывала, что когда смотрела моё резюме, подумала: «Вау, это же тот чувак, который ведёт телеграм-канал, который я читаю!» А я подумал: «Вау, я теперь буду работать в Практикуме!»

— Мантия мудреца пошивается на вырост и стимулирует рост бороды.

Помню, в первый вебинар у меня холодели руки от страха, но благодаря Школе наставников я теперь уверенно могу выступать на аудиторию в 400–500 человек. Ещё научился объяснять сложные вещи простым языком. Наставничество меняет парадигму мышления: на работе нужно аргументировать своё решение, это бизнес-ориентированная коммуникация, а здесь коммуникация ориентирована на пользу для людей.

Наставничество прокачивает и хард-скиллы. Прежде чем дать ответ студенту, ты сначала сам решаешь задачку, — это помогает чётче формулировать запросы, лучше разбираться в документации. Ты как буфер между джуном и информацией, в процессе впитываешь в себя знания.

Я бы советовал попробовать себя в наставничестве тем, кого привлекает не только техническая сторона дела, но и философская и гуманитарная. Совмещение технологических скиллов и скиллов наставника — это то, что позволяет раскрыть в себе больший потенциал и делает тебя более широкопрофильным специалистом.

Алексей Макаров Руководитель сопровождения и фидбэка в направлении «Анализ данных», Яндекс Практикум

Наставничество — полезная практика для IT-специалистов, мидлов и сеньоров. Это возможность прокачать софт-скиллы, которые не получается улучшить на основной работе, углубиться в хард-скиллы, двигаться вперёд и развиваться дальше по карьерному треку."'https://habrastorage.org/getpro/habr/upload_files/b7f/a0e/fe2/b7fa0efe24deb533df1d046f7a055b31.PNG'"['https://habrastorage.org/getpro/habr/upload_files/4cf/b2d/d76/4cfb2dd764cabb93b125988faf3c09b4.PNG', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/003/990/2b1/0039902b173af58c574037ffe627d134.jpeg', 'https://habrastorage.org/getpro/habr/upload_files/f30/b0a/b74/f30b0ab740ada761d69908db810fffb4.JPG', 'https://habrastorage.org/getpro/habr/upload_files/1a6/961/48e/1a696148e07690e68d181f33235bc763.JPG', 'https://habrastorage.org/getpro/habr/upload_files/b01/46e/99a/b0146e99aa18f176db4a78abc14311bb.PNG', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/390/7ee/a0d/3907eea0d1ee1aa07b77a1a2fb420827.png', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/561/0b2/c21/5610b2c21be9386b56394f704d09732f.jpg', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/upload_files/b7f/a0e/fe2/b7fa0efe24deb533df1d046f7a055b31.PNG', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/d3f/839/534/d3f839534057a25a6a14b2b1ced704e6.jpg', 'https://habrastorage.org/getpro/habr/upload_files/25f/67f/f17/25f67ff17c22693e40aa91c1ff05bf16.JPG', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/68e/84a/022/68e84a022ae9ac2fead4d342bc5eef2c.png', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/7c9/f04/22d/7c9f0422d4106dd8eb19b5fb472217fb.jpeg', 'https://habrastorage.org/getpro/habr/upload_files/83b/acb/644/83bacb6444195d21ce7b6af47eab66b5.JPG']"
