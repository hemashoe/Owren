post_id'post_id'title'description'source_link'body'image'images
0'720986'Синий свет — зеленый свет: релизим без даунтаймов'Привет! Меня зовут Николай Тихонов, я работаю в команде Tinkoff eCommerce. Я начинал как фронтендер, а потом стал писать бэкенд и занялся CI/CD. А дальше — FullStack Ops и руководство процессами и...'https://habr.com/ru/post/720986/'"Привет! Меня зовут Николай Тихонов, я работаю в команде Tinkoff eCommerce. Я начинал как фронтендер, а потом стал писать бэкенд и занялся CI/CD. А дальше — FullStack Ops и руководство процессами и собственной командой. Сегодня расскажу про FrontOps, blue-green и релизы без даунтайма. Эта статья — текстовая адаптация моего доклада для FrontendConf 2022.

С минимальными знаниями Ops-технологий фронтендер может показывать каждому пользователю, что он сделал, и в одиночку реализовывать крутые продукты. А релизы с даунтаймом все еще существуют и не дают клиентам пользоваться продуктом ночью, поэтому в этой статье поговорим про практику blue-green.

Зачем нужен blue-green?

Например, чтобы:

повысить доступность;

предоставить окружение — второй прод, на котором можно без побочных прогонять автотесты и нагрузочное тестирование;

расширить кругозор, а как известно, больше кругозор — больше денег.

В Ops-мире миллиарды технологий — для одного только деплоя их сотни. То, чем обычно пользуются в Tinkoff eCommerce, может сильно отличаться от того, чем пользуетесь вы. Поэтому в статье будет максимально абстрактная теория без особенностей реализации. Возможно, эта информация поможет настоящим продакшенам, которые по какой-то причине не переехали в Kubernetes.

Главные герои: котики, которые разрабатывают невероятные фронтенды для кошачьего банка

Эти два кота помогут в нашем рассказе. Маленький только пришел в индустрию и готов впитывать всю информацию от старших коллег, а большой полон знаний и технической мудрости.

В один чудесный день большой кот решил, что пришло время научить кота-джуна нажимать кнопку деплоя. Да, он еще не понимает, что происходит, но ведь нажать кнопку — это просто.

На радостях маленький кот убедился, что все тесты на тестовом окружении прошли и все работает. Тестировщик сказал, что все супер, значит, можно деплоить. И вот кот нажал кнопку и собрался уходить на обед, но вдруг что-то произошло.

Наверное, многим знакома эта ситуация: котик сломал прод и не понимает, что случилось, ведь все тесты работали. Откатывать сам он боится, поэтому ждет, когда придет большой кот и все откатит.

В итоге прод продолжил жить своей жизнью. Но жизнь маленького котика уже не станет прежней. Ему хочется понять, что пошло не так. А случиться могло многое:

Код недостаточно покрыли тестами или автотестами. Или покрытие тестами в норме, но мы не все протестировали.

Тест и прод сильно различаются. Например, тест гоняется локально на машине разработчика с выходом во внутреннюю сеть, а прод — в Kubernetes. В этом случае 100% что-то пойдет не так.

Нестабильная инфраструктура. К сожалению, рядовой разработчик мало что может с этим сделать. Перепады напряжения, проблемы с сетью, и деплой падает — со всеми бывает.

Простая человеческая ошибка. Мы могли выкатить в прод что-то, что вообще не должно было туда попасть. Например, интеграцию, которую в тестовом режиме уже настроили, а на проде — еще нет. Или не загрузили секреты в продовые конфигурации, и прод лег.

Котик подумал, подошел к большому коту и сказал:

Большой кот призадумался и вспомнил, что, вообще-то, такая штука есть — это blue-green деплоймент. И стал про него рассказывать.

Blue-green to the rescue!

Есть обычный продакшен, присоединенный к обычному интернету, в который приходят обычные люди, — каждый может в него зайти. Чтобы настроить blue-green, поднимают еще один абсолютно идентичный по конфигурации сервер. Если это виртуалка — то еще одну виртуалку. Если сервис в Kubernetes — то еще один такой же.

Сервисы абсолютно одинаковы по конфигурации, но версии приложения на них могут лежать разные, поэтому второй прод и подписан на иллюстрации как лучший. Один из продов назовем зеленым, а второй — синим, чтобы различать, потому что не знаем, на каком из них лежит продакшен.

При этом синий остается настоящим продакшеном, куда ходят люди, а зеленый — запасной вариант. Туда пока что нет никакого трафика, и мы можем заходить туда, делать определенные операции: ломать, чинить, тестировать — все что угодно. И уходить.

На самом деле без разницы, какими цветами вы назовете эти конфигурации, просто прижилось blue-green. Можно называть их хоть black-white — как угодно.

Зеленый сервер называем стейджингом, ведь он для нашего личного использования.

Синий остается продакшеном, эта роль с него пока не снималась. Перед ними поставим балансировщик. Это прокси, который будет проксировать все запросы в синий сервер, потому что это продакшен, куда нам нужно отправлять трафик. Но как это сделать?

На самом деле продакшен не знает, что он продакшен, потому что и синий, и зеленый сервер идентичны. Оба прода равноправны. За их роли отвечает балансировщик, он знает, что прод — тот, что синий. Именно ему мы сообщаем, какой из продов настоящий.

Так происходит релиз:

Допустим, на синем контуре у нас вторая версия приложения, на зеленом — первая. Когда решаем выкатить релиз, просто раскатываем наше приложение на зеленом контуре, куда никто больше не зайдет. Там мы можем гонять автотесты или нагрузочное тестирование прямо на проде. Его не жалко уронить, потому что там нет трафика.

А потом берем и переключаем весь пользовательский трафик с синего на зеленый.

Для этого нам ничего не нужно трогать на серверах, достаточно сказать балансировщику, что прод теперь зеленый. Делается это за несколько секунд. Если на локальной машине, то хватит всего полсекунды. Эту процедуру обычно называют свитчем.

Откаты мгновенны

Даже если после свитча что-то пошло не так — например, к нам пришел разъяренный котик-продакт и сказал, что мы выкатили лишнего или с опечатками, — можно просто нажать свитч еще раз и за считаные секунды откатиться обратно.

Как попасть на стейджинг

Нам нужно как-то сообщить балансировщику, что мы не рядовые пользователи. Чаще всего это разделение делается на уровне кук или хедеров.

Например, если заходим на балансировщик с проставленным хедером X-Variant: Staging, он понимает, что мы — свои, и пускает нас в стейджинг.

И вот спустя какое-то время в нашей бравой команде из двух котов появляется blue-green. Котик с замиранием сердца снова нажимает на свой первый свитч и видит, что все прошло нормально: на проде и в инкогнито ничего не упало. Но у пользователей почему-то ошибка 404. Большой кот говорит: «Откатывай, что-то пошло не так!» Котик откатывает, заходит на прод — снова все нормально. Только у пользователей ненормально — ошибок 404 стало еще больше. Причина — кэш, или lazyload.

Кэширование, или lazyload

Ребята, которые сидели на прошлом проде, продолжали пользоваться приложением после релиза. Например, пытались перейти в какое-то не прогруженное заранее место. Из-за lazyload они получили ошибку 404, потому что на настоящем проде этого чанка уже нет, он остался на стейдже.

В интернете в этом случае предлагают несколько решений.

CDN. Нам не нужно думать о старой статике, если она всегда доступна. Если вытащить статику в CDN, в индексе не будет прямых путей к приложению, когда мы зайдем в чанки. Пропатченный индекс делает так, чтобы все ссылки на чанки вели в CDN. Нам не нужно думать о статике, когда она лежит в левом месте.

Хранилище статики. Не нашли файл на проде? Балансировщик попробует поискать там. Если у нас нет CDN, а есть другое хранилище, например внутренний S3, можно сделать фулбэк на уровне балансера. Когда балансер хочет выдать ошибку 404, он подумает еще раз и сходит либо в это хранилище, либо в стейджинг. Если он найдет этот чанк, то отдаст его, и только если не найдет — выдаст 404.

Жесткая перезагрузка. Версия, чтобы пользователи точно оставались на текущем проде, но делать так не очень хорошо. Можно попробовать сделать fallback на фронте — жесткую перезагрузку. Можем написать обертку над выполнением запросов, которая будет выполнять location.reload в момент возникновения ошибки 404. Этот вариант подходит только в крайнем случае.

Окраска запросов. Некоторые облака, например Amazon Web Services, позволяют «клеить» сессию к серверу, к которому от клиента пришел первый запрос. Если мы зашли на зеленый прод, то всегда остаемся на нем, так как сессия приклеится к нему. Но тогда приложение будет знать, как мы его деплоим, а это плохо. Ведь если вдруг мы продадим свое приложение в другое облако, где нет такой возможности, придется переписывать большую часть приложения, а это расходы.

Backend-for-Frontend без запар с контрактами

Бэкенд для фронтенда — это когда у нас один огромный бэкенд и куча сервисов, которые пытаются к нему прикрепиться. Например, веб-фронт, фронт с мобилки, нативные мобильные приложения и интеграции. Если мы хотим описывать все одним контрактом, это, как правило, не очень хорошо заканчивается. На фронтенде неудобно пользоваться бэкендом, который старается быть удобным для всех.

Фронтендеры в какой-то момент устали от этого и решили написать свой бэкенд, который будет для них удобен и сможет по контрактам кэшировать ответы.

Если вы все-таки используете бэкенд для фронтенда, то вот несколько лайфхаков.

На самом деле, когда есть специальные бэкенды для фронтенда, следить за контрактами бывает лень. Одно поле переименовали в контрактах — уже боль. Многие решают это хорошими инженерными практиками — например, плавными переходами, версионированием и составлением старых эндпойнтов на несколько релизов вперед. Но если у нашего бэкенда только один потребитель — и это мы сами, — то ничего не мешает соединить два приложения в одно и релизить бэкенд вместе с фронтендом. Многие так и делают.

Но обычно релизы все равно не происходят синхронно, потому что фронтенд, как правило, выкатывается быстрее бэкенда и инициализировать в нем нечего. Между этими релизами все равно есть микролаг. Например, фронтенд обгоняет бэкенд на 10 секунд, час или четыре часа.

Релиз всего сразу

Blue-green позволяет спрятать за одним местом балансировки неограниченное количество размеченных сервисов, а не только фронтенд.

Их стоит покрасить в один и тот же цвет — тогда можно и фронтенд, и бэкенд задеплоить с разницей хоть в несколько часов. Они все равно останутся на стейджинге. Попасть туда можно только самостоятельно, пользователей туда не пускают, а потом просто делают свитч.

Все — пользователи ушли на новый фронт и бэк одновременно. Но теперь будет сложнее работать с кэшированием, ведь если у нас подкэшировалось хоть что-то на фронтенде, мы, скорее всего, будем ходить в старый бэкенд.

Когда проблема решена, бэкенды для фронтендов сделаны и приложение продолжает жить своей жизнью. Мы спокойно его релизим, но подходит подросший маленький котик, который уже стал крутым мидлом, и говорит, что слышал про canary-релизы. Он предлагает внедрить их. Большой кот спрашивает: «А что это такое?»

Canary

На самом деле Canary похожа на blue-green.

Тут опубликована схема из blue-green, потому что, если он хорошо спроектирован, его можно легко переделать под Canary.

Здесь не нужны цвета, потому что продакшены будут хоть и одинаковые по конфигурации, но немного разные по размерам. Например, если приложение довольно большое, по три сервера под прод и под стейджинг — на каждый цвет по три сервера.

Один из этих серверов можно оставить под стейджинг, а остальные пять перегнать в прод. Тогда прод будет более устойчивым и выдержит больше нагрузки.

Один оставшийся сервер оставляют под так называемую канареечную версию.

Канареечные релизы — это когда небольшая часть пользователей самостоятельно заходит на стейджинг и смотрит на новый деплоймент. По факту релизы обходятся без нас. По метрикам или просто сидя в чате с алертами можно проверять, что происходит: не упало ли что-то, молчит ли Sentry. Если все хорошо, выкатываем канареечную версию в продакшен, если нет — откатываем изменения.

Еще нужно понимать, что пользователи на самом деле лояльны и готовы к таким релизам. Чтобы это проверить, можно размечать cookie при авторизации. Например, если пользователь залогинился, проставим cookie «лояльный». Балансировщик видит лояльность пользователя и пускает его на стейджинг или в прод. Это делается примерно так же, как в blue-green.

Но здесь снова могут возникнуть проблемы. Например, канареечный прод может упасть и какое-то время лежать. Даже если лояльный пользователь зашел на прод, а он лежит, это не весело. Если мы пользуемся бэкендами для фронтендов, появляется лаг: когда мы раскатываем на продакшен, бэкенд снова может закатиться позже фронтенда. Приходится включать голову, становиться хорошими инженерами, что-то версионировать и придумывать. А не хочется.

Но мы и не будем говорить про Canary, потому что это уже совсем другая история. Если у вас настоящий прод и вы не уехали в Kubernetes, можете их попробовать.

Заключение

Ни для кого не секрет: чтобы стать большим мудрым котом, надо много практиковаться. Еще недавно говорили, что достаточно 10 тысяч часов, сейчас уже говорят о 30 тысячах часов. Но стать мастером своего дела без отличных инструментов невозможно.

Поэтому, если вы еще в начале пути, берите на вооружение blue-green и все остальное, о чем мы говорили. Не стесняйтесь спрашивать и пробовать. Ищите — и обязательно найдете.

А если вы уже большой кот, не забывайте делиться своей мудростью с маленькими. От этого и им польза, и вам профит!"'https://habrastorage.org/getpro/habr/upload_files/f45/682/208/f4568220844fb00377dea1bbf8401e44.png'"['https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/55f/6cd/9af/55f6cd9afcf1763d8e80f944ad81f08e.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/a8a/63d/89b/a8a63d89b75f01dbe14b68143c9ba516.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/b0c/70b/907/b0c70b9075f6ae7706aa1b170575a6fd.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/3ff/3fd/e41/3ff3fde41d4031afd8e8c0ae423b2097.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/335/107/d92/335107d9256a31e270af551d8221a9a0.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/3be/6cb/1ce/3be6cb1cea28652497d435fec43509d3.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/b03/f58/8b6/b03f588b6ead2c38bbc21f08c435f5eb.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/019/ce6/c03/019ce6c034226b6341fc891cb4a31832.jpeg', 'https://habrastorage.org/getpro/habr/branding/2df/b89/248/2dfb89248ab9bc5d3c0794e44d25f33c.png', 'https://habrastorage.org/getpro/habr/upload_files/f45/682/208/f4568220844fb00377dea1bbf8401e44.png', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/c21/915/1fc/c219151fc33d1417a0f3bf7e00718530.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/87f/3cb/c5a/87f3cbc5a8b7f703b2ca74167bc6319e.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/8c7/618/b28/8c7618b288edb81059d234ebcdd5b83f.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/a02/028/744/a020287448d9a3896a2a706353aa26b1.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/6e7/4b2/967/6e74b2967493b7eb9e5a052c2fa61eb1.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/728/845/32d/72884532d6b6c615256a2406c25a6cdc.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/eb8/ba0/31f/eb8ba031f4030ffdcd013bd8706843e3.jpeg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/8ee/9e6/ef5/8ee9e6ef55c3eb830187956a163f3b89.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/f45/682/208/f4568220844fb00377dea1bbf8401e44.png', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/3d8/564/83a/3d856483a52ba5f627b63ba345d2cd52.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/5c6/1e8/36e/5c61e836e3324f868c0e7a7b0270e085.jpeg', 'https://habrastorage.org/getpro/habr/company/6f0/965/c1f/6f0965c1feae8a79dc103989aa6c7e91.png', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/209/086/ca3/209086ca36bb61016d1fd786e73ad552.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/48b/0bc/95a/48b0bc95ab460ffb6ca7c0490dbda3c8.jpeg']"
1'720982'[Перевод] Как вавилонянам удалось вычислить √2 с точностью до шести знаков после запятой?'Эта изготовленная примерно в 1800-1600 годах до нашей эры глиняная табличка свидетельствует, что древние вавилоняне смогли аппроксимировать квадратный корень двух с точностью 99,9999%. Как им это...'https://habr.com/ru/post/720982/'"Расшифровка таблички

Вавилонский алгоритм вычисления квадратного корня

Погрешность вавилонской аппроксимации

Метод Ньютона-Рафсона

Метод Ньютона-Рафсона и вавилонский алгоритм

Скорость сходимости

Недостатки

Вывод

Предполагаем первоначальное значение x₀

Временно заменяем функцию касательной к ней в x₀

Определяем, где касательная пересекает ось X

Используем это пересечение x₁ в качестве новой начальной точки процесса.

Эта изготовленная примерно в 1800-1600 годах до нашей эры глиняная табличка свидетельствует, что древние вавилоняне смогли аппроксимировать квадратный корень двух с точностью 99,9999%.Как им это удалось?Для начала расшифруем саму табличку. Она маркирована как YBC 7289 (сокращённо от «7289-й предмет из Вавилонской коллекции Йеля, Yale Babylonian Collection»). На табличке показан квадрат, его диагональ, а рядом написаны числа. Вот её стилизованная версия из книги Episodes from the Early History of Mathematics Асгера Обое.Как следует из теоремы Пифагора, длина диагонали единичного квадрата равна √2. Давайте разберёмся с символами!На табличке указаны числа, записанные в виде вавилонских клинописных нумералов. Они означают 1, 24, 51 и 10.Так как вавилоняне использовали систему счисления по основанию 60 (также называющуюся шестидесятеричной), число 1,24 51 10 в десятичной системе означает 1,41421296296.Это совпадает со значением √2 до шестого знака после запятой, то есть соответствует точности в 99,9999%!Точность вычислений поражает. Попробуйте воссоздать её без калькулятора, на бумаге, это не так уж просто!И мы расскажем, как им это удалось.Сейчас я буду изображать фокусника: сначала покажу алгоритм, а затем отдёрну занавес и объясню его.Мы начинаем с выбора числа x₀ между 1 и √2. Я знаю, это кажется случайным, но не будем торопиться. Например, таким числом может быть 1,2, что станет нашей первой аппроксимацией.Исходя из этого, 2/x₀ больше √2.Следовательно, интервал [x₀, 2/x₀] включает в себя √2.Из этого следует, что средняя точка интервала [x₀, 2/x₀] является более точной аппроксимацией значения √2. Как видно на рисунке ниже, она существенно лучше!Давайте определим из этого x₁.Развивая эту тему, мы можем определить последовательность аппроксимации, беря средние точки таких интервалов.Вот несколько первых членов последовательности. Даже третий член уже является на удивление хорошей аппроксимацией.Если мы нанесём эти числа на диаграмму рассеяния, то спустя несколько шагов нам уже практически понадобится микроскоп, чтобы увидеть отличия от √2.Как видите, это сходится к √2 чрезвычайно быстро.Но насколько быстро?Погрешность между этой аппроксимацией и значением √2 определяется просто как расстояние между ними, замеренное по абсолютному значению их разности. Например, погрешность нашего первого предположения e₀ задаётся следующим образом:Каким бы малым или большим ни было e₀, мы можем использовать её для оценки последующих погрешностей.Давайте займёмся алгеброй и посмотрим, как e₀ относится к e₁! Сначала выразим e₁ в виде дроби.Тогда поскольку мы выбрали x₀ больше единицы, то можем выразить его в членах e₁. Так как числитель e₀ возведён в квадрат, наша задача проста.Повторяя эти рассуждения, мы получаем, что сходимость очень быстра, даже быстрее экспоненциальной!Повезло ли вавилонянам, или они угодили в самую точку?На самом деле, второе. Настало время поднять занавес!Давайте перефразируем задачу аппроксимации квадратного корня из двух. Вместо того, чтобы вычислять функцию f(x) = √x в заданной точке, попробуем найти корень (положительный) f(x) = x² — 2. (Который, как оказывается, тоже равен √2.)Существует ли обобщённый метод решения такой задачи? Да, это метод Ньютона-Рафсона. Чтобы показать, как он работает, давайте приблизим корень f(x).Как мы можем переместиться от нашей первоначальной догадки x₀ к корню?Например, можно следовать по направлению касательной и посмотреть, где она пересекает ось X. Поскольку угол касательной определяет производная, это пересечение можно сразу вычислить. Я покажу, как это сделать.Уравнение касательной задаётся следующим образом.Приравняв его к нулю и решив, мы получим точку, в которой касательная пересекает ось X.Таким образом, выбрав следующую догадку x₁ в качестве этой точки пересечения, мы получим более точную (надеемся) аппроксимацию.Вот и всё! На основании этой идеи мы можем определить рекурсивную последовательность.Это называется методом Ньютона-Рафсона. Вот следующий шаг. Как видите, третий шаг находится почти в √2.Остаётся один важный вопрос: такой ли способ применили вавилоняне? Да, и вот почему.В предыдущем примере мы решили найти корень f(x) = x² — 2. Давайте найдём явную формулу рекурсивной последовательности, заданной методом Ньютона-Рафсона. Её производную легко вычислить, так что мы готовы.Применив немного алгебры, мы можем прийти к не особо удивительному выводу.Следовательно, вавилонский алгоритм — это частный случай метода Ньютона-Рафсона!Мы помним, что сходимость в этом конкретном случае крайне быстрая. Справедливо ли это в общем случае? Если нам повезёт.Если не вдаваться в подробности, сходимость и её скорость зависят от локального поведения функции.Например, если f(x) дважды дифференцируема, то член погрешности для n-ного элемента может быть описан членами производных и квадратом (n-1)-ной погрешности.(Если вам интересны подробности, то доказательство есть в Википедии.)В частности, если производные «ведут себя хорошо» (то есть первая производная отделена от нуля, а вторая производная ограничена), то скорость сходимости квадратичная.Квадратичная сходимость истинна не только для поиска квадратного корня двух аппроксимацией положительного корня f(x) = x² — 2, но и для широкого спектра функций.К сожалению не всё так идеально. Метод Ньютона-Рафсона может давать серьёзные сбои в довольно часто встречающихся случаях, к тому же имеет множество недостатков.Например, если функция рядом с корнем «плоская», то сходимость будет мучительно медленной. Один из таких случаев показан ниже.Это происходит, когда корень имеет большую повышенную неоднозначность, то есть производные тоже равны нулю. Кстати о производных, в отличие от случая с квадратным корнем вавилонян, их может быть сложно вычислить, из-за чего этот метод оказывается неприменимым.Более того, весь процесс сильно зависит от первоначальной догадки: итерация может сойтись к неверному корню или даже разойтись.То, что древние вавилоняне смогли вычислить √2 до шестого знака после запятой, достаточно удивительно. Эта точность вызывает большое уважение, особенно учитывая, что она была достигнута почти четыре тысячи лет назад и вычисления выполнялись вручную.Как оказалось, им не просто повезло; они обнаружили особый случай мощного метода, способного аппроксимировать корень широкого спектра функций. Он стал известен под названием «метод Ньютона-Рафсона».Принцип прост:Если функция ведёт себя достаточно хорошо (то есть её производная локально отделена от нуля, а вторая производная ограничена), то сходимость происходит чрезвычайно быстро: именно поэтому вавилоняне смогли достичь «наивысшей в древнем мире вычислительной точности»."'https://hsto.org/r/w1560/webt/a0/kx/qr/a0kxqr2gy_698bivw_njtiscb6i.jpeg'"['https://habrastorage.org/r/w1560/webt/uc/zw/rj/uczwrjpwj74bb-s0keesjlwgr5u.png', 'https://habrastorage.org/r/w1560/webt/yb/cx/lh/ybcxlhb479hgeinbp-6g-l09qiw.png', 'https://habrastorage.org/r/w1560/webt/vx/bx/pb/vxbxpblhcpjkdp7xa3m1teqqe-m.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w1560/webt/ah/zz/ai/ahzzain128sfit4-fkf5br_edpk.png', 'https://habrastorage.org/r/w1560/webt/54/kt/0b/54kt0bwyovxuxq4sxmfidrbx7um.png', 'https://habrastorage.org/r/w1560/webt/jh/e0/i_/jhe0i_jvxo3zn36mckrz2dytilq.png', 'https://habrastorage.org/r/w1560/webt/to/co/3j/toco3j3pjpsz5mluowa5l8giww8.png', 'https://habrastorage.org/r/w1560/webt/bz/is/jm/bzisjmrzksiydlqdsabh5ubi1lq.png', 'https://hsto.org/r/w1560/webt/a0/kx/qr/a0kxqr2gy_698bivw_njtiscb6i.jpeg', 'https://habrastorage.org/r/w780q1/webt/a0/kx/qr/a0kxqr2gy_698bivw_njtiscb6i.jpeg', 'https://habrastorage.org/r/w1560/webt/ky/e_/m2/kye_m2x4-6ba2s-zztvep75paro.png', 'https://habrastorage.org/r/w1560/webt/ek/ln/ce/eklnceyqbtxv0xkqn0n0sfsh-li.png', 'https://habrastorage.org/r/w1560/webt/l7/3y/9j/l73y9jv5qsx3dr9aomr_cb2i0gy.png', 'https://habrastorage.org/r/w1560/webt/fu/a5/ff/fua5ffupkivl75qnnxuxlpbvho0.png', 'https://habrastorage.org/r/w1560/webt/md/xe/qe/mdxeqenhq4btzoeenk6rpy7gshg.png', 'https://habrastorage.org/r/w1560/webt/4n/mu/-0/4nmu-0jqnayzv-8gbitzclthnp0.png', 'https://habrastorage.org/r/w1560/webt/wc/fj/e-/wcfje-zzvio6icnhoraagbglcai.png', 'https://habrastorage.org/r/w1560/webt/td/il/mi/tdilmi8o7gx1iozov0csr39xvww.png', 'https://habrastorage.org/r/w1560/webt/oz/wx/3l/ozwx3liq0r7sy8im4imstdx10go.png', 'https://habrastorage.org/r/w1560/webt/sf/3w/qn/sf3wqn9pf_gjl9zostt9inmzjxk.png', 'https://habrastorage.org/r/w1560/webt/wq/0k/cx/wq0kcxhockvgzuacyg1o2p9cedw.png', 'https://habrastorage.org/r/w1560/webt/zt/dl/_b/ztdl_bbqfehcu-yumlsb6y4c-ii.png', 'https://habrastorage.org/r/w1560/webt/ay/38/je/ay38jeimdtpe84nzykxs_2gogbi.png', 'https://habrastorage.org/r/w1560/webt/ot/hc/gv/othcgvtneq-upfpqflgsmlpzz60.png', 'https://habrastorage.org/r/w1560/webt/g2/1a/lh/g21alhfzkeqotjwxfk9dmzbvobc.png', 'https://habrastorage.org/r/w1560/webt/a0/pa/qv/a0paqvjd-da9uff_du0yi9ngotc.png', 'https://habrastorage.org/getpro/habr/company/ccd/aa0/353/ccdaa03534f7ede206885bc43d3dbda6.jpg', 'https://habrastorage.org/r/w1560/webt/qp/bo/yx/qpboyxkq_peew8km8purp7ql4ds.png', 'https://habrastorage.org/r/w1560/webt/qy/ht/dg/qyhtdgdfdmwnb9h4gfu11l7jx9s.png']"
2'720962'Дополняем реальность в мобильных приложениях через ARCore'Технологии дополненной реальности (Augmented Reality, AR) развиваются с первых экспериментов с шлемами в 1968 году и прогнозируются как один из быстрорастущих сегментов развития интерфейсов (особенно...'https://habr.com/ru/post/720962/'"Технологии дополненной реальности (Augmented Reality, AR) развиваются с первых экспериментов с шлемами в 1968 году и прогнозируются как один из быстрорастущих сегментов развития интерфейсов (особенно при появлении специализированных устройств, таких как Hololens, Xiaomi Smart Glasses и проекта с непонятной судьбой Google Glass). Не могли не заметить этот тренд и разработчики операционных систем для мобильных устройств, Apple выпустила свой набор инструментов ARKit, также как и Google создала набор библиотек ARCore. Особенно важно, что поддержка этих библиотек доступна на большом количестве устройств (для Android нужна версия 7.0 или новее, а это более 94% доступных устройств, при этом почти 90% из них поддерживают Depth API, необходимый для корректной работы алгоритмов размещения объектов виртуального мира в сложном окружении). В этой статье мы рассмотрим основные вопросы использования ARCore и размещения объектов виртуального мира над поверхностями реального.

Прежде всего нужно убедиться, что ваш телефон поддерживает ARCore, для этого необходимо установить Google Play Services for AR по ссылке. В любом случае для разработки можно использовать эмулятор. Для корректной работы на эмуляторе нужно установить этот пакет (версия для эмулятора).

При создании приложения необходимо указать минимальную версию SDK 24, а также необходимость использования AR и, если использование ARCore критично для основной функциональности приложения, также отметить его как Required, например так (в AndroidManifest.xml):

<uses-feature android:name=""android.hardware.camera.ar"" /> <application> <meta-data android:name=""com.google.ar.core"" android:value=""required"" /> </application>

Также в коде должны быть выполнены вызовы ArCoreApk.checkAvailability() для проверки доступности библиотеки и ArCoreApk.requestInstall() для перехода к установке библиотеки. Также в зависимости проекта нужно добавить библиотеку:

dependencies { //... implementation(""com.google.ar:core:1.36.0"") }

Добавим в onCreate в MainActivity проверку доступности AR:

val availability = ArCoreApk.getInstance().checkAvailability(this) println(""AR is supported: ${availability.isSupported}"")

Если возвращается значение true, то мы можем использовать функциональность библиотеки иначе можно запросить установку ArCore:

if (!availability.isSupported) { val result = ArCoreApk.getInstance().requestInstall(this, false) when (result) { ArCoreApk.InstallStatus.INSTALLED -> println(""Do some work"") ArCoreApk.InstallStatus.INSTALL_REQUESTED -> println(""Just wait for user actions"") } }

Поскольку дополненная реальность использует камеру телефона, то нужно дополнительно объявить и запросить разрешение для камеры, добавим в AndroidManifest.xml

<uses-feature android:name=""android.hardware.camera"" android:required=""false"" /> <uses-permission android:name=""android.permission.CAMERA""/>

И запросим runtime-разрешение для доступа к камере (CAMERA_REQUEST_CODE=1000):

when (PackageManager.PERMISSION_GRANTED) { ContextCompat.checkSelfPermission( this, Manifest.permission.CAMERA ) -> { println(""Do some job"") } else -> { requestPermissions( arrayOf(Manifest.permission.CAMERA), CAMERA_REQUEST_CODE ) } }

и также обработаем результат действия выдачи разрешения (если было запрошено):

override fun onRequestPermissionsResult( requestCode: Int, permissions: Array<out String>, grantResults: IntArray ) { super.onRequestPermissionsResult(requestCode, permissions, grantResults) if (requestCode== CAMERA_REQUEST_CODE) { if (grantResults.first() == PackageManager.PERMISSION_GRANTED) { println(""Do some work with camera"") } } }

Все взаимодействие с библиотекой выполняется через сессию:

import com.google.ar.core.Config import com.google.ar.core.Session lateinit var session: Session fun onCreate() { //...инициализация... session = Session(config) val config = Config(session) //здесь можно изменить конфигурацию //например setFocusMode устанавливает режим фокусировки //также можно управлять алгоритмами поиска поверхностей (setPlaneFindingMode), //включать поддержку режима глубины (setDepthMode), управлять способами обнаружения //источников освещения (setLightEstimationMode) и т.д. session.configure(config) } fun onDestroy() { session.close() }

Для создания объектов 3D-сцены будут использоваться возможности OpenGL ES, для этого на сцену необходимо поместить View с типом GLSurfaceView (activity_main.xml):

<?xml version=""1.0"" encoding=""utf-8""?> <androidx.constraintlayout.widget.ConstraintLayout xmlns:android=""http://schemas.android.com/apk/res/android"" xmlns:tools=""http://schemas.android.com/tools"" android:layout_width=""match_parent"" android:layout_height=""match_parent"" tools:context="".MainActivity""> <android.opengl.GLSurfaceView android:id=""@+id/surface"" android:layout_width=""match_parent"" android:layout_height=""match_parent""/> </androidx.constraintlayout.widget.ConstraintLayout>

Выполним инициализацию контекста и присоединим класс для отрисовки сцены:

val surface = findViewById<GLSurfaceView>(R.id.surface) surface.setRenderer(MainRenderer()) surface.requestRender()

Сам класс MainRenderer будет наследоваться от android.opengl.GLSurfaceView.Renderer и определять следующие методы:

override fun onSurfaceCreated(gl: GL10?, config: EGLConfig?) { println(""Surface created"") //здесь нужно выполнить инициализацию контекста } override fun onSurfaceChanged(gl: GL10?, width: Int, height: Int) { println(""Surface changed"") //здесь можно обновить контекст (например, размеры сцены) } override fun onDrawFrame(gl: GL10?) { println(""Draw frame"") val frame = session.update() //... логика отрисовки }

Здесь наиболее важным является объект frame, который позволяет как получить доступ к актуальному изображению с камеры и получить результаты распознавания источников света, ключевых точек, поверхностей или поз, например:

acquireCameraImage() извлекает текущее изображение с камеры

acquireRawDepthImage16Bits() получить карту глубины для изображения

getLightEstimate() обнаруживает фоновый источник освещения (если это разрешено в конфигурации)

getUpdatedAnchors() получает список обнаруженных точек привязки (используются для позиционирования объектов в дополненной реальности, могут быть созданы над любым обнаруженным объектом или точкой на поверхности

Из сессии можно получить информацию об отслеживаемых объектах (включая поверхности) следующим образом:

session.getAllTrackables<Plane>(Plane::class.java)

Также сейчас поддерживается GeoSpatial API для позиционирования объектов с привязкой к географическим координатам (через session.earth), что может быть полезно для отображения подсказок при навигации, визуализации исторических изображений при посещении достопримечательностей и т.д.

Поскольку для отображения на сцене требуется работать с GL-текстурами, удобно использовать готовый рендерер, который умеет работать с фоновым изображением, отображением 3D-моделей и собственных шейдеров. Например можно взять пример отсюда, где поддерживается работа с моделями в формате WaveFront (.obj) и текстурами в PNG. Также можно использовать любую библиотеку для работы поверх EGL.

Наиболее просто начать разработку AR приложения с базового шаблона hello_ar_kotlin, в котором реализовано отображение фоновой текстуры (изображение с камеры), карты глубины, возможность загрузки 3D-моделей и примеры для добавления объектов к якорным точкам (с отслеживанием положения точки при изменении изображения на камере). В этом проекте есть дополнительные helper-классы, упрощающие управление жизненным циклом сессии ARCore и запрос необходимых разрешений (ARCoreSessionLifecycleHelper). Также Sample Renderer представляет базовый класс, который дает возможность загружать 3D-модели и текстуры, отображать растровые изображения как текстуру (класс FrameBuffer), например, изображение с камер, через BackgroundRenderer, работать с 3D-объектами и трансформировать их в набор полигонов (а также загружать из WaveFront OBJ-файла) в классе Mesh, загружать текстуры (класс Texture). В примере можно увидеть как используется поиск поверхностей и добавления якорных точек для размещения 3D-объектов:

Для добавления виртуального объекта необходимо в onSurfaceCreated загрузить mesh (3D-сетку из obj-файла) и текстуры для поверхности и отражения:

virtualObjectAlbedoInstantPlacementTexture = Texture.createFromAsset( render, ""models/pawn_albedo_instant_placement.png"", Texture.WrapMode.CLAMP_TO_EDGE, Texture.ColorFormat.SRGB ) val virtualObjectPbrTexture = Texture.createFromAsset( render, ""models/pawn_roughness_metallic_ao.png"", Texture.WrapMode.CLAMP_TO_EDGE, Texture.ColorFormat.LINEAR ) virtualObjectMesh = Mesh.createFromAsset(render, ""models/pawn.obj"") virtualObjectShader = Shader.createFromAssets( render, ""shaders/environmental_hdr.vert"", ""shaders/environmental_hdr.frag"", mapOf(""NUMBER_OF_MIPMAP_LEVELS"" to cubemapFilter.numberOfMipmapLevels.toString()) ) .setTexture(""u_AlbedoTexture"", virtualObjectAlbedoTexture) .setTexture(""u_RoughnessMetallicAmbientOcclusionTexture"", virtualObjectPbrTexture) .setTexture(""u_Cubemap"", cubemapFilter.filteredCubemapTexture) .setTexture(""u_DfgTexture"", dfgTexture)

Для отображения обнаруженных поверхностей используется класс PlaneRenderer (в onDraw):

val projectionMatrix = FloatArray(16) camera.getProjectionMatrix(projectionMatrix, 0, Z_NEAR, Z_FAR) planeRenderer.drawPlanes( render, session.getAllTrackables<Plane>(Plane::class.java), camera.displayOrientedPose, projectionMatrix )

Визуализация присоединенных к якорным точкам объектов выполняется аналогично с использованием матрицы для преобразования экранных координат в мировую систему координат:

val viewMatrix = FloatArray(16) val projectionMatrix = FloatArray(16) val modelViewMatrix = FloatArray(16) // view x model val modelViewProjectionMatrix = FloatArray(16) // projection x view x model camera.getViewMatrix(viewMatrix, 0) Matrix.multiplyMM(modelViewProjectionMatrix, 0, projectionMatrix, 0, viewMatrix, 0) for ((anchor, trackable) in wrappedAnchors.filter { it.anchor.trackingState == TrackingState.TRACKING }) { // Get the current pose of an Anchor in world space. The Anchor pose is updated // during calls to session.update() as ARCore refines its estimate of the world. anchor.pose.toMatrix(modelMatrix, 0) // Calculate model/view/projection matrices Matrix.multiplyMM(modelViewMatrix, 0, viewMatrix, 0, modelMatrix, 0) Matrix.multiplyMM(modelViewProjectionMatrix, 0, projectionMatrix, 0, modelViewMatrix, 0) // Update shader properties and draw virtualObjectShader.setMat4(""u_ModelView"", modelViewMatrix) virtualObjectShader.setMat4(""u_ModelViewProjection"", modelViewProjectionMatrix) val texture = if ((trackable as? InstantPlacementPoint)?.trackingMethod == InstantPlacementPoint.TrackingMethod.SCREENSPACE_WITH_APPROXIMATE_DISTANCE ) { virtualObjectAlbedoInstantPlacementTexture } else { virtualObjectAlbedoTexture } virtualObjectShader.setTexture(""u_AlbedoTexture"", texture) render.draw(virtualObjectMesh, virtualObjectShader, virtualSceneFramebuffer) }

Для добавления якорной точки координаты нажатия на экране преобразуются в координаты на обнаруженной поверхности:

val hitResultList = frame.hitTest(tap) //находим ближайший к нам val firstHitResult = hitResultList.firstOrNull { hit -> when (val trackable = hit.trackable!!) { is Plane -> trackable.isPoseInPolygon(hit.hitPose) && PlaneRenderer.calculateDistanceToPlane(hit.hitPose, camera.pose) > 0 is Point -> trackable.orientationMode == Point.OrientationMode.ESTIMATED_SURFACE_NORMAL is InstantPlacementPoint -> true is DepthPoint -> true else -> false } } if (firstHitResult != null) { wrappedAnchors.add(WrappedAnchor(firstHitResult.createAnchor(), firstHitResult.trackable)) }

Поскольку ARCore принимает на себя основные задачи по обнаружению и отслеживанию объектов (может распознавать двумерные объекты на данный момент, а также есть эксперименты с обнаружение 3D-объектов с использование ARCore ML), поверхностей, точек (например, угловых точек 3D-объектов для измерения расстояний), то в большинстве случае остается только решить задачу позиционирования и ориентировки 3D-объектов по указанным координатам и размещения источников света в виртуальной сцене для корректной визуализации бликов и теней на создаваемых объектах и здесь уже начинается работа с низкоуровневыми методами библиотеки EGL и преобразованиями координат, которые могут быть реализованы через дополнительные библиотеки, например libgdx.

В завершение приглашаю вас на бесплатный урок, в рамках которого рассмотрим Jetpack Compose - современный тулкит от компании Google для создания приложений под ОС Android на языке программирования Kotlin. Jetpack Compose упрощает написание и обновление визуального интерфейса приложения, предоставляя декларативный подход."'https://habrastorage.org/getpro/habr/upload_files/50d/6c8/137/50d6c8137dac2ec53506f5db6cd29f0d.png'"['https://habrastorage.org/getpro/habr/avatars/651/3fb/4ae/6513fb4aec629fee26795ee8f11e9a50.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/d23/d18/ac4/d23d18ac45e70350b33d5abf8d8ee88e.png', 'https://habrastorage.org/r/w32/getpro/habr/avatars/651/3fb/4ae/6513fb4aec629fee26795ee8f11e9a50.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/company/2d5/0ed/b57/2d50edb57cf45fa07cc4f39f53b78395.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/34d/b63/e4a/34db63e4a6d3b9c13cc23ec3acbd3077.png', 'https://habrastorage.org/getpro/habr/upload_files/50d/6c8/137/50d6c8137dac2ec53506f5db6cd29f0d.png']"
3'720688'[Перевод] Исследование нескольких проблем, обнаруженных при статическом анализе'В последнее время мы занимались статическим анализом нашей кодовой базы. В результате было выявлено несколько проблем в коде C++, которые мне пришлось исправлять. Это в очередной раз помогло мне...'https://habr.com/ru/post/720688/'"Удаление элементов из вектора с применением цикла

int main() { std::vector<int> data{ 1,1,2,3,5,8,13,21,34,55 }; for (auto it = data.begin(); it != data.end();) { /* делаем что-то с *it */ if (*it % 2 == 0) { data.erase(it); } else { ++it; } } for (auto const& e : data) std::cout << e << '

'; }

it = data.erase(it);

data.erase(std::remove_if(data.begin(), data.end(), [](int const n) {return n % 2 == 0; }), data.end());

Возврат без освобождения динамически выделенной памяти

void Process(int* ptr) { std::cout << *ptr << '

'; delete ptr; } void Demo(int const limit) { int* ptr = new int(rand() % 100); if (*ptr > limit) { return; } Process(ptr); }

void Demo(int const limit) { std::unique_ptr<int> ptr = std::make_unique<int>(rand() % 100); if (*ptr > limit) { return; } Process(ptr.release()); }

Путаница между освобождением и сбросом

std::unique_ptr<int> MakeObject(int const limit) { std::unique_ptr<int> ptr = std::make_unique<int>(rand() % 100); if (*ptr > limit) { ptr.release(); } return ptr; }

std::unique_ptr<int> MakeObject(int const limit) { std::unique_ptr<int> ptr = std::make_unique<int>(rand() % 100); if (*ptr > limit) { ptr.reset(); } return ptr; }

Подробнее об утечках памяти

void Demo() { DWORD size = 0; TCHAR* buffer = nullptr; BOOL ret = ::GetUserName(nullptr, &size); if(!ret && ::GetLastError() == ERROR_INSUFFICIENT_BUFFER) buffer = new TCHAR[size]; if (buffer) { ::GetUserName(buffer, &size); std::cout << buffer << '

'; } }

void Demo() { DWORD size = 0; std::vector<TCHAR> buffer; BOOL ret = ::GetUserName(nullptr, &size); if (!ret && ::GetLastError() == ERROR_INSUFFICIENT_BUFFER) buffer.resize(size); if (!buffer.empty()) { ::GetUserName(buffer.data(), &size); std::cout << buffer.data() << '

'; } }

Количество элементов в массиве

int AnArray[] = {1, 1, 2, 3, 5, 8}; #define COUNT_OF_ARRAY sizeof(AnArray) / sizeof(int) int main() { for (size_t i = 0; i < COUNT_OF_ARRAY; ++i) { std::cout << AnArray[i] << '

'; } }

int AnArray[] = {1, 1, 2, 3, 5, 8}; int main() { for (size_t i = 0; i < sizeof(AnArray); ++i) { std::cout << AnArray[i] << '

'; } }

int AnArray[] = {1, 1, 2, 3, 5, 8}; int main() { for (size_t i = 0; i < std::size(AnArray); ++i) { std::cout << AnArray[i] << '

'; } }

std::array<int, 6> AnArray {1, 1, 2, 3, 5, 8}; int main() { for (size_t i = 0; i < AnArray.size(); ++i) { std::cout << AnArray[i] << '

'; } }

В последнее время мы занимались статическим анализом нашей кодовой базы. В результате было выявлено несколько проблем в коде C++, которые мне пришлось исправлять. Это в очередной раз помогло мне осознать, каково совершать такие ошибки, которые обычно трудно найти, просто взглянув на код (человеческим глазом). Я считаю, что стоит поделиться опытом решения некоторых из этих проблем. Не могу опубликовать мой реальный код, он все равно будет слишком сложным, но я использую несколько простых фрагментов, в которых продемонстрированы те же проблемы, что встретились мне в проанализированном коде. Это (надеюсь) поможет вам легко понять как проблему, так и её решение. Первая задача, которую мы обсудим — это удаление элементов из контейнера, например, std::vector, при помощи цикла. Наш код выглядел примерно так, как показано в следующем упрощённом фрагменте:Здесь у нас есть вектор, содержащий данные, и цикл, в котором каждый элемент вектора проверяется на соответствие некоторому условию. Те элементы, которые соответствуют этому условию, удаляются из контейнера. Вы уже заметили, в чём проблема?Мы используем std::vector::erase для удаления элемента. Таким образом удаляется элемент, занимающий указанную позицию, и инвалидируются все итераторы и ссылки в точке удаления или после неё, включая итератор end(). При удалении элемента итератор не приращивается, поэтому в следующем цикле будет обрабатываться только что удаленный элемент. А это добром не кончится.Решение довольно простое: присвойте переменной it значение, возвращаемое функцией erase() (это итератор, следующий за удалённым элементом).Это и есть решение. Но есть ли решение получше? Да, мы можем использовать общий алгоритм для удаления всех нужных нам элементов. Для этого мы воспользуемся std::remove_if . Можно заменить цикл for, описанный выше, на следующий:Нужно по возможности использовать стандартные алгоритмы, потому что тогда, как правило, придётся написать меньше кода, и вероятность возникновения ошибок в этом коде будет невелика.Второй пример, который я хочу привести, — это утечка памяти, которая происходит в функции, выделяющей память, а затем возвращающейся, не освободив эту память. Проблему можно проиллюстрировать следующим фрагментом:Это может показаться глупым, но представьте, что вместо int есть пользовательский класс, и предусмотрено несколько проверок объекта (или, возможно, связанных с ним данных). Причём, в некоторых случаях выполнение функции должно прекратиться. В противном случае выделенный объект передаётся функции (которая, возможно, получает его во владение). Хватит малейшего недосмотра (особенно если это более длинная и сложная функция), чтобы забыть удалить выделенную память перед возвратом из функции.Для исправления ситуации можно использовать std::unique_ptr, который хорош освобождением собственного динамически выделенного объекта при выходе из области видимости.Если вам нужно передать основной объект в функцию, которая принимает необработанный указатель и получает объект во владение, просто используйте std::unique_ptr::release() . Это открепит управляемый объект от умного указателя, передавая владение объектом вызывающей функции, именно той, которая отвечает за его высвобождение.И это приводит к следующей проблеме…Класс std::unique_ptr имеет два метода: release() : освобождает управляемый объект, передавая владение вызывающей стороне. reset() : заменяет управляемый объект на другой (принимает аргумент null в качестве значения по умолчанию); если умный указатель содержит не нулевой указатель на объект, этот объект удаляется.Следующий фрагмент содержит ошибку:Если условие не выполняется, управляемый объект высвобождается. Но на самом деле предполагается его удаление. Это приводит к утечке памяти. В данном случае правильно вызвать reset():Выше было показано, как можно использовать std::unique_ptr, чтобы избежать утечки объектов, выделенных в куче. Но что если нам нужно выделить и освободить массив объектов? Класс std::unique_ptr поддерживает массивы, но, вместе с тем, имеет такие недостатки: размер массива фиксирован, копирование невозможно, std::unique_ptr<[]> не является контейнером и поэтому не совместим с алгоритмами и концепциями. Но есть контейнеры, такие как std::vector, которые могут быть использованы именно в таких целях.В качестве примера рассмотрим следующий фрагмент:Многие API Windows требуют, чтобы в распоряжении был буфер определённого размера. Но их можно вызвать и с нулевым буфером, и они вернут размер, необходимый для буфера, чтобы затем вы могли выделить необходимую память. Именно это и происходит в данном фрагменте, который извлекает и выводит имя текущего пользователя. Но выделенный буфер никогда не освобождается (из-за ошибки). Этого можно надёжно избежать, если использовать std::vector, который позаботится о выделении и освобождении внутрисистемно.Независимо от того, как возвращается эта функция (нормально или даже из-за исключения), память, выделенная объектом std::vector, в этот момент освобождается.Я много раз встречал этот паттерн в унаследованном коде. Где-то объявляется массив (обычно в глобальной области видимости), а затем используется макрос для определения количества элементов в массиве. Вот небольшой фрагмент:Это неудачный стиль, который чреват ошибками. Одну из них (которую было бы трудно заметить) я приведу ниже. Но статический анализ сразу же поймал её:Предполагается перебрать все элементы массива, но sizeof(AnArray) возвращает размер, занимаемый массивом в памяти, а не количество его элементов. Это приведет к выходу за границы.Существует несколько решений этой проблемы. Одно из них — использовать некомпонентную функцию std::size(), позволяющую получить количество элементов в массиве:Целесообразнее было бы использовать std::array вместо C-подобных массивов. Это даст множество преимуществ, включая лёгкое получение количества элементов с помощью компонентной функции size() (или некомпонентной std::size()).Итак, статический анализ отлично помогает выявлять в коде такие проблемы, которые иначе трудно заметить. Несколько приведенных здесь примеров должны наглядно продемонстрировать это."'https://habr.com/share/publication/720688/0c1779ee2e90fedbf29f56c2c6154a18/'"['https://habr.com/share/publication/720688/0c1779ee2e90fedbf29f56c2c6154a18/', 'https://habrastorage.org/getpro/habr/company/e4d/50a/630/e4d50a630923dbf6e6b786d26b9da6d7.png', 'https://habrastorage.org/getpro/habr/avatars/3be/b11/2b0/3beb112b076924a916b27c845d88dcfa.jpg', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w32/getpro/habr/avatars/3be/b11/2b0/3beb112b076924a916b27c845d88dcfa.jpg']"
4'720972'Плюсы и минусы использования мессенджеров в качестве каналов делового общения внутри организации'В последнее время при устройстве на работу новоиспечённого сотрудника тут же включают в различные чаты и вкратце объясняют ему правила общения, принятые в организации (от «читать обязательно в любое...'https://habr.com/ru/post/720972/'"В последнее время при устройстве на работу новоиспечённого сотрудника тут же включают в различные чаты и вкратце объясняют ему правила общения, принятые в организации (от «читать обязательно в любое время суток, прочитанное тут же принимать к исполнению» до «с 21-00 до 8-30 пишем в чат только экстренные сообщения, и то если невозможно решить проблему иным образом»).

Мне довелось поработать в организациях с разными подходами к использованию мессенджеров в качестве каналов связи (и привязи), так что здесь представлен исключительно мой опыт и наблюдения. Если вам есть что добавить или уточнить исходя из вашего опыта, пишите в комментариях.

Плюсы использования мессенджеров для общения внутри организации:

Наличие истории переписки, её доступность. Палка о двух концах: с одной стороны, ляпнешь что-то в переписке, и это тебе же и припомнят, но с другой стороны, если вам что-то пообещали письменно (в мессенджере), делаете скриншот и потом сами можете припомнить. В целом можно оценить позитивно.

Доступность канала связи в режиме 24/7. Это скорее плюс для начальства: подчинённому можно отправить сообщение в любое время дня и ночи. Для подчинённого это, конечно же, минус и почва для взаимного недопонимания с начальством. Многие люди и так имеют разное понимание модальности сообщения (один, прочитав слово «нужно», поймёт как свою обязанность сделать это, а второй начнёт задавать вопросы – «кому нужно?» «а мне нужно?» «а зачем это мне нужно?» и сделает в конце концов вывод: «вот кому нужно, тот пусть и делает, а я приму к сведению»).

Чёткое определение тематики группы, когда и если в неё добавляются только те сотрудники, которые имеют непосредственное отношение к обсуждаемому в группе вопросу. Это плюс и с точки зрения работника, и с точки зрения организации (коммуникации выстраиваются эффективно, никто не тратит своё время на пролистывание ничего не значащих для него сообщений в попытке вычленить только те, которые его непосредственно касаются).

Минусы:

Размывание ответственности: сотрудник, отправивший «широковещательное» сообщение, считает, что обозначенный в этом сообщении вопрос будет решён, что все, кто должен быть задействован в решении вопроса, подключены к этому. На самом деле всё это может быть очень далеко от действительности.

«Сообщение, отправленное в чат, прочитано»: мессенджеры позволяют увидеть статус сообщения – «просмотрено» или «не просмотрено». Если бы всё было так просто. Во-первых, если сообщение отправлено не лично, а в группу, оно может быть открыто и просмотрено только одним из участников и не факт, что это был тот, кто действительно нужен. Во-вторых, если сообщение отправлено лично и имеет небольшую длину, оно может быть целиком прочитано во всплывающем уведомлении, а отправитель будет считать, что его сообщение не прочитано, поскольку статус сообщения не изменился.

Чем больше организация (а следовательно, и число решаемых через мессенджеры вопросов), тем большее число чатов вынужден читать сотрудник (а чаты нужно не только читать, но и реагировать хотя бы иногда на сообщения – выполнять полученные задания, давать ответы на заданные вопросы), в результате чего больше времени уходит на просмотр всех необходимых чатов и выше вероятность упустить из виду что-то по-настоящему важное.

Рассогласованность указаний, поступающих по разным каналам. По работе я включён в дюжину разных каналов, по одному из них вчера было отправлено сообщение о необходимости присутствовать на учёбе, по другому на то же время было назначено собрание, и там и там быть обязательно. Чем больше каналов, тем больше вероятность возникновения такой ситуации. Причём, как водится, каждый отправитель считает своё сообщение самым важным и первостепенным.

Обезличивание общения – в какой-то момент понимаешь, что в чате общаются не живые люди, а «рабочие функции», при этом правила этикета и в целом нормы общения начинают размываться, поскольку отправитель не видит реакцию собеседника, которого потревожил (предположим, что в час ночи в мессенджер вам никто не напишет, но что вы скажете, получив очередное задание или «информацию к сведению» в 23:45, когда вы уже почти уснули?)

«Широковещательные» сообщения («для сведения», «возможно, это будет интересно каждому», новости организации и т.п.) распыляют внимание и в худшем случае способны создать представление о канале связи как о незначительном, который можно и не читать.

Нарушение субординации – если в обычной рабочей среде исполнитель контактирует только со своим непосредственным начальником, то в среде виртуальной очень легко «прыгнуть через голову», связавшись напрямую с более высоким начальством, это же характерно и для каналов связи в обратном направлении – руководителю компании становятся доступны непосредственные исполнители (в том числе потенциальные) той или иной задачи, и редко кто из руководства не воспользуется возможностью озадачить непосредственного исполнителя, минуя остальные уровни (заместители, начальники отделов и т.д.). В итоге исполнитель получает задания от нескольких руководителей разных рангов, занимающихся разными направлениями работы, что потенциально ведёт к росту стресса и снижению работоспособности работника, а также к рассогласованности работы организации в целом (когда непосредственный руководитель перестаёт понимать, чем именно занимается его подчинённый).

Использование мессенджеров для чтения новостей и подписок на интересующие аккаунты или каналы (характерное как минимум для общения в telegram и vk) «топит» рабочие коммуникации. Например, в моём основном аккаунте я подписан на примерно 150, если уже не 200 разных каналов, каждый из которых мне важен и нужен для работы (новости индустрии, книги, тематические чаты и т.п.). В некоторых из них ежедневно публикуется более сотни сообщений, в других – десятки. Сообщение от контакта, отправленное мне разово, в течение часа-двух уходит на дно в общем списке каналов, на которые у меня оформлена подписка, при этом я могу увидеть в уведомлениях, что мне кто-то написал, но потом буду долго искать, где именно мне написали. Можно махнуть шашкой и заставить человека вести общение в аккаунте только по рабочим вопросам, но… как быть, когда самообразование и отслеживание индустрии являются не менее важными рабочими моментами? Заставлять человека заводить ещё один аккаунт (на другой номер телефона) и сидеть в обоих? Можно, конечно, но где гарантия, что он не уйдёт с головой в тот аккаунт, где приходит информация более интересная, но менее «важная»? Опять же, нужно понимать, важная для кого? Кроме того, даже наличие второго аккаунта не гарантирует возникновения подобной ситуации и на нём.

При широком использовании в организации сразу нескольких мессенджеров (vk, telegram, whatsapp, discord и др.) наблюдаются попытки обязать человека устанавливать на свой личный смартфон сразу все эти программы. При этом никого не интересует, что у сотрудника смартфон может быть очень даже бюджетной модели, и установка туда сразу целого зоопарка приложений (да что там зоопарка – один только discord требует немало ресурсов) сделает устройство практически неработоспособным."'https://habr.com/share/publication/720972/0095c5ecb16be0350d2119bee7d33f50/'"['https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/avatars/9ab/2d5/aef/9ab2d5aef8460c724fc3f0023f85d403.jpg', 'https://habrastorage.org/r/w32/getpro/habr/avatars/9ab/2d5/aef/9ab2d5aef8460c724fc3f0023f85d403.jpg', 'https://habr.com/share/publication/720972/0095c5ecb16be0350d2119bee7d33f50/']"
5'720970'Первый уголовный срок за P2P-сделку: какие риски висят над криптанами в России'Парень торговал через peer-to-peer криптобиржи Garantex – в итоге нарвался на перевод от мошенников (тех самых сотрудников колл-центра «службы безопасности банка») и в итоге получил уголовную...'https://habr.com/ru/post/720970/'"Парень торговал через peer-to-peer криптобиржи Garantex – в итоге нарвался на перевод от мошенников (тех самых сотрудников колл-центра «службы безопасности банка») и в итоге получил уголовную судимость. В общем-то, подобная история может произойти практически с любым, кто пользуется P2P.

На фото Станислав Другалев – основатель криптобиржи Garantex, который погиб в Дубае в 2021 году при довольно странных обстоятельствах; а примерно через год США внесли эту биржу в санкционный список

После февраля 2022 года огромное число людей в России одномоментно превратились в крипто-энтузиастов – ведь многие другие привычные способы свободно перемещать свои деньги туда-сюда просто перестали работать. Про способы покупки криптовалюты в РФ я писал отдельный большой гайд. Так вот: не последнее место по популярности среди россиян занимают именно peer-to-peer (P2P) сделки с криптой.

В двух словах про P2P

В чем суть P2P? Если проводить простую аналогию, то сделка с криптой на централизованной бирже – это примерно похоже на покупку долларов в банке; а то же самое через P2P – это как если бы вы купили эту самую валюту «с рук» у некоего типа в подворотне.

Наглядная иллюстрация peer-to-peer сделки

Так как большинство крупных и популярных криптобирж (типа Binance) в связи с санкциями закрыли возможность заводить к себе деньги с российских банковских карт, покупка крипты через биржу стала для многих россиян недоступной. Вместо нее как раз расцвели P2P-сделки – причем, поиск покупателя/продавца для таких сделок нередко осуществляется через сервисы тех же самых популярных криптобирж.

Только вот при P2P-сделке покупатель отправляет оплату за крипту не через биржу, а напрямую продавцу – чаще всего перечислением на его банковскую карту. Получается, в данной схеме криптобиржа выступает исключительно как посредник, который помог покупателю и продавцу найти друг друга – сама она никаких рисков не несет, и тем более не осуществляет каких-либо проверок на «чистоту» участвующих в сделке денег.

Риски P2P-операций

Самый базовый риск P2P-сделок почти всем хорошо известен: если вы проводите через свою банковскую карту множество подобных операций на не совсем маленькие суммы, то вашему банку это может не понравиться – и он просто заблокирует счет по 115-ФЗ (закон о противодействии отмыванию доходов, полученных преступным путем, и финансированию терроризма).

Как говорится: мало того, чтобы на сделку были согласны покупатель и продавец – участвующие в процессе банки тоже должны быть не против

А вот дальше могут возникать чуть более пугающие вещи. Банки ведь не просто так блокируют счета направо и налево, ссылаясь на закон о противодействии отмыванию «грязных денег» – они именно что не хотят оказаться причастны никаким боком к преступным и/или террористическим бабкам.

Но у банка, по крайней мере, есть специальные отделы, которые пытаются вычислить подобные операции по косвенным признакам. А насколько тщательно подходите к проверке контрагентов вы, когда участвуете в P2P-сделке? Запрашиваете ли вы сканы паспортов, требуете ли прислать вам подтверждения того, что деньги были получены другой стороной законным путем? Ну вот, я так и думал...

Получается, если вы продадите свою крипту какому-нибудь парнишке, которого вы нашли в P2P-разделе Бинанса, а тот окажется мошенником, вором или (того хуже) террористом – то у вас возникает совершенно неиллюзорный риск заехать на нары за получение от него денег на свою карту. Такие дела.

К конкретном примеру: 2 года условно за P2P

Вчера Forbes опубликовали разбор знакового уголовного дела, которое закончилось признанием подсудимых виновными. Рекомендую ознакомиться с оригиналом, ниже я перескажу лишь самое интересное.

Жил-да-был парень, который промышлял с 2021 года перепродажей «денежных кодов» с криптобиржи Garantex. Это когда ты заводишь свои деньги на биржу, и они там как бы временно блокируются – а взамен выпускается специальный код, по которому сумму можно разблокировать и зачислить себе на счет. Соответственно, такие коды можно перепродавать другим людям как эквивалент денег – это удобно для тех, кому по какой-то причине не очень «удобно» напрямую зачислять свои деньги на биржу.

Как видите – речь тут не идет про P2P-сделки именно с криптой. Но суть такая же: парень (в статье он фигурирует под выдуманной фамилией «Евдокимов») находил покупателей, готовых заплатить за такой «денежный код» на пару процентов дороже номинала, принимал от них деньги напрямую на карту, и отправлял им сам код.

Причем, из статьи следует, что он даже пытался делать какие-то базовые проверки: просил покупателей сфотографировать физическую карту, чтобы убедиться, что используются не уведенные с помощью фишинга данные чужой карты. Но и это ему не сильно помогло.

23 февраля 2022 года наш горе-предприниматель Евдокимов принял на карту (кстати, не собственную, а своей девушки) 900 тыс. рублей от некоего пользователя с ником tradeoffer – ну и тот в итоге оказался участником классической мошеннической схемы со «звонками из службы безопасности банка», а деньги, соответственно, – ворованными.

Если вы часто балуетесь P2P-сделками – то вполне возможно, что ваши контрагенты по этим сделкам выглядят как-то так

Отправитель денег (обычный курьер-доставщик суши) тоже не сильно был в курсе всех перипетий – его мошенники использовали просто как «дропа» (то есть, физлица, которое дает данные своей карты другим людям для совершения мутных операций). В итоге правоохранительные органы пришли к обоим и заявили, что они осуществляли мошеннические действия по предварительному сговору (несмотря на то, что это просто два незнакомых между собой пользователя P2P-сервиса) – так что они получили уголовные судимости и по 2–2,5 лет условно каждый.

Вместо выводов

Из уголовного дела выше может сложиться впечатление, что под риском находится только продавец крипты через P2P – ведь именно он принимает деньги, которые могут оказаться нажитыми преступным путем.

Но, на мой взгляд, покупатель крипты волей обстоятельств легко может оказаться фигурантом дела по финансированию терроризма или другой «нежелательной» деятельности. Ведь сейчас списки запрещенных, нежелательных, иноагентских, террористических, и иных юридических/физических лиц, признанных таковыми на территории Российской Федерации разнообразными органами, растут буквально каждую неделю. И каждый ваш перевод денег с карту на карту может, теоретически, оказаться в адрес такого лица – ну а дальше уже вам в отделении объяснят, что имел место преступный сговор, и деньги вы адресно отправляли именно на подрыв конституционного строя, и никак иначе!

Если подобного рода риски вам кажутся неприемлемыми – то, возможно, полностью их избежать можно, только не трогая P2P даже трехметровой палкой.

Если материал оказался для вас полезным – буду благодарен за подписку на мой ТГ-канал RationalAnswer про разумные подходы к личным финансам и инвестициям."'https://habrastorage.org/getpro/habr/upload_files/b47/5f2/016/b475f2016e567483f7ee773d137ea5e3.jpg'"['https://habrastorage.org/r/w32/getpro/habr/avatars/e54/688/8ae/e546888aed94c31b9d15b7fa3d86f381.jpg', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/avatars/e54/688/8ae/e546888aed94c31b9d15b7fa3d86f381.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/e62/610/1fe/e626101fe2530bc39b183c788d05d029.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/6c5/992/3f5/6c59923f5ed3fa2136fa144ca717bea9.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/b47/5f2/016/b475f2016e567483f7ee773d137ea5e3.jpg', 'https://habrastorage.org/getpro/habr/upload_files/b47/5f2/016/b475f2016e567483f7ee773d137ea5e3.jpg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/51d/6a1/8dd/51d6a18dd679026554f0776af35ee166.jpg']"
6'719632'Smart Tomo Engine 2.0. Выход на новый уровень'В сегодняшней статье речь пойдет о Smart Tomo Engine 2.0 – новой версии нашего продукта реконструкции трехмерных объектов из набора их томографических проекций (рентгенограмм). По сравнению с...'https://habr.com/ru/post/719632/'"В сегодняшней статье речь пойдет о Smart Tomo Engine 2.0 – новой версии нашего продукта реконструкции трехмерных объектов из набора их томографических проекций (рентгенограмм). По сравнению с предыдущей версией у новой выше качество получаемых изображений, существенно повышено быстродействие, улучшена технологическая совместимость с программами анализа трехмерных данных и с различными видами томографов. Заходите под кат, чтобы увидеть работу новой версии STE на примере реконструкции цветов (в честь Международного женского дня).

Рис.1. Реконструкция розы в STE 2.0.

О том, как устроен метод компьютерной томографии (КТ), мы писали ранее. Здесь только кратко повторим. Рентгеновское излучение проходит сквозь объект под разными углами и регистрируется детектором. Полученный набор рентгенограмм обрабатывается с помощью программы, результатом работы которой является 3D-изображение внутренней структуры объекта. Smart Tomo Engine как раз и является такой программой.

С чего все начиналось или Smart Tomo Engine 1.0

Первая версия Smart Tomo Engine 1.0, которая вышла 2020 году и про которую мы, конечно же, писали в блоге на Хабр, предоставляла посредством API следующие функции: чтение томографических изображений (проекций) в DICOM формате, собственно томографическую реконструкцию и сохранение результатов реконструкции в DICOM или PNG. Для выполнения реконструкции на выбор предлагалось три алгоритма: FBP, DFR и наш уникальный ускоренный алгоритм реконструкции – HFBP [1]. STE 1.0 включал в себя графический интерфейс пользователя, обеспечивающий двухмерную визуализацию томографических изображений и результатов реконструкции.

STE 1.0 могла реконструировать данные с медицинских томографов первого поколения, синхротронных центров и некоторых лабораторных микротомографов. Как можно заметить не очень широкий круг применений для первой версии продукта. Кроме этого, реальные данные зачастую подвержены искажениям, которые необходимо учитывать при выполнении реконструкции. Из всего этого следует, что продукт нуждался в улучшениях как в ширину, так и в глубину.

Что нового в Smart Tomo Engine 2.0

Расширенная линейка поддерживаемых устройств

В STE 2.0 поддерживается реконструкция данных с индустриальных томографов, портальных инспекционно-досмотровых комплексов, новейших медицинских приборов, лабораторных микротомографов. Есть возможность работы с данными синхротронов, которая уже была в первой версии STE. Как этого добились и какие фичи у нас появились, покажем на примере сравнения с мировыми аналогами.

Функциональное сравнение STE 2.0 c мировыми аналогами

Мы решили сравнить STE 2.0 с мировыми и российскими программами для томографической реконструкции. В таблице приведена усредненная информация по следующим мировым программам: VGMax (Volume Graphics, США), CERA (Siemens Healthineers Global, Германия), efX-CT (North Star Imaging’s Inspection Services Group, США). Для сравнения с образцами из РФ выбраны следующие программы: TomoRec (ЗАО ""ЭЛТЕХ-Мед"", г. Санкт-Петербург) и МикроКТ (ООО ""ПРОДИС.НДТ"", г. Москва). Вся информация взята с сайтов производителей и из брошюр.

Реконструкция и коррекция томографических данных

В следующей таблице приведены сравнительные характеристики по реконструкции, коррекции томографических данных, подавлению артефактов, возникающих на реконструкциях.

Таб. 1. Сравнительная таблица характеристик STE 2.0 с миром и РФ: реконструкция и коррекция.

Загрузка, выгрузка и визуализация

В таблице ниже приведены сравнительные характеристики по возможностям загрузки томографических данных, выгрузке результатов реконструкции, также приведен список поддерживаемых платформ, вычислителей и ОС.

Таб. 2. Сравнительная таблица характеристик STE 2.0 с миром и РФ: загрузка, выгрузка и визуализация.

По сравнительным таблицам видно, что STE 2.0 стал хорошей альтернативой зарубежным аналогам, наголову превосходя российские программы для реконструкции.

А теперь мы расскажем о специализированных функциях STE 2.0, которые подчеркивают наше преимущество. Почти все уникальные возможности программы содержат запатентованные разработки нашей команды.

Уникальные фичи STE 2.0

Технология ""под контролем реконструкции""

Если методы кругового и спирального сканирования с последовательным сбором рентгенограмм стали практически традиционными, то технология под контролем реконструкции действительно уникальная и является нашей новой разработкой. Она позволяет снизить дозовую нагрузку (что важно для медицинских применений) и время проведения измерения. Последнее актуально для измерений в нанометровом диапазоне.

При обычной схеме сканирования сначала регистрируются все данные, а потом уже происходит реконструкция. В схеме ""any-time"" результаты томографической реконструкции анализируются сразу после получения каждой новой серии проекций и, если точность реконструкции достигнута или не может быть достигнута, то принимается решение об остановке процесса сканирования [2].

Мы попытались показать на рис. 2 как это работает. Применение подхода в нанотомографии описано в работе [3]. Результат впечатляет: время измерений удалось сократить на 20% в сравнении с классическим протоколом.

Рис.2. Схема томографических измерений и обработки данных в технологии ""под контролем реконструкции"".

Алгоритм FOVEA

FOVEA - итерационный алгоритм, предназначенный для расширения области видимости реконструкции, позволяющий устранить характерные артефакты реконструкции, возникающие при выходе объекта из поля зрения томографа. Алгоритм - наша собственная запатентованная разработка, о которой мы писали здесь. Пример реконструкции без использования FOVEA, и демонстрация эффективной работы алгоритма приведены на рисунке 3.

Рис. 3. Сравнение результатов работы классического алгоритма реконструкции и FOVEA на зубной щетке.

Как видно по рис.4, результат налицо. Вопрос о реконструкции объектов, которые чуть больше детектора, теперь решен!

Алгоритмы коррекции томографических данных

Все алгоритмы коррекции томографических данных, которые реализованы в Smart Tomo Engine, являются нашей собственной разработкой и каждый заслуживает отдельной статьи на Хабр. Кстати, про некоторые мы уже писали: тут и тут.

В этой статье мы приведем наглядные иллюстрации работы алгоритмов на проблемных данных. На них мы покажем, что делать коррекции необходимо, иначе картинка получается далека от истинного изображения объекта.



Первый вид коррекции - автоматическое определение и коррекция геометрии измерения. Коррекция положения оси вращения с параметрами, автоматически определенными нашим уникальным алгоритмом, предотвращает появление серповидных искажений (1) и двоение краев (2) на восстановленном цифровом изображении объекта (рис.4).

Рис. 4. Результат работы алгоритма автоматического поиска и коррекции положения оси вращения.

STE 2.0 производит коррекцию кольцевых артефактов, вызванных нелинейным откликом ячеек детектора. Результат работы алгоритма и разностное изображение показаны на рис. 5.

Рис. 5. Результат работы алгоритма подавления кольцевых артефактов.

В STE 2.0 реализован алгоритм автоматической коррекции артефактов, вызванных полихроматичностью рентгеновского излучения, отсутствие учета которой приводит к появлению на изображении чашевидных искажений. Реконструкция однокомпонентной пористой керамической мембраны с ""эффектом чаши"", скорректированное изображение и их разность представлены на рис.6.

Рис. 6. Результат работы алгоритма автоматического подавления чашевидных искажений.

Наличие сильно поглощающих включений очень мешает получению хорошей картинки. Уменьшить выраженность возникающих артефактов крайне непростая задача, но мы справились. В STE 2.0 внедрен авторский алгоритм подавления ""металлических"" артефактов, работа которого показана на примере реконструкции Ethernet-коннектора RJ-45 (рис.7).

Рис. 7. Результат работы алгоритма подавления ""металлических"" артефактов.

С шумами мы тоже умеем работать. Их подавление реализовано в виде специальных шумоподавляющих алгоритмов реконструкции, в основе которых лежат методы ИИ. Результаты реконструкции стандартным алгоритмом и нейросетевым методом, использованным для шумоподавления, приведены на рис. 8.

Рис. 8. Результат работы нейросетевого шумоподавляющего алгоритма.

Графический интерфейс пользователя и 3D-визуализации

С прошлой версии Smart Tomo Engine сильно поменял свой облик. Мы повысили удобство взаимодействия с пользователем. В частности были добавлены окна для новой функциональности (предпросмотра реконструкции, подбора оптимальных параметров коррекции, просмотра информации о загруженных данных и др.).

В STE 2.0 также появились новые инструменты для измерений: линейки и гистограммы. Их можно использовать в окнах визуализации проекций и реконструкций. Возможности GUI продемонстрированы на видео ниже.

Мы добавили трехмерную визуализацию данных. Этот прием позволяет оценить в перспективном виде получившуюся реконструкцию или ее часть с помощью одного из четырех типов визуализации: полупрозрачной, цветной или режимы визуализации изоповерхностей. Реализация всех типов визуализации сделана с помощью OpenGL и фрагментных шейдеров. Визуализацию объема 2500 на 2500 на 2500 вокселей на видеокарте NVIDIA RTX 3080Ti можно выполнять в режиме реального времени.



Ниже продемонстрированы возможности 3D-визуализации на цветах, которые мы так любим дарить на 8-е марта.

Рис.9. Возможности 3D-визуализатора STE 2.0.

Рис.10. Возможности 3D-визуализатора STE 2.0.

Заключение

Выпуск второй версии Smart Tomo Engine открыл новые возможности и новые горизонты. Уже сейчас STE 2.0 вырвалась вперед среди российских программ. STE 2.0 обладает целой серией уникальных запатентованных алгоритмов, которые позволяют ей на равных соперничать с лучшими мировыми решениями.

P.S.: Один из цветков, который мы томографировали, был розой. А как называется второй цветок?"'https://habrastorage.org/getpro/habr/upload_files/faa/5c9/46d/faa5c946d5d1f963006f15d945a9f794.gif'"['https://habrastorage.org/getpro/habr/upload_files/0c4/f4b/cc8/0c4f4bcc894e7e51128132123b06d483.gif', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/847/ef3/266/847ef3266e992973d82069f3559871c2.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/414/ab0/c0c/414ab0c0c2d88f8769fab3b7c5e4520e.png', 'https://habrastorage.org/getpro/habr/company/439/17f/3a4/43917f3a42181b24938574547931179a.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/0d7/072/e15/0d7072e155f627786c91b41eca0563b3.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/d14/701/9df/d147019dfbcd4ab7689b282a722d5815.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/2bc/d68/8ca/2bcd688ca602ba78719230f947bcfedf.png', 'https://habrastorage.org/getpro/habr/upload_files/f61/a70/dde/f61a70dde945aeabbef76e9479e36878.gif', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/5fb/3e9/13e/5fb3e913ebb5d3afa074bc7afecebd0a.png', 'https://habrastorage.org/getpro/habr/upload_files/faa/5c9/46d/faa5c946d5d1f963006f15d945a9f794.gif', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/908/3ad/bb6/9083adbb67c4645290b3ee8fa33ae25b.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/e84/125/d2e/e84125d2e6bac7d5d6b130fd2c12d37a.png', 'https://habrastorage.org/getpro/habr/avatars/6f9/109/ce5/6f9109ce57a913501a91c65a04740f59.png', 'https://habrastorage.org/getpro/habr/upload_files/4a8/53b/24f/4a853b24f765097a4f2b14de8cf2302f.gif', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/a2a/5e1/e19/a2a5e1e19ea6a7c4ee29dc9e9257da94.png', 'https://habrastorage.org/r/w32/getpro/habr/avatars/6f9/109/ce5/6f9109ce57a913501a91c65a04740f59.png']"
7'720072'Тезисы с пресс-конференции «Руссофт» о развитии ИТ-рынка в 2022 году и прогнозах на 2022 год'2 марта в Москве прошла пресс-конференция «Руссофт» по теме «ИТ-индустрия в период перемен: что нас ждёт в 2023 году?», которую посетила информационная служба Хабра. В мероприятии приняли участие...'https://habr.com/ru/post/720072/'"2 марта в Москве прошла пресс-конференция «Руссофт» по теме «ИТ-индустрия в период перемен: что нас ждёт в 2023 году?», которую посетила информационная служба Хабра. В мероприятии приняли участие отечественные ИТ-компании: Vinteo, «Ред Софт», ISPsystem, «Базальт СПО», Content AI, «Группа Т1» и «Рексофт Консалтинг». Они отметили некоторые итоги своей работы и тенденции 2022 года, а также дали прогнозы на 2023 год. В материале представлены основные тезисы, озвученные представителями компаний, и ответы на вопросы журналистов.

Обсуждение открыл модератор пресс-конференции Валентин Макаров, президент «Руссофт». Он привёл первые предварительные данные по экспорту ИТ-отрасли за 2022 год для ознакомления с существующими условиями рынка и его действующими трендами.

«В 2022 году из России ушли центры разработки иностранных компаний, которые давали 10-12% всего экспорта ПО и услуг по его разработке. В связи с этим наши экспортёры пережили два шока за год: сначала санкции запретили иностранным клиентам работать с российскими компаниями, потом появился запрет на финансовые транзакции с Россией, который до сих пор создаёт много проблем как в работе с недружественными странами, так и с дружественными. Из-за первичных и вторичных санкций объём зарубежных продаж в недружественные страны снизился на 15%-20%. Параллельно с этим, те компании, которые остались в России, показывают рост зарубежных продаж за год до 5%. Наши экспортёры переориентировались на дружественные страны — в первую очередь, на страны ЕАЭС (Узбекистан, Казахстан, Республика Беларусь), Юго-Восточную Азию, Ближний Восток, Латинскую Америку. Другой тренд, который явно проявил себя в 2022 году, это реальное импортозамещение. Через Фонд РФРИТ государство выделило около 13 млрд рублей на импортозамещение ПО, что приведет к росту продаж и в значительной мере компенсирует снижение экспорта. Таким образом, общий объём продаж индустрии в 2022 снизился незначительно (не более 10%), а в 2023 году индустрия разработки ПО может выйти на показатели выше рекордного 2021 года».

Далее приводятся выжимки из выступлений и ответов на вопросы журналистов от спикеров мероприятия.

Фото: Руссофт

Олег Сажин, советник генерального директора Content AI

Уход зарубежных вендоров наряду со стартовавшей под эгидой Минцифры РФ политикой импортозамещения стали ключевыми событиями для ИТ-рынка России и определили общий вектор его движения. Практически в каждой области появились стартапы или сформировались новые ниши. В частности, основу Content AI составили бывшие сотрудники российского офиса международной группы ABBYY, ушедшей из России весной 2022 года.

В числе важных трендов на рынке Сажин отметил запрос бизнеса на «повышение интеллектуальности» ИТ-продуктов. Компании ищут комплексные решения, закрывающие сразу несколько операционных задач. При этом ПО должно легко интегрироваться в существующую среду и быть простым для настройки и управления.

По словам Сажина, на текущий момент важно говорить не просто об импортозамещении, то есть о воспроизведении решений, уже давно присутствующих на мировом рынке, а об опережающем развитии. Российские разработки должны быть устремлены в будущее, а бизнесу для повышения эффективности следует ориентироваться на передовые технологии на основе ИИ.

Блок с вопросами.

Западные IT-компании объявили о сокращении штата, не дойдет ли эта тенденция до нас?

Ответ: «Нам это в настоящее время точно не грозит. У российских ИТ-компаний большие амбиции и большое количество задач для реализации. Для их воплощения нужны профессионалы».

В последнее время много говорять о ИИ, есть ли у Российских разработчиков, чем ответить ChatGPT? Можно ли назвать российские проекты, их особенности?

Ответ: «В России есть сильные решения на базе технологий ИИ. Например, продукты Content AI в области интеллектуальной обработки информации, которые могут с высокой точностью распознавать и обрабатывать данные из любых типов документов — отсканированных файлов, фотографий и т.д.».

Как в целом развитие ИИ повлияет на отрасль, журналисты уже морально готовятся к тому, что их заменит ИИ, готовятся ли к этому программисты?

Ответ: «Пока общедоступные нейросети не способны выдавать окончательный, применимый в работе результат. Участие человека в финализации контента обязательно. Порой это участие — половина всей необходимой работы. Но нейросети стремительно развиваются, и мы вполне допускаем, что часть задач скоро можно будет полностью автоматизировать. От профессионалов в отрасли это потребует расширения собственных компетенций и умения использовать ИИ для их усиления».

Что дала Content AI смена собственника с российского на турецкого?

Ответ: «Смена собственника — мера техническая и временная. Она была необходима компании для решения вопросов лицензирования технологий и для выполнения всех обязательств перед клиентами».

В России довольно много пользователей используют пиратское ПО иностранного производства. Насколько серьёзно практика распространена относительно ПО российского производства?

Ответ: «По продуктам Content AI мы фиксируем единичные случаи пиратского использования. И даже в корпоративном сегменте. В таких случаях мы действуем в правовом поле. Большинство клиентов понимают преимущества лицензионного ПО для своего бизнеса: это и наличие технической поддержки, и комплексный консалтинг со стороны наших экспертов. Повышение комфорта интеграции и использования ПО, а также предоставление комплексных решений для глобальных задач бизнеса — это залог роста интереса к лицензионным продуктам».

Алексей Смирнов, председатель совета директоров «Базальт СПО»

Технологический суверенитет возможен при наличии собственных технологий, для ОС — собственной открытой инфраструктуры разработки. Проект «Сизиф», развёрнутый на серверной инфраструктуре «Базальт СПО», на протяжении 20 лет создает и развивает такую оригинальную инфраструктуру. Проект обладает собственной независимой пакетной базой.



Переход на российские решения — долгий процесс, где ключевую роль играет возможность работы в гетерогенной инфраструктуре, а также широкая база совместимости с другими российскими и международными продуктами. По словам Смирнова, «Базальт СПО» отвечает обоим требованиям: обладает развитой партнёрской экосистемой и инструментами постепенной миграции групповых политик Active Directory на оригинальные свободные решения управления сетями, в том числе крупными предприятиями и организациями.

Как ограничения по доступу России к современному железу повлияют на разработку и внедрение ПО? Не придётся ли в перспективе переписывать ПО под более слабые вычислительные мощности компьютеров? Не произойдет ли из-за этого ослабление экспортных возможностей российских разработчиков, которые просто не будут иметь возможности разрабатывать ПО под более мощные машины, поскольку не будут иметь к ним доступа?

Ответ: «У нас для разработки есть значительные вычислительные мощности, и мы их наращиваем. Риска деградации разработки из-за ограничения поставок железа не вижу, хотя пользователей они могут затронуть».

Павел Гуральник, генеральный директор ISPsystem

2022 год стал переломным для ИТ-отрасли. В 2023 продолжится активное развитие ИТ в РФ. Как указал Гуральник, России предстоит создать технические решения, необходимые для приобретения полноценного технологического суверенитета. Тенденция не замедлится, особенно на фоне спроса от рынка и поддержки, которая существует сейчас.

Также в области развития инсорсинговой разработки в госкорпорациях и госкомпаниях акцент делается на увеличение и развитие отделов внутренней разработки, которые обслуживают все департаменты. Этот подход влияет на конъюнктуру рыночной экономики в стране, указывает Гуральник. В долгосрочной перспективе его развитие к 2030 году может пагубно сказаться на эффективности развития всей индустрии. Гуральник уверен, что в условиях ограниченных кадровых, технологических и временных ресурсов крайне важно не дублировать усилия, а отдавать приоритет более эффективным участникам рынка.

Как ограничения по доступу России к современному железу повлияют на разработку и внедрение ПО? Не придётся ли в перспективе переписывать ПО под более слабые вычислительные мощности компьютеров? Не произойдет ли из-за этого ослабление экспортных возможностей российских разработчиков, которые просто не будут иметь возможности разрабатывать ПО под более мощные машины, поскольку не будут иметь к ним доступа?

Ответ: «Мы, как вендор, не видим критической разницы между зарубежным серверным оборудованием и отечественным. Наше ПО уже совместимо с серверной продукцией ряда российских производителей. Важно подчеркнуть, что, например для управления ЦОД не нужны высокие мощности: управление тысячами железных серверов происходит за счет малых ресурсов. Конечно, многое зависит от задачи заказчика, но мы не наблюдаем проблем».

Рустам Рустамов, заместитель генерального директора «РЕД СОФТ»

В 2022 году российская ИТ-отрасль столкнулась с беспрецедентными вызовами. Резкий уход западных компаний и необходимость в сжатые сроки перевести инфраструктуры на отечественные решения привели к росту спроса и к новым задачам, от оперативного решения которых зависят безопасность и жизнедеятельность РФ. Например, по мнению Рустамова, отсутствие возможности осуществить миграцию без выстраивания параллельной ИТ-инфраструктуры влекут за собой не только трату дополнительных ресурсов, но и риски, связанные с отсутствием «права на ошибку». Для выхода из сложившейся ситуации «РЕД СОФТ» разработала решение для «мягкой» миграции «РЕД АДМ».

Открытость и доступность для сторонних разработчиков ПО для поддержки их продуктов в RED ОС — только для больших компаний пока что или можно любому разработчику связаться и общаться с ними?

Ответ: «РЕД СОФТ открыта к сотрудничеству со всеми разработчиками отечественного программного обеспечения и готовы провести совместные работы по тестированию различного ПО с РЕД ОС с последующим подтверждением совместимости. Запросы можно отправлять по почте partner@red-soft.ru».

В конце января этого года Минцифры задумалось о введении доптребований к ПО на базе open source для включения в реестр отечественного ПО. Как вы относитесь к данной инициативе?

Ответ: «Безусловно, следить за чистотой реестра надо. Мы считаем важным, чтобы вендор обладал необходимой компетенцией, мог самостоятельно развивать продукт и обеспечивать техническую поддержку. Конкретные предлагаемые меры можно и нужно обсуждать отдельно».

В небольшом разговоре с информационной службой Хабра после конференции Рустамов отметил, что на текущий момент необходимо финансировать не разработку ПО, а его спрос. В частности, по его словам, «РЕД СОФТ» нуждается не в госфинансировании, а в потребителях. По мнению Рустамова, государству необходимо выделять средства различным структурам на смену ПО на отечественное, чтобы обеспечить российским компаниям рынки сбыта. Сейчас госструктурам, организациям и ведомствам не выделяют дополнительные средства на смену софта, бюджеты под это дело остаются прежними.

Илья Сивцев, генеральный директор ГК «Астра»

Вторая половина года стала ударной по количеству пилотов и запущенных проектов, указал Сивцев. ГК «Астра» серьёзно нарастила свою отраслевую экспертизу и продолжила осваивать новые сегменты. По её опыту, экосистемный подход отвечает требованиям заказчиков в реализации комплексных проектов. В компании усилили программный стек новыми системными и инфраструктурными решениями, синхронизировали дорожные карты всех продуктов, сделали ставку на серверную версию ОС Astra Linux, адаптированную под различные сценарии использования и высокий уровень нагрузок. Также компания нацелилась на развитие подхода «инфраструктура как код» — автоматизация в действии для повышения качества и скорости работы всех служб. ГК «Астра» интегрирует все продукты с собственными средствами защиты безопасности разработки, выпускает новые релизы. Также она серьёзно переработала архитектуру многих продуктов для соответствия требованиям еnterprise-заказчиков.

Как ограничения по доступу России к современному железу повлияют на разработку и внедрение ПО? Не придётся ли в перспективе переписывать ПО под более слабые вычислительные мощности компьютеров? Не произойдет ли из-за этого ослабление экспортных возможностей российских разработчиков, которые просто не будут иметь возможности разрабатывать ПО под более мощные машины, поскольку не будут иметь к ним доступа?

Ответ: «В настоящее время большинство классов программного обеспечения универсально и работает на различных поколениях процессоров. Замена процессора на более слабый или более мощный не требует переработки ПО или необходимости в его деградации. Более зависимыми от процессоров являются ОС, которым требуется более серьезная интеграция. Но мы используем в том числе наработки мирового опенсорс-сообщества, которые помогают оперативно решать вопрос совместимости с новыми поколениями процессоров».

Борис Попов, директор по развитию бизнеса компании Vinteo

Уход иностранных компаний с рынка РФ обеспечил отечественным ВКС-разработчикам карт-бланш – рост запросов со стороны заказчиков в 3–4 раза, увеличение доходов, поддержку государства, указал Попов. Стала очевидной и проблема отечественного сегмента видеокоммуникаций — отсутствие ВКС как готовой и массовой полнофункциональной услуги.

В 2023 году российский ВКС-рынок продолжит рост, уверен Попов. По его прогнозу, рост составит в среднем на 30%, расширится пул заказчиков, выбирающих отечественные решения. Один из главных трендов – упрощение. Заметно увеличится спрос на «лёгкие» решения веб-конференций, бесплатные или условно-бесплатные, позволяющие быстро собрать видеоконференцию любому пользователю. Сегмент классической ВКС на базе MCU останется для регламентированных мероприятий, требующих централизованного администрирования, качественного видео, бесперебойности работы и соблюдения мер безопасности.

Что мешает сейчас создать полноценную замену Zoom? Текущие решения от VK / «Яндекса» и «Сбера» всё равно сильно уступают, большинство компаний внутри всё ещё используют Zoom + MS Teams. Как это изменить, не прибегая к запретам?

Ответ: «Zoom - массовая полнофункциональная услуга ВКС. Чтобы такой сервис сделать у нас, нужно соединить разработчика ВКС и оператора связи, который и обеспечит ту самую массовость при небольшой стоимости для пользователей.



Также стоит учитывать технологическую особенность Zoom — это отдельное решение, сочетающее технологию классической видеоконференцсвязи на базе серверов MCU и упрощённые решения веб-конференций. Когда мы говорим про российский аналог Zoom нужно понимать, какие именно сервисы в нем будут. Если это максимально полный аналог оригинала, то без профессионального ВКС-решения не обойтись — в Zoom есть большая часть интеграции инфраструктуры с классической ВКС, которую нужно реализовывать на соответствующем решении.



Если мы говорим только о простейшей видеосвязи в формате «говорящей головы», то подойдут как раз уже представленные на рынке решения «Яндекс Телемост», VK Teams. То есть на российском рынке сейчас все эти элементы Zoom (классическая ВКС, веб-конференция и оператор связи) представлены отдельно и каждый делает свою видеосвязь, а нужно их соединить вместе в одном сервисе».

Антон Якимов, заместитель гендиректора «Группы T1» по технологическому развитию

По мнению Якимова, 2022 год показал, что злоумышленники становятся организованнее, современный ландшафт киберугроз быстро трансформируется и усложняется. Обеспечение информационной безопасности в текущих условиях требует от отечественных поставщиков решений и компаний дополнительных совместных усилий по всем направлениям: от исследовательской работы и ускоренной разработки собственных ИБ-продуктов до перенастройки бизнес-процессов с точки зрения управления киберрисками. Уже сейчас при проектировании ИБ-систем надо учитывать появление новых типов угроз, к которым можно уверенно отнести использование ИИ и квантовых технологий в преступных целях.

Где брать кадры для работы в ИБ? Из каких сфер? Обучать с нуля, обучать айтишников?

Ответ: «Для решения этой задачи необходим комплексный подход. Важно взращивать собственные кадры, при этом бизнес должен оказывать помощь ВУЗам, например, вести ИБ-курсы для студентов, поскольку «мощностей» ВУЗов сегодня не хватает. Необходимо также переквалифицировать заинтересованных в ИБ специалистов из других ИТ-сфер. Традиционно направление информационной безопасности является очень закрытым, в результате чего ИТ-сотрудники из смежных профессий имеют недостаточное представление о специфике деятельности в сфере ИБ. Рассказывая об этом направлении подробнее, мы сможем заинтересовать и стимулировать переход в данную сферу других ИТ-специалистов, в первую очередь разработчиков, архитекторов, DevOps-инженеров».

Алексей Богомолов, директор практики «Стратегия трансформации» компании «Рексофт Консалтинг»

Богомолов обратил внимание на проблему экспорта отечественных ИТ-решений. По его мнению, российским разработчикам нужно ориентироваться на наиболее перспективные технологии: прикладные решения с использованием ИИ, кибербезопасность, облачные сервисы.

В области прикладных решений сосредоточен опережающий рост в 9,3% при общем росте сектора в 2,4%. Международный рынок приложений и услуг разработки ИИ в 2022 году совокупно вырос до $200 млрд. При консервативном сценарии Ближний Восток, Африка, Азия, Латинская Америка (потенциальный рынок для российского ПО и услуг) составляют 15% от общемирового объёма, что весьма перспективно, указал Богомолов. У российских игроков есть возможность проявить себя в нише платформенного ПО для решений промышленного ИИ — на глобальных рынках общий рост этого сегмента составил более 30%. Объединение существующих наработок, опыта и системная поддержка от государства создают для этого опережающий темп. По словам Богомолова, «дружественные» рынки высококонкурентны, что создаёт необходимость в чётком позиционировании и высоком качестве ПО».

Модератор встречи, Валентин Макаров, подвёл итоги выступлений спикеров. «Первый важный тренд связан с переосмыслением целей и порядка реализации импортозамещения ПО. Произошло понимание того, что невозможно и не нужно проводить тотальное замещение импортного ПО. Нужно выстраивать приоритеты импортозамещения ПО на конкретных объектах, иметь возможность поддерживать существующие системы и параллельно строить замещающие стеки ПО, создавая конвергентную среду, которая со временем будет переходить на российские решения. Нам необходим плавный переход. Второй тренд – консолидация сообщества ИТ-компаний для выстраивания комплексных решений импортозамещения ПО. Компании идут в этом направлении разными путями, либо формируя консорциумы, обеспечивающие совместимость решений разных независимых вендоров, либо формируют вертикально интегрированные холдинги. Формирование комплексных решений и сообществ взаимодействующих компаний должно быть направлено не только на замещение импортных платформ, но и на обязательное продвижение этих решений на глобальный «рынок большинства». Следующий тренд — это экспорт. Благодаря потребности дружественных стран в достижении технологического суверенитета в области ИТ по примеру России, наша индустрия способна занять 15% мирового рынка ПО и услуг по его разработке. Это такой потенциал, о котором мы раньше даже не мечтали. Экспорт ИТ может занять самую большую долю мирового рынка среди всех российских экспортных отраслей. Возможность достичь новые рынки лежит в продвижении платформенных решений нового технологического уклада. Эти платформы должны базироваться на достижении альтернативного уровня информационной и кибербезопасности, используя квантовую криптографию и постквантовое шифрование, применение искусственного интеллекта. Необходимо менять и модель ведения бизнеса в дружественных странах, предлагая партнёрам обучение работе с нашими платформами и вовлекая их в процесс поддержки и развития платформ. Эти страны смотрят на нас как на источник технологической независимости, поэтому хотят получить наш опыт, чтобы так же, как и мы, стать независимыми от «недружественных стран». Через партнёрство и создание новых моделей ведения бизнеса с Китаем и Индией, другими странами БРИКС, мы сможем стать игроками мирового уровня, расширяя наше общее влияние на другие регионы. У России есть все условия для этого, поэтому важно получить поддержку экспорта на государственном уровне, чтобы продвижение экспорта ИТ стало приоритетом Национальных проектов и программ в области цифровой экономики».

Если у вас есть вопросы в адрес компаний, можете оставлять их в комментариях, передадим спикерам."'https://habrastorage.org/getpro/habr/upload_files/7ab/7a7/17b/7ab7a717b0e086f22267ff7c3e24c16f.jpeg'"['https://habrastorage.org/getpro/habr/upload_files/7ab/7a7/17b/7ab7a717b0e086f22267ff7c3e24c16f.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/7ab/7a7/17b/7ab7a717b0e086f22267ff7c3e24c16f.jpeg', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w32/getpro/habr/avatars/58c/384/5bb/58c3845bbfabf731f299673268716e07.jpg', 'https://habrastorage.org/getpro/habr/upload_files/c29/50c/61c/c2950c61c33faf6dc0c8707919095e55.JPG', 'https://habrastorage.org/getpro/habr/avatars/58c/384/5bb/58c3845bbfabf731f299673268716e07.jpg']"
8'720664'Обзорная статья о видах тестирования IT-продуктов'На курсе, где я учился frontend-разработке, нас познакомили только с unit тестированием. Но уже на первом месте работы, я столкнулся и с регрессионным тестированием, и с автотестами, и с E2E-тестами....'https://habr.com/ru/post/720664/'"На курсе, где я учился frontend-разработке, нас познакомили только с unit тестированием. Но уже на первом месте работы, я столкнулся и с регрессионным тестированием, и с автотестами, и с E2E-тестами. Мне было сложно понять, чем они отличаются, какие еще есть виды тестирования и кто их должен писать. Эта статья для начинающих разработчиков, которые задаются подобными вопросами.

Модульное тестирование (Unit testing)

Модульное тестирование выполняется на уровне отдельных блоков приложения. Это может быть тест, который проверяет корректность работы отдельной функции или React-компонента.

Пример. Функция на JavaScript принимает два числа и возвращает сумму.

Пример модульного теста, который проверяет работу этой функции:

function sum(a, b) { return a + b; } describe('sum', function() { it('should return 4 when 2 and 2 are passed', function() { expect(sum(2, 2)).toEqual(4); }); it('should return 10 when 7 and 3 are passed', function() { expect(sum(7, 3)).toEqual(10); }); it('should return -3 when -5 and 2 are passed', function() { expect(sum(-5, 2)).toEqual(-3); }); });

Кто пишет тесты: модульные тесты обычно пишет сам автор кода.

Примеры инструментов:

ПРИМЕЧАНИЕ: я написал обзорную статью на библиотеку React-testing-library, в ней подробно рассказывается, как покрыть unit-тестами React-компонент.

Интеграционное тестирование (Integration testing)

Интеграционное тестирование проверяет правильность взаимодействия узлов IT-продукта. При проведении интеграционного тестирования необходимо убедиться в том, что каждый компонент приложения работает корректно при определенном сценарии. Веб-интерфейс должен правильно отправлять запросы к API, API должен правильно обрабатывать запросы и взаимодействовать с базой данных, а база данных сохранять нужную информацию.

Кроме того, нужно убедиться в том, что приложение работает корректно в случае возникновения ошибок, например, при отсутствии соединения с базой данных.

ВАЖНО: Корректность обработки ошибок - это часть любого вида тестирования.

Интеграционное тестирование выполняется как вручную, так и автоматизированно с использованием специальных инструментов, таких как Postman или SoapUI.

Пример. Приложение для онлайн-оплаты, которое включает в себя: веб-интерфейс, API для обработки платежей и базу данных для хранения информации о платежах. Интеграционное тестирование поможет убедиться в том, что компоненты работают корректно вместе.

Один из возможных сценариев тестирования:

Запрос на создание нового платежа отправляется через веб-интерфейс. API для обработки платежей получает запрос и создает новую запись в базе данных. Веб-интерфейс получает уведомление о создании нового платежа и отображает его в списке платежей на странице. Пользователь проверяет, что новый платеж отображается на странице, и подтверждает его.

Кто пишет тесты: Интеграционные тесты обычно пишутся командой QA-инженеров.

Примеры инструментов:

Функциональное тестирование (Functional testing)

Функциональное тестирование проводится для проверки функциональности приложения. Оно позволяет убедиться в том, что приложение работает корректно и выполняет функции, соответствующие требованиям пользователей и заказчика.

Функциональное тестирование проверяет отдельные функции и возможности приложения, а интеграционное тестирование проверяет взаимодействие компонентов системы в целом.

Пример. Веб-приложение для онлайн-бронирования номеров в отеле. Функциональное тестирование поможет убедиться в том, что приложение работает корректно и выполняет свои функции.

Возможный сценарий тестирования:

Пользователь открывает веб-страницу приложения и выбирает нужную дату заезда и выезда. Приложение отображает свободные номера в отеле на выбранные даты. Пользователь выбирает номер и вводит свои данные для бронирования. Приложение подтверждает бронирование и отправляет подтверждение на электронную почту пользователя.

Кто пишет тесты: функциональные тесты обычно пишутся командой QA-инженеров.

Примеры инструментов:

ПРИМЕЧАНИЕ: Пара слов о End-to-End(E2E) тестировании, это тестирование, которое позволяет проверить работу всей системы или приложения с точки зрения пользователя от начала до конца (от ""end"" - начала, до ""end"" - конца). Оно включает в себя проверку основных сценариев использования приложения - от взаимодействия пользователя с интерфейсом до проверки корректности ответа сервера. Также при E2E тестировании проверяются функциональные возможности приложения, такие как формирование отчетов, работа с базами данных и другие. Таким образом, E2E тестирование можно рассматривать и как функциональное и как интеграционное.

Регрессионное тестирование (Regression testing)

Регрессионное тестирование проводится после внесения изменений в приложение и позволяет убедиться в том, что уже существующая функциональность продукта продолжает работать корректно после изменений.

Регрессионное тестирование и функциональное тестирование имеют схожие, но все же разные цели и задачи.

Функциональное тестирование проверяет, что приложение соответствует требованиям, которые описаны в функциональных спецификациях.

Регрессионное тестирование проверяет, что изменения в приложении не повлияли на уже существующую функциональность и не вызвали регрессию (возврат к более ранней стадии разработки).

Пример. Онлайн-магазин, в котором можно выбирать товары, добавлять их в корзину и оформлять заказы. После выпуска новой версии нужно убедиться, что функциональные возможности, которые работали в предыдущей версии, продолжают работать корректно в новой версии. Для этого проводится регрессионное тестирование.

Функции которые нужно проверить:

Поиск товаров по названию. Добавление товаров в корзину. Удаление товаров из корзины. Редактирование корзины перед оформлением заказа. Оформление заказа с выбранными товарами.

Для каждой из этих функций нужны тесты, которые будут проверять правильность работы приложения в новой версии. Успешное прохождение тестов подтвердит, что приложение работает корректно и что пользователи не будут сталкиваться с проблемами при использовании сайта.

Кто пишет тесты: регрессионные тесты обычно пишутся командой QA-инженеров.

Примеры инструментов:

Нагрузочное тестирование (Load testing)

Нагрузочное тестирование проводится для определения максимальной нагрузки, которую может выдержать приложение. В процессе проверяется производительность приложения и выявляются возможные проблемы в работе при большой нагрузке.

Пример. Необходимо убедиться, что онлайн-магазин продолжает работать корректно, когда к нему обращается большое количество пользователей.

Для нагрузочного тестирования создается тест-сценарий, который имитирует действия пользователя на сайте, например:

Пользователь открывает главную страницу магазина. Пользователь ищет товары по определенным категориям. Пользователь добавляет товары в корзину. Пользователь оформляет заказ.

Тест-сценарий запускается под разной нагрузкой, например, с одновременным выполнением скрипта на 100, 500 и 1000 пользователей. Анализ результатов тестирования помогает определить, как много пользователей приложение может обрабатывать одновременно, не замедляя работу и не выходя из строя.

Кто пишет тесты: нагрузочные тесты обычно пишутся командой QA-инженеров.

Примеры инструментов:

Тестирование на производительность (Performance testing)

Тестирование на производительность проверяет производительность продукта при различных нагрузках и условиях использования. Цель - убедиться в том, что продукт может обрабатывать большое количество запросов сохраняя скорость и стабильность.

Отличие между тестированием на производительность и нагрузочным тестированием заключается в целях тестирования.

Цель тестирования на производительность - проверить, как быстро работает приложение и с какой скоростью обрабатывает определенный объем данных. Например проверяется скорость открытия страницы, время загрузки данных и т.д.

Нагрузочное тестирование, проверяет как много пользователей может использовать приложение одновременно без существенного замедления работы или падения производительности.

Пример. Как пример теста на производительность используем пример нагрузочного тестирования. Те же условия, тот же тест-сценарий, но главное отличие будет в фокусе тестирования, т.е. в том, на какие показатели будут смотреть тестировщики.

Тестирование на производительность и тестирование на нагрузку могут быть взаимосвязаны и часто проводятся вместе.

Кто пишет тесты: тестирование на производительность обычно пишет команда QA-инженеров.

Примеры инструментов:

Автоматизированное тестирование (Automated testing)

Автоматизированное тестирование - это способ проведения тестирования. Пример интеграционного тестирования, описанный выше, можно выполнить вручную, без использования специальных инструментов, а можно автоматизировать. Для автоматизации используются специальные инструменты и программы.

Автотесты имеют ряд преимуществ перед ручным тестированием, например:

Эффективность и экономия времени: выполняются быстрее, чем ручные, и могут работать 24/7 без остановки.

Повторяемость: можно повторять бесконечное количество раз с одинаковой точностью и результатами.

Объективность: исключается влияние субъективного мнения и человеческих ошибок.

Охват: могут охватывать большее количество функций приложения, чем это возможно в ручном режиме.

Пример: примером может быть любой из приведенных выше, если это тестирование было автоматизировано.

Кто пишет тесты: Автоматизированные тесты обычно пишутся командой QA-инженеров.

Примеры инструментов:

Заключение

В видах тестирования легко запутаться. Во-первых их часто пишут одни и те же люди, во-вторых, у них могут совпадать инструменты, и наконец, даже сами тесты могут быть полностью идентичными. Чтобы понять, какой вид тестирования перед вами, важно выявить какие цели оно преследует.

Модульное тестирование. Цель - протестировать отдельные блоки приложения.

Интеграционное тестирование. Цель - проверить взаимодействие компонентов приложения.

Функциональное тестирование. Цель - проверить работу всех заявленных функций приложения.

Регрессионное тестирование. Цель - проверить, что после изменений уже существующий функционал не сломался.

Нагрузочное тестирование. Цель - проверить максимальную нагрузку , при которой приложение работает корректно.

Тестирование на производительность. Цель - проверить скорость и корректность работы приложения при разной нагрузке.

Автоматизированное тестирование - способ тестирования, при котором тестирование выполняется автоматически с использованием специальных инструментов.

Я постарался описать не все виды тестирования, а те, с которыми, на мой взгляд, чаще всего сталкиваются начинающие разработчики. Если вам интересно углубиться в тему, вот неполный список других видов тестирования:

Список Тестирование безопасности (Security Testing) Тестирование доступности (Accessibility Testing) Тестирование совместимости (Compatibility Testing) Тестирование локализации (Localization Testing) Тестирование отказоустойчивости (Fault Tolerance Testing) Тестирование масштабируемости (Scalability Testing) Тестирование надежности (Reliability Testing)

В преддверии старта специализации Fullstack Developer от OTUS хочу порекомендовать вам несколько бесплатных уроков, которые будут полезны начинающим fullstack-разработчикам."'https://habrastorage.org/getpro/habr/upload_files/f11/441/d38/f11441d3888ab53eada68b20b4b767de.png'"['https://habrastorage.org/r/w1560/getpro/habr/upload_files/7c2/281/854/7c2281854c921d1e98790ca7cad5fdc2.png', 'https://habrastorage.org/getpro/habr/upload_files/f11/441/d38/f11441d3888ab53eada68b20b4b767de.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/company/2d5/0ed/b57/2d50edb57cf45fa07cc4f39f53b78395.png', 'https://habrastorage.org/getpro/habr/avatars/0fa/ede/719/0faede71987e6ce5757200da00052b02.jpg', 'https://habrastorage.org/r/w32/getpro/habr/avatars/0fa/ede/719/0faede71987e6ce5757200da00052b02.jpg']"
9'712014'[Перевод] ACI VRF leaking'Некоторые люди говорят, что BGP — сложный протокол (я бы поспорил, особенно в сравнении с OSPF). Однако я ни разу не встречал никого, кто считал бы простым ACI,...'https://habr.com/ru/post/712014/'"Некоторые люди говорят, что BGP — сложный протокол (я бы поспорил, особенно в сравнении с OSPF). Однако я ни разу не встречал никого, кто считал бы простым ACI, если, конечно, не брать в расчёт маркетинг. ACL и prefix‑lists являются частью программы CCNA; а вот контрактам ACI посвящён аж целый white paper. Долгое время для меня оставались загадкой детали реализации inter‑VRF контрактов. Не поймите меня неправильно — необходимая настройка подробно описана в документации, однако мне было подчас непонятно, почему нужны те или иные шаги. Сегодня я бы хотел поделиться парой‑тройкой своих находок по этой теме.

Топология весьма проста:

Host — это L3 коммутатор, который выполняет роль provider (VRF Provider) и consumer (VRF Consumer). В плане маршрутизации ACI выступает шлюзом по умолчанию:

Host# show run vrf Provider interface Ethernet1/1.100 vrf member Provider vrf context Provider ip route 0.0.0.0/0 192.168.1.254 address-family ipv4 unicast ip route 0.0.0.0/0 192.168.1.254 vrf Provider Host# show ip interface brief vrf Provider IP Interface Status for VRF "" Provider ""(47) Interface IP Address Interface Status Eth1/1.100 192.168.1.1 protocol-up/link-up/admin-up Host# show run vrf Consumer interface Ethernet1/2.100 vrf member Consumer vrf context Consumer ip route 0.0.0.0/0 192.168.2.254 address-family ipv4 unicast ip route 0.0.0.0/0 192.168.2.254 vrf Consumer Host# show ip interface brief vrf Consumer IP Interface Status for VRF "" Consumer ""(48) Interface IP Address Interface Status Eth1/2.100 192.168.2.1 protocol-up/link-up/admin-up

Что касается ACI, нам нужны только пара EPG, BD и VRF в рамках одного tenant, а также некоторая настройка Access Policies.

Модуль Tenant:

resource ""aci_tenant"" ""TestTenant"" { name = ""TestTenant"" } resource ""aci_vrf"" ""TestVrf1"" { tenant_dn = aci_tenant.TestTenant.id name = ""TestVrf1"" } resource ""aci_vrf"" ""TestVrf2"" { tenant_dn = aci_tenant.TestTenant.id name = ""TestVrf2"" } resource ""aci_bridge_domain"" ""TestBD1"" { tenant_dn = aci_tenant.TestTenant.id name = ""TestBD1"" relation_fv_rs_ctx = aci_vrf.TestVrf1.id } resource ""aci_subnet"" ""Subnet1"" { parent_dn = aci_application_epg.Provider.id ip = ""192.168.1.254/24"" scope = [""private"", ""shared""] } resource ""aci_bridge_domain"" ""TestBD2"" { tenant_dn = aci_tenant.TestTenant.id name = ""TestBD2"" relation_fv_rs_ctx = aci_vrf.TestVrf2.id } resource ""aci_subnet"" ""Subnet2"" { parent_dn = aci_bridge_domain.TestBD2.id ip = ""192.168.2.254/24"" scope = [""private"", ""shared""] } resource ""aci_application_profile"" ""TestAP"" { tenant_dn = aci_tenant.TestTenant.id name = ""TestAP"" } resource ""aci_application_epg"" ""Provider"" { application_profile_dn = aci_application_profile.TestAP.id name = ""Provider"" relation_fv_rs_bd = aci_bridge_domain.TestBD1.id } resource ""aci_application_epg"" ""Consumer"" { application_profile_dn = aci_application_profile.TestAP.id name = ""Consumer"" relation_fv_rs_bd = aci_bridge_domain.TestBD2.id } resource ""aci_epg_to_domain"" ""ProviderDomain"" { application_epg_dn = aci_application_epg.Provider.id tdn = aci_physical_domain.PhysicalDomain.id } resource ""aci_epg_to_domain"" ""ConsumerDomain"" { application_epg_dn = aci_application_epg.Consumer.id tdn = aci_physical_domain.PhysicalDomain.id }

Модуль Access Policies:

resource ""aci_vlan_pool"" ""TestPool"" { name = ""TestPool"" alloc_mode = ""static"" } resource ""aci_ranges"" ""range_1"" { vlan_pool_dn = aci_vlan_pool.TestPool.id from = ""vlan-1"" to = ""vlan-1000"" alloc_mode = ""static"" } resource ""aci_physical_domain"" ""PhysicalDomain"" { name = ""PhysicalDomain"" relation_infra_rs_vlan_ns = aci_vlan_pool.TestPool.id } resource ""aci_attachable_access_entity_profile"" ""TestAAEP"" { name = ""TestAAEP"" } resource ""aci_aaep_to_domain"" ""PhysicalDomain-to-TestAAEP"" { attachable_access_entity_profile_dn = aci_attachable_access_entity_profile.TestAAEP.id domain_dn = aci_physical_domain.PhysicalDomain.id } resource ""aci_leaf_interface_profile"" ""TestInterfaceProfile"" { name = ""TestInterfaceProfile"" } resource ""aci_access_port_block"" ""TestAccessBlockSelector"" { access_port_selector_dn = aci_access_port_selector.TestAccessPortSelector.id name = ""TestAccessBlockSelector"" from_card = ""1"" from_port = ""2"" to_card = ""1"" to_port = “2"" } resource ""aci_access_port_selector"" ""TestAccessPortSelector"" { leaf_interface_profile_dn = aci_leaf_interface_profile.TestInterfaceProfile.id name = ""TestAccessPortSelector"" access_port_selector_type = ""range"" relation_infra_rs_acc_base_grp = aci_leaf_access_port_policy_group.TestAccessInterfacePolicy.id } resource ""aci_leaf_access_port_policy_group"" ""TestAccessInterfacePolicy"" { name = ""TestAccessInterfaceProfile"" relation_infra_rs_att_ent_p = aci_attachable_access_entity_profile.TestAAEP.id } resource ""aci_leaf_profile"" ""TestSwitchProfile"" { name = ""TestSwitchProfile"" leaf_selector { name = ""LeafSelector"" switch_association_type = ""range"" node_block { name = ""Block1"" from_ = ""101"" to_ = ""102"" } } relation_infra_rs_acc_port_p = [aci_leaf_interface_profile.TestInterfaceProfile.id] }

Подсеть, в которой находится provider, должна быть задана в EPG вместо BD. Поскольку мы используем разные EPG, нужно определить контракт, чтобы установить между ними связность.

Модуль Contract:

resource ""aci_application_epg"" ""Provider"" { application_profile_dn = aci_application_profile.TestAP.id name = "" Provider"" relation_fv_rs_bd = aci_bridge_domain.TestBD1.id relation_fv_rs_prov = [aci_contract.TestContract.id] } resource ""aci_application_epg"" ""Consumer"" { application_profile_dn = aci_application_profile.TestAP.id name = "" Consumer"" relation_fv_rs_bd = aci_bridge_domain.TestBD2.id relation_fv_rs_cons = [aci_contract.TestContract.id] } resource ""aci_contract"" ""TestContract"" { tenant_dn = aci_tenant.TestTenant.id name = ""TestContract"" scope = ""tenant"" } resource ""aci_contract_subject"" ""TestSubject"" { contract_dn = aci_contract.TestContract.id name = ""TestSubject"" } resource ""aci_contract_subject_filter"" ""PermitIPSubj"" { contract_subject_dn = aci_contract_subject.TestSubject.id filter_dn = aci_filter.PermitIPFilter.id } resource ""aci_filter"" ""PermitIPFilter"" { tenant_dn = aci_tenant.TestTenant.id name = ""PermitIPFilter"" } resource ""aci_filter_entry"" ""PermitIPFilterEntry"" { filter_dn = aci_filter.PermitIPFilter.id name = ""permit_ip "" ether_t = ""ip"" }

Как только мы применим этот контракт, Consumer сможет общаться с Provider:

Host# traceroute 192.168.1.1 vrf Consumer traceroute to 192.168.1.1 (192.168.1.1), 30 hops max, 40 byte packets 1 192.168.2.254 (192.168.2.254) 1.946 ms 0.758 ms 0.691 ms 2 192.168.1.254 (192.168.1.254) 2.231 ms 0.708 ms 0.705 ms 3 192.168.1.1 (192.168.1.1) 0.708 ms 0.577 ms 0.578 ms

На данном этапе настройки корректны, поэтому мы можем перейти к наблюдениям. Почему необходимо определять подсеть provider в настройках EPG, а не BD? В классической настройке L3VPN нет подобного требования, поэтому, должно быть, оно относится сугубо к ACI. Рассмотрим, как именно происходит маршрутизация трафика:

leaf-102# show ip route vrf TestTenant:TestVrf2 <output omitted> 192.168.1.0/24, ubest/mbest: 1/0, attached, direct, pervasive *via 10.0.88.66%overlay-1, [1/0], 00:07:29, static, tag 4294967294 192.168.2.0/24, ubest/mbest: 1/0, attached, direct, pervasive, dcs *via 10.0.88.66%overlay-1, [1/0], 00:11:01, static, tag 4294967294 192.168.2.254/32, ubest/mbest: 1/0, attached, pervasive *via 192.168.2.254, Vlan11, [0/0], 00:11:01, local, local leaf-102# leaf-102# show ip route vrf TestTenant:TestVrf2 192.168.1.0/24 det <output omitted> 192.168.1.0/24, ubest/mbest: 1/0, attached, direct, pervasive *via 10.0.88.66%overlay-1, [1/0], 00:07:38, static, tag 4294967294 recursive next hop: 10.0.88.66/32%overlay-1 vrf crossing information: VNID:0x238000 ClassId:0x2ab4 Flush#:0x1

Обратите внимание, что подсеть Provider доступна через статический маршрут с парой необычных атрибутов. Во-первых, next-hop – это адрес anycast IP for IPv4 hardware proxy:

spine-201# show ip interface lo9 IP Interface Status for VRF ""overlay-1"" lo9, Interface status: protocol-up/link-up/admin-up, iod: 81, mode: anycast-v4 IP address: 10.0.88.66, IP subnet: 10.0.88.66/32 IP broadcast address: 255.255.255.255 IP primary address route-preference: 0, tag: 0

Чтобы proxy обработал пакет в правильном VRF, consumer leaf переписывает VNID так, чтобы тот попал в provider VRF (0x238000 = 2326528):

Оффтоп: для NX‑OS VXLAN характерно ровно противоположное поведение, если не брать в расчёт downstream VNI.

Inter‑VRF контракт ВСЕГДА применяет именно consumer leaf. Однако такой подход должен бы сломать conversation‑based forwarding: consumer начинает транзакцию, поэтому он не может предварительно получить пакет от provider, чтобы запомнить его pcTag. Решение очевидно: consumer должен заранее знать pcTag, относящийся к provider. Именно этот факт отражается в виде необходимости настраивать подсеть provider в EPG: как только контракт становится активен, APIC настраивает на consumer leaf статический маршрут с перезаписью VNID и provider pcTag, который в RIB называется ClassID (0×2ab4 = 10 932):

В результате consumer leaf обладает всей необходимой информацией, чтобы передать пакет в provider VRF и применить правильные политики:

leaf-102# show zoning-rule scope 2719744 +---------+--------+--------+----------+----------------+---------+---------+-------------------------+----------+------------------------+ | Rule ID | SrcEPG | DstEPG | FilterID | Dir | operSt | Scope | Name | Action | Priority | +---------+--------+--------+----------+----------------+---------+---------+-------------------------+----------+------------------------+ | 4101 | 0 | 15 | implicit | uni-dir | enabled | 2719744 | | deny,log | any_vrf_any_deny(22) | | 4100 | 0 | 0 | implarp | uni-dir | enabled | 2719744 | | permit | any_any_filter(17) | | 4099 | 0 | 0 | implicit | uni-dir | enabled | 2719744 | | deny,log | any_any_any(21) | | 4098 | 0 | 49153 | implicit | uni-dir | enabled | 2719744 | | permit | any_dest_any(16) | | 4102 | 10932 | 49154 | 4 | uni-dir-ignore | enabled | 2719744 | TestTenant:TestContract | permit | fully_qual(7) | | 4103 | 49154 | 10932 | 4 | bi-dir | enabled | 2719744 | TestTenant:TestContract | permit | fully_qual(7) | | 4104 | 10932 | 0 | implicit | uni-dir | enabled | 2719744 | | deny,log | shsrc_any_any_deny(12) | +---------+--------+--------+----------+----------------+---------+---------+-------------------------+----------+------------------------+

Что насчёт обратного трафика от provider EPG?

leaf-101# show ip route vrf TestTenant:TestVrf1 192.168.2.0/24 det <output omitted> 192.168.2.0/24, ubest/mbest: 1/0, attached, direct, pervasive *via 10.0.88.66%overlay-1, [1/0], 00:01:13, static, tag 4294967294 recursive next hop: 10.0.88.66/32%overlay-1 vrf crossing information: VNID:0x298000 ClassId:0 Flush#:0

Можно догадаться, что мы найдём похожий маршрут для consumer EPG:

Он указывает на Anycast IPv4 hardware proxy address. Он задаёт корректной значение для перезаписи VNID.

Однако ClassID равен нулю. Означает ли это, что provider leaf не применяет политику? Действительно:

leaf-101# show zoning-rule scope 2326528 +---------+--------+--------+----------+---------+---------+---------+------+----------+----------------------+ | Rule ID | SrcEPG | DstEPG | FilterID | Dir | operSt | Scope | Name | Action | Priority | +---------+--------+--------+----------+---------+---------+---------+------+----------+----------------------+ | 4101 | 0 | 16387 | implicit | uni-dir | enabled | 2326528 | | permit | any_dest_any(16) | | 4098 | 0 | 0 | implicit | uni-dir | enabled | 2326528 | | deny,log | any_any_any(21) | | 4099 | 0 | 0 | implarp | uni-dir | enabled | 2326528 | | permit | any_any_filter(17) | | 4100 | 0 | 15 | implicit | uni-dir | enabled | 2326528 | | deny,log | any_vrf_any_deny(22) | | 4102 | 10932 | 14 | implicit | uni-dir | enabled | 2326528 | | permit_override | src_dst_any(9) | +---------+--------+--------+----------+---------+---------+---------+------+----------+----------------------+

Ненулевые значения pcTag в zoning table либо являются зарезервированными, либо относятся к BD:

Декодирование остальных записей в zoning table я оставлю в качестве упражнения (вначале можно изучить этот раздел).

Стоит подчеркнуть, что inter‑VRF трафик не попадает под endpoint learning ни для одного из направлений передачи. Такой подход позволяет заставить leaf‑коммутаторы всегда использовать статический маршрут, а значит, применять правильные политики и корректно переписывать VNID. Тут есть неочевидное следствие: inter‑VRF трафик всегда проходит через spine, даже если provider и consumer подключены к одному и тому же leaf.

Я надеюсь, теперь очевидно, что ACI — это очень сложная система с множеством внутренних нюансов. Впрочем, это не является негативной характеристикой; в конце концов, компьютеры включают в себя существенно больше элементов, чем стрелы из Каменного века. Однако для операторов ACI это неплохой повод помнить про сложность системы в целом, а также придерживаться опубликованных рекомендаций, предварительно протестировав всё на соответствие требованиям функциональности и производительности. В противном случае можно оказаться в серой зоне, что потенциально приведёт к малоприятной необходимости переделывать дизайн всей системы с нуля.

Спасибо за рецензию: Анастасии Куралёвой

Канал в Телеграме: https://t.me/networking_it_ru"'https://habrastorage.org/getpro/habr/upload_files/a52/881/7e3/a528817e3d496b8ecf05ded670d13d40.png'"['https://habrastorage.org/r/w1560/getpro/habr/upload_files/7d9/839/4d1/7d98394d1a821c710959c2eeb09063c1.png', 'https://habrastorage.org/getpro/habr/upload_files/a52/881/7e3/a528817e3d496b8ecf05ded670d13d40.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/avatars/dd3/012/c1f/dd3012c1f1f30f8ceffd158800dfd2e9.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/789/89b/e16/78989be1687e1f02796de98c5e4a5c2c.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/d88/66a/b17/d8866ab175105b2f7772be6c742dc963.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/6c1/7e0/166/6c17e01665bb5d354244eb053d3e8ea4.png', 'https://habrastorage.org/r/w32/getpro/habr/avatars/dd3/012/c1f/dd3012c1f1f30f8ceffd158800dfd2e9.jpg']"
10'720896'Как избежать проблем при запуске MVP'Всем привет! Меня зовут Алексей Половинкин, и я отвечаю за Python в AGIMA . За последние 2 года мне повезло запускать сразу 2 крупных MVP-проекта: классифайд автомобилей для Казахстана и проект в...'https://habr.com/ru/post/720896/'"Всем привет! Меня зовут Алексей Половинкин, и я отвечаю за Python в AGIMA. За последние 2 года мне повезло запускать сразу 2 крупных MVP-проекта: классифайд автомобилей для Казахстана и проект в сфере телемедицины. За это время у меня и моей команды накопилось много опыта по запуску подобных проектов, и им хотелось бы поделиться. В этой статье рассказываю, как не допускать ошибок на этапе MVP и какие практики полезно внедрять сразу.

Почему MVP — это важно

Задача любого MVP (Minimal Viable Product) — быстро вывести на рынок решение, охватить максимальную аудиторию, привлечь клиентов. И ключевые факторы в запуске MVP-продукта — именно скорость вывода продукта на рынок.

MVP должен быть качественным. На сайте проекта пользователь не должен увидеть потекшую верстку и миллион ошибок — иначе он навсегда покинет страницу. То же касается внутренних систем. Человек должен без проблем пройти все ключевые шаги в системе:

зарегистрироваться;

авторизоваться;

увидеть объявления или подобрать специалиста;

увидеть интеграцию с CRM;

получить СМС или пуш и т. п.

Все основные шаги пользователя должны работать без багов. Особенно это касается вопроса оплаты. Любой баг в MVP — это боль, и их должно быть мало. Мы можем закрыть глаза на шероховатости процесса или ручные действия, но всё должно работать.

Поэтому в MVP и должно быть минимальное количество жизнеспособных функций. Вы быстро выводите качественный продукт с ограниченной функциональностью, который позволит бизнесу проверить теорию или захватить кусок рынка, а вам — не сгореть от 120-часовой рабочей недели.

Какие сложности ждут команду

Расплывчатые требования к проекту на старте.



В самом начале заказчик, как правило, еще не вполне понимает, как будет выглядеть проект. И речь не про дизайн, а про бизнес-логику. Какой будет флоу оплат, какие разделы должны быть, что из изначального скоупа действительно важно. Почти всегда на старте теряются нюансы, и финальный скоуп расширяется.



Дело в том, что бизнес тоже развивается. Первоначальные идеи не всегда доживают до финала. Они развиваются, меняют приоритетность. Поэтому периодически возникают новые важные фичи, без которых нельзя запускать проект. Бывает и наоборот: фича, над которой команда билась неделю, вдруг становится не нужна. Ошибки в реализации интеграций или объектной модели.



Важно понимать, что решения, которые вы принимаете на старте, будут жить с проектом долго. А изменить объектную модель сложно, так как у вас нет времени переписывать все сервисы, завязанные на нее. То же касается и архитектуры. Если вы ошиблись, скорее всего, придется жить с ней до завершения работ. Подводные камни: сетевые инфраструктурные проблемы, БД нельзя использовать в соответствии с политикой компании, санкции, сервер слабее, чем должен быть, и т. п.



Тут может быть много неочевидных проблем. Например, на проекте в Казахстане мы с удивлением узнали, что у местного облачного провайдера нет Кубера. Нам пришлось вместе с ними тестировать и поднимать его. На это ушло 2 недели. Из-за проблем выше часто страдает изначальная архитектура решений.



Архитектура, наверно, ключевой фактор в вашем проекте. Как вы построите систему, какие базы выберете, кто будет брокером, как будут общаться сервисы, какие будут модули. Ошибка на этом этапе многократно увеличивает количество часов в будущем на отладку и рефакторинг. Любая ошибка в CORE очень много стоит. Под CORE я понимаю модули или сервисы вашей системы. MDM, IDM и прочие сокращения, на основе которых вы строите обвязку сервисов для тех или иных нужд, либо какие-то ключевые пакеты, которые интегрируются во все сервисы системы. Проблемы выше выглядят как проблемы менеджера, тимлида или архитектора. Но на этом этапе закладывается фундамент будущих проблем разработчика, поэтому понимать все эти обстоятельства важно.

Главные ошибки в MVP

Неправильно собирают скоуп работ.



Недавний пример из практики: на старте проекта мы договорились с заказчиком, что сервис отзывов не так важен на старте проекта, потому что там не будет большого трафика. Значит, отзывы можно отложить. Договорились, что мы оставим простую форму, где будет возможность оценить сеанс и написать комментарий.

Позже появилась вторая ошибка.

Не фиксируют скоуп работ.



В конце концов этот сервис расширился еще до выхода из MVP. В нем появилась возможность выбрать несколько вариантов фидбека. В каждом были свои варианты проблем в чекбоксе. В итоге мы сначала внедрили не самую важную фичу на MVP, а потом еще и расширили ее.

Кажется, что это недолго, но таких небольших задач много. Разработчик делает ее пару часов, тестировщик полчаса тестит ее, фронты поправят за час-полтора, тимлид провалидирует за то же время. И вот задача на 30 минут съела 4–6 часов.



А когда таких задач 30, вы потратите недели на то, что можно было бы сделать без нервов и горячки.

Неправильно выбирают архитектуру. Слишком много или слишком мало времени тратят на ТЗ.



Бывает, что на простую функциональность пишут огромное ТЗ. Оно не очень поможет разработке, но съест много времени. Бывает и наоборот, когда на простую функциональность не дают ТЗ вовсе, хотя эта функциональность влияет на логику соседних сервисов. В итоге у нас получается несогласованность данных.

Из этого списка вы можете повлиять только на финальный скоуп и на его качество. Любой разработчик — это высокоинтеллектуальный юнит, и его мнение должно учитываться. Не стесняйтесь сказать тимлиду, что половину сервисов можно выкинуть из MVP, потому что они не важны. Или что сервис можно упростить, сделать хуже, но зато он будет работать. А после MVP можно вернуться к нему и отрефакторить.

Важно сделать качественно, важно сделать быстро. Остальное можно наверстать.

Пакеты и утилки

В основном в этой статье буду говорить про микросервисы, потому что у монолитных MVP всё проще. Монолитные платформы проще в реализации, особенно когда у вас урезанный функционал. Но рынок больших продуктов массово уходит к микросервисам. Это понятная тенденция — масштабировать и развивать монолит сложнее.

На этапе MVP ваша основная концепция должна заключаться в грамотном разделении логики и создании модульного монолита, который в будущем будет достаточно легко растащить на сервисы. Там вы столкнетесь с такими же проблемами, о которых речь пойдет дальше.

При старте MVP на микросервисах стоит учитывать, что они всегда дороже в разработке, чем монолит. Связано это с количеством утилит, которые нужны для реализации: коммуникация между сервисами, стандартизация их работы, работа с SSO, стандартизации логов, настройка общих принципов общения через брокер, выстраивание унифицированных API, распределение и мониторинг трафика и т. д.

Вторая причина — более сложная инфраструктурная работа. Например, монолит мы можем запускать даже на Systemctl. С микросервисами — сложнее. Тут как минимум нужно больше времени на отладку, запуск и тестирование системы.

SSO и внутренние запросы

Возьмем два сервиса: сервис эквайринга и корзина магазина. Они должны коммуницировать друг с другом. При оформлении корзины на фронтенде пользователь отправляет запрос на оплату в эквайринг. Эквайрингу необходимо получить его и убедиться в корректности корзины — что стоимость верная и т. д. Значит, должен произойти внутренний сетевой запрос от сервиса эквайринга в сервис корзины. Корзина отвечает, что всё корректно, и эквайринг проводит оплату.

В этой простой коммуникации одна проблема: нам нужно сделать внутренний сетевой запрос. Для его реализации нужно учесть 2 фактора:

от чьего имени мы делаем запрос из корзины в эквайринг — от пользователя или от сервиса;

где находится соседний сервис.

Чтобы решить эту проблему, мы можем проксировать токен пользователя или создавать токен для сервиса. Последнее, кстати, в любом случае придется делать.

Для этого придется создать отдельный клиент, который будет вытаскивать токен пользователя, перекладывать его в новый запрос и отправлять в другой сервис. При этом мы должны знать, где лежит наш сервис корзины. Мы не можем жестко зашить его, ведь тогда при изменении настроек сети администраторами может поломаться логика системы.

Значит, наш пакет должен решать 3 проблемы:

обеспечение проксирования токенов,

обеспечение коммуникации между сервисами,

инкапсулирование адресов сервисов.

Для конфигурирования нам потребуется получать адреса соседних сервисов извне — от наших DevOps. Для этого отлично подходят переменные окружения, которые пробрасываются в контейнер при сборке. Внутри кода нам достаточно определить класс настроек и в него положить наши envs. Для сериализации этих данных отлично подойдут модели Pydantic.

Проксирование токена пользователя — задача достаточно простая. Нужно взять заголовок из запроса и переложить его в другой. Но что, если выполняется бэкграунд таска и нам нужно сделать межсервисный запрос без наличия токена пользователя? Тут сложнее. Можно просто настроить внутренние сети Кубера, которые будут доступны только при коммуникации между контейнерами, но вряд ли вам это позволит специалист по информационной безопасности.

Мы решили реализовать этот механизм следующим образом.

Определили клиента для коммуникации с сервисом — например, с SSO. Он наследуется от базового S2S Client, который требует при инициализации набор параметров, таких как логин и пароль сервиса, а также дополнительные настройки — логгер, таймауты и т. д.

Для оптимизации скорости работы мы также решили сохранять токен прямо в память и работать с ней напрямую. В случае, если токен протух или отсутствует в памяти, клиент заново авторизует сервис в SSO и обновляет токен. При этом мы оставляем возможность сохранить токен в кэше, а не в памяти.

Внутри S2S Client инкапсулируется логика получения токена у SSO, логика обновления токена, если он протух, ретраи, если сервис не ответил с первого раза. Также в этом же клиенте происходит логирование всех запросов — как успешных, так и с ошибкой.

Теперь всё, что нам остается, создать новый клиент для общения с соседним сервисом, отнаследовавшись от S2S-клиента. В нем мы можем расширить логику. В итоге на создание унифицированного клиента для общения с любым сервисом требуется кратно меньше времени, правила общения остаются общими, снижается количество ошибок и упрощается работа с системой.

Логирование

Тут же возникает второй вопрос — что, если что-то пошло не так? В этом случае нам нужен лог.

Но мы не просто должны настроить стандартный логгер. В микросервисах хорошо бы иметь возможность смотреть трейсинг запросов от входа до выхода:

через какие сервисы прошел запрос,

сколько было запросов в соседние сервисы,

сколько они выполнялись и т. д.

В отличие от монолитного Django, нам не так просто выяснить, где было самое большое время исполнения, ведь происходит межсервисное взаимодействие.

Но вернемся к логам. Это эффективный способ понять, где была совершена ошибка и с какими данными мы ее получили. Логи — это отдельная и большая тема, о которой можно говорить долго: как настроить мониторинги, Prometheus и т. д. Но в этой статье приведу лишь основные «аксиомы» логирования.

Логи должны складироваться в отдельной системе, которая по принципу веб-воркеров будет забирать логи из stdout. Это системы типа ELK, graylog и т. п. У всех этих систем есть одна общая черта — они индексируют логи. Для удобного и быстрого поиска логов необходимо, чтобы их формат был одинаковым. Иначе придется преобразовывать их внутри этой системы, а это дорого.



Иными словами, когда один сервис вам плюет в лог XML, другой — JSON, третий — просто текст без определенного форматирования, лучшим вариантом будет всегда записывать логи в JSON и выплевывать в stdout JSON-строки. В этом случае их легко будет сохранить, индекснуть и проводить по ним поиск.

Зачастую лог выглядит примерно так:



""Start integration. Time %%%%""

""Integration finished. Time %%%%""

""Integration error: %errors""



Это не плохо, что написаны какие-то отладочные сообщения. Но в них нет метаданных. В этом логе мы можем только увидеть, что началась какая-то интеграция; началась в N времени и закончилась в X. Но что было между этими засечками, непонятно.



Поэтому мы логируем все параметры запроса, результаты ответа, обязательно указываем, куда шел запрос. При этом логи обязательно разделяются на разные уровни сложности: Info, Error и т. д. Debug-логи, кстати, тоже весьма полезны.

Тот же принцип стоит использовать для обычных логов, которые вы реализуете. Замените стандартный логгер, чтобы он мог забирать больше метаданных и преобразовывать их в JSON-строки. Без логов отлаживать систему будет сложно.

Собственно, логи в наших системах пишутся всегда на всех межсервисных взаимодействиях. Это реализовано так же в отдельном клиенте, который является базовым компонентом наших SDK.

Если развернуть init в S2S-клиенте, то можно увидеть, что внутри S2S-клиента при инициализации создаются несколько клиентов: для общения с SSO и с сервисом. И это клиенты, унаследованные от BaseClient.

BaseClient, в свою очередь, отвечает за стандартное поведение запросов: сертификаты SSL, политику ретраев, таймаут и прочее. Это еще одна абстракция над запросами, которая позволяет унифицировать общение между сервисами. И еще одна немаловажная задача, которую он решает, это запись в лог сообщений.

Я описал один из вариантов, как можно обрамлять ваши запросы. При каждом запросе происходит запись, куда и какой запрос был отправлен. Также логируются все ошибки с исчерпывающей информацией о том, с какой информацией был совершен запрос. При этом логируется и запрос, и ответ.

Затем всё это выкладывается в stdout и сохраняется в ELK, а в будущем этот лог используется в отладке и мониторинге.

Автотесты

Мы прививаем любовь к автотестам всем нашим сотрудникам. В AGIMA практически нет питоновских проектов, где нет тестов.

В Django-мире есть библиотека Pytest-Django. Она предоставляет удобный тулинг в первую очередь при работе с БД. Поскольку у Django своя ORM-ка, Pytest-Django позволяют глубоко интегрироваться внутрь, предоставлять разный удобный тулинг из коробки для работы с БД, в том числе и в случае запуска тестов с помощью Xdist.

Фреймворки типа Fastapi/Flask/aiohttp не диктуют нам инструменты и архитектуру и подразумевают самостоятельный выбор различных инструментов, из которых предстоит построить приложение. В отличие от Django, придется потратить немало времени, чтобы написать свой собственный каркас для тестов.

Мы делали свой каркас в первую очередь с оглядкой на Pytest-Django, поэтому пришлось реализовать следующий функционал:

создание тестовой БД;

установка миграций;

корректная работа с транзакциями для откатки изменений каждого теста;

клонирование БД для каждого воркера в случае запуска вместе с Xdist;

различные удобные генераторы JWT-токенов с необходимыми пермишенами/ролями.

Чтобы создать хороший тестовый каркас со всеми нужными фикстурами и поведением, нужно много времени. Но в будущем это дает сильный буст в написании тестов. К тому же это отличный путь к тому, чтобы научить команду TDD. Без хорошего каркаса это сложно, но, когда он хорошо сделан, даже специалисты уровня джуниор, глядя на другие тесты, могут создавать свои. Еще один неочевидный плюс — отладка с тестами значительно упрощается, так как воспроизвести проблему в тесте за счет кода становится проще, чем руками.

Ключевые моменты, которых мы придерживаемся при написании тестов в микросервисной архитектуре:

Unitests на ключевые функции и классы системы.



Они нужны в первую очередь для уверенности, что лишние изменения не поломают все сервисы. Тесты на API-эндпоинты.



Такой выбор обусловлен в первую очередь тем, что, тестируя API, мы тестируем максимально возможное количество слоев приложения. Далее, если позволяют финансы и время, можно тестировать каждый слой уже отдельно. Детерменированность данных.



При тестировании API-эндпоинтов мы стараемся максимально детерминировать все входные данные, которые требуются в процессе обработки пользовательского запроса. Сравнение с эталонными результатами.



Как следствие предыдущего пункта, в идеале надо сравнивать HTTP-статус и текст ответа полностью, один в один. Если тестируемые эндпоинты начнут выдавать больше или меньше полей или обнаружится несоответствие формата каких-то полей, тесты тут же нам об этом просигнализируют. Моки.



Очень много моков ответов от различных сервисов. Причем мокать лучше не классы/функции, а именно ответы сервисов, чтобы протестировать то, как отрабатывают ваши клиенты.



Например, наш стандартный тест выглядит так: в него подгружаются основные фикстуры, где мокаются все внешние запросы. После этого мы имитируем запрос на создание контракта с определенными данными. Проверяем статус ответа и сравниваем полученные данные с ожидаемым контрактом.



Есть также пачка универсальных тестов, которые мы копипастим из сервиса в сервис:



- тесты для Healthcheck-эндпоинтов;

- тесты для проверки эндпоинтов с разной дебаг-информацией;

- тесты для эндпоинтов, которые всегда «падают» для проверки Sentry;

- тесты миграций лесенкой (идея подсмотрена у Александра Васина из Яндекс): накатываем одну миграцию, откатываем, накатываем 2 миграции, откатываем и т. д.;

- тесты для проверки корректности пермишенов у эндпоинтов.

Есть также самописные модули для Django/Fastapi, которые умеют интроспектить и доставать из внутрянки приложений все существующие эндпоинты и HTTP-методы. Они проверяют, что для каждого эндпоинта сервиса, за исключением списка определенных урлов, выдается соответствующая ошибка:

при запросе без jwt-токена;

при запросе с некорректным форматом авторизационного хедера;

при запросе с некорректным jwt-токеном (просрочен, несовпадающая подпись);

при отсутствии прав доступа.

Последний пункт — самый важный, так как помогает понять, где забыли навесить нужный пермишен или корректно проверить права. Особенно хорошо, что такие тесты помогают нивелировать человеческий фактор или просто подсветить новому человеку в команде, что он что-то сделал, но не навесил корректную проверку прав.

В этом тесте all_routes — функция, которая вернет список кортежей вида (‘урл’, ‘метод’). Причем урлы она генерирует с корректными path_params: допустим, для урла вида «/payments/{payment_id:uuid}/» она сгенерирует рандомный UUID. Пусть даже объекта с таким ID не существует в системе, нам это не так важно. Важно лишь то, что это корректный существующий урл для фреймворка, на который у нас не будет ошибки 404.

Асинхронные таски

Для межсервисного асинхронного взаимодействия мы использовали RabbitMQ. В нашем случае и Celery, и Dramatiq верхнеуровнево подходили для Background-обработки тасок. Изначально мы остановились на Celery. Однако после мы заметили Dramatiq и некоторое время работали с ним.

Основные отличия Dramatiq и Celery:

Dramatiq работает под Windows;

для Dramatiq можно создавать Middleware;

субъективно, но исходный код Dramatiq более понятный, чем у Celery;

Dramatiq поддерживает перезагрузку при изменении кода.

Однако в процессе эксплуатации эти инструменты на нашем стеке показали себя не очень хорошо. В первую очередь оба эти инструмента нативно не поддерживают Asyncio. Это проблема, когда весь ваш код написан для работы в асинхронном режиме, а вам надо запустить это из синхронного кода.

Конечно, запустить его можно, но мы стали ловить разные трудноуловимые баги при работе с БД, редкие проблемы с транзакциями, фантомно закрывающиеся коннекты и т. д. К тому же оказалось, что логи нужного нам формата не очень легко прикрутить к Celery. Также непросто настроить корректный алертинг ошибок в Sentry согласно бизнес-логике. Бизнесовые эксепшены мы не хотим отправлять в Sentry, хотим только неожиданные от Python. Плюс конструкции для запуска асинхронного кода из синхронного выглядели ужасно.

Учитывая всё это, код наших тасок был монструозный, с разными костылями и трудно отлаживаемыми багами. Поэтому мы написали собственную реализацию продюсера-консьюмера на базе библиотеки Aiopika. Из кода исчезли костыли для запуска асинхронного кода, появилась возможность добавлять свои Middleware, но для воркера.

И поскольку теперь мы можем нативно работать с асинхронностью Python, наш воркер теперь умеет обрабатывать не строго одну таску, а сразу несколько за один момент времени. Выглядит это всё плюс-минус так же, как это было бы в Celery или Dramatiq.

Более того, в процессе разработки мы немного пересмотрели подход к общению между сервисами через брокер. В начале разработки MVP-системы сервисы-продюсеры знали о сервисах-консьюмерах и явно отсылали события в очереди друг друга. Поначалу это работало неплохо, но в процессе роста мы поняли, что сервисы-продюсеры стали слишком много знать о сервисах-консьюмерах и перенимать часть бизнес-логики в момент, когда решали, надо или не надо отправлять сообщение.

Поэтому мы перешли на другой подход: события сервисов-продюсеров стали просто бродкастами, то есть продюсеры перестали знать о своих консьюмерах, а сервисы-консьюмеры уже сами решали, надо ли им подписываться на эти события и сами, согласно бизнес-требованиям, стали проверять, надо ли обрабатывать это событие. Поскольку событие одно, а триггеров может быть сколько угодно, мы написали вдобавок простые классы-джобы такого вида:

Здесь is_triggered — метод, который возвращает True/False в зависимости от того, должен ли он сработать на этом ивенте или нет. Метод process — бизнес-логика этой джобы.

Далее список джоб передается специальному экзекутора, который по методу is_triggered проверяет необходимость запуска джобы и запускает нужные. Возможно, это не самое красивое решение, но важнее другое: мы смогли красивее и понятнее описывать правила срабатывания джоб — не используя кучи IF’ов, запускать джобы в асинхронном режиме, легко добавлять/удалять новые по требованию бизнеса. А разработчикам стало легче понимать структуру кода и поддерживать логику.

Рекомендации

Охватить все вопросы, которые приходится решать на MVP, в одной статье невозможно. Но вот несколько пунктов, которым точно стоит уделить особое внимание.

Проверка ИБ. С ними всегда очень много проблем, нужно постоянно сканировать код, чтобы не позволить появиться критическим уязвимостям. Нагрузочное тестирование.

Проводить его нужно обязательно. Сложно заранее понять, какую нагрузку выдержит ваша система. Зачастую проблемы возникают после нагрузки выше определенного RPS.

S3-хранилища.

В монолитных архитектурах вы всегда можете управлять файлами в отдельном пакете. Чаще всего проверяется объем файла, допустимое расширение, проверка на исполняемость файла, ограничение на количество загрузок файлов. Также нам необходимо проверять, не протух ли файл, вовремя чистить хранилище и т. д.

В микросервисах этим пакетом является отдельный микросервис. А значит, вам придется агрегировать всю логику обработки файлов в нем. Это несет накладные расходы. Нам нужно знать, от какого сервиса файл был загружен, какие у него метаданные. Файл приходится грузить напрямую в сервис S3. То есть микросервис, по бизнес-логике которого требуется загрузить файл (фотографии, docx, excel и т. п.), ничего про файл не знает, у него есть только метаданные.



Следовательно, нам потребуется отдельная асинхронная процедура синхронизации данных: сообщить микросервису, где лежит файл (какой у него урл), какой у этого файла идентификатор, всё ли с ним вообще окей и т. д.

Обязательно используйте авгоненерируемую документацию.

В FastAPI она идет из коробки. В Django есть Django-yasg. Готовая автодока экономит огромное количество времени фронтам и мобилкам.

Не пренебрегайте типизацией.

Отличный способ убедится, что вы правильно используете пакеты и классы и не совершаете глупостей.

Пишите автотесты.

Там, где много коммуникаций, без них не обойтись. Для вас это будет защита от лишних багов и прекрасный инструмент для разработки.

Просите помощи у коллег, если зависли на каком-то вопросе.

Это не стыдно, это нужно делать. Так вы быстрее завершите задачу, не сорвете сроки, не будете заниматься самобичеванием. Брейншторм — отличная практика, пользуйтесь ей.

Настройте Sentry.

Это простой и мощный инструмент, который легко поднимается в Standalone-режиме. Sentry легко настроить в любом фреймворке. Внедрить его в проект — не более 30 минут.

Фиксируйте версии библиотек."'https://habrastorage.org/getpro/habr/upload_files/e00/6b9/018/e006b9018e4d5cb327a4c549371cf2fc.png'"['https://habrastorage.org/getpro/habr/avatars/b2b/a28/7c9/b2ba287c906ecd058e8396d40b24442c.jpg', 'https://habrastorage.org/r/w32/getpro/habr/avatars/b2b/a28/7c9/b2ba287c906ecd058e8396d40b24442c.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/930/7d7/fa7/9307d7fa7c71beb55370460bbd8c7ec0.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/companies/ce1/7da/85b/ce17da85b6ec11846947fcb804159cdc.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/e5c/db6/9d4/e5cdb69d4a2adf6f9905345213b9d71a.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/e00/6b9/018/e006b9018e4d5cb327a4c549371cf2fc.png', 'https://habrastorage.org/getpro/habr/upload_files/e00/6b9/018/e006b9018e4d5cb327a4c549371cf2fc.png']"
11'720930'Формула, соединяющая е и пи'Факториал натурального числа определяется так: . Например, - число со 157 цифрами. равно числу способов переставить элементов между собой. Для оценки используют формулу Стирлинга: Обратим внимание,...'https://habr.com/ru/post/720930/'"Факториал натурального числа определяется так: . Например, - число со 157 цифрами. равно числу способов переставить элементов между собой. Для оценки используют формулу Стирлинга:

Обратим внимание, что эта формула содержит числа и и возникла из очень простой модели с перестановками.

Посмотрим, как она работает, на примерах



Возьмём среднее арифметическое первых натуральных чисел. Легко видеть (геометрически и алгебраически), что , середина отрезка натурального ряда . Как ведёт себя среднее геометрическое первых натуральных чисел? С помощью формулы Стирлинга легко установить:

Что довольно намного меньше . Попробуйте понять как ведёт себя среднее гармоническое первых натуральных чисел. Кстати, среднее геометрическое произведения случайных чисел, выбранных независимо и равномерно на отрезке также стремится к числу с ростом числа множителей.

Рассмотрим следующую вероятностную задачу. Пусть у нас есть симметричных монет. Подкинем их все. С какой вероятностью орлов выпадет столько же, сколько и решек?

Эта вероятность равна . При больших можно применить формулу Стирлинга и получить

Это значит, что такая вероятность при броске 40 монет будет примерно вдвое меньше, чем при броске 10 монет.

Также это даёт нам способ вычислить число в домашних условиях (если у вас есть достаточно большое количество монет): возьмём 100 монет и повторим наш эксперимент 10000 раз. Посчитаем, в скольких экспериментах орлов выпало столько же, сколько и решек.

Пусть это число равно .

Тогда из сказанного выше вытекает:

Остаётся вопрос о точности такого приближения. Но это совсем другая история, связанная с центральной предельной теоремой.

Пишите в комментариях какие вы знаете применения формулы Стирлинга.

Авторы:"'https://habrastorage.org/getpro/habr/upload_files/4aa/2e2/34a/4aa2e234a99ec8f704aea842a8fc9afc.jpg'"['https://habrastorage.org/r/w780q1/getpro/habr/upload_files/60d/7cf/9d5/60d7cf9d5384c8e473760b8197317b45.jpg', 'https://habrastorage.org/getpro/habr/upload_files/54a/00a/6f8/54a00a6f899d584bf79d091e2edf4e6d.svg', 'https://habrastorage.org/getpro/habr/upload_files/62a/2d3/0e2/62a2d30e205c1e6c66fb218af8c6939f.svg', 'https://habrastorage.org/getpro/habr/upload_files/92f/ef7/626/92fef7626baeeddb6af072e7898e0066.svg', 'https://habrastorage.org/getpro/habr/upload_files/df6/681/602/df6681602b0d85809ace6833b2c658ce.svg', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/upload_files/d3c/0fc/98c/d3c0fc98c5e3f2d67329236efa5c4f2b.svg', 'https://habrastorage.org/getpro/habr/upload_files/e87/75e/757/e8775e7571a8dbceaffc9d29f187d7c6.svg', 'https://habrastorage.org/getpro/habr/upload_files/74c/4ad/e3a/74c4ade3a21d6246f1bb9401533d8890.svg', 'https://habrastorage.org/getpro/habr/upload_files/cb4/e97/54f/cb4e9754f2283aa3a0725573eb84bc6c.svg', 'https://habrastorage.org/getpro/habr/upload_files/4aa/2e2/34a/4aa2e234a99ec8f704aea842a8fc9afc.jpg', 'https://habrastorage.org/getpro/habr/upload_files/5f5/780/312/5f57803120c2b938dd25717a5d86846b.svg', 'https://habrastorage.org/getpro/habr/upload_files/1da/2af/5be/1da2af5be7ce09b01a444440d61434d7.svg', 'https://habrastorage.org/getpro/habr/upload_files/d81/677/68a/d8167768ac2fd65ff5a484e85d65c530.svg', 'https://habrastorage.org/getpro/habr/upload_files/376/d30/715/376d30715bd6b7071944bb779fc4c278.svg', 'https://habrastorage.org/getpro/habr/upload_files/5da/f17/972/5daf17972174cb65da30821bc0c4a4e2.svg', 'https://habrastorage.org/getpro/habr/upload_files/7eb/404/a83/7eb404a837125e5067ff38e74ecb631f.svg', 'https://habrastorage.org/getpro/habr/upload_files/72e/a01/3bd/72ea013bda4a6b0d55954c219f710fb7.svg', 'https://habrastorage.org/getpro/habr/upload_files/abd/b54/89f/abdb5489f5f852f96fb0f4d004b45430.svg', 'https://habrastorage.org/getpro/habr/upload_files/11b/b21/c3c/11bb21c3c36f68a396c5dafcabee3579.svg', 'https://habrastorage.org/getpro/habr/upload_files/752/a70/08e/752a7008e26e610b7dba4d612d9b8a9a.svg', 'https://habrastorage.org/getpro/habr/upload_files/b8e/d09/948/b8ed099488534a42fc48d8c899590a84.svg', 'https://habrastorage.org/getpro/habr/upload_files/4b6/f1b/96a/4b6f1b96a6d865bc9c10db5b84847c7f.svg', 'https://habrastorage.org/getpro/habr/upload_files/24c/d7f/873/24cd7f873ba39cb65226f1c3a77427dd.svg', 'https://habrastorage.org/getpro/habr/upload_files/206/731/c17/206731c17950e2afd9b737b33983a1e0.svg', 'https://habrastorage.org/getpro/habr/upload_files/5b1/aa5/814/5b1aa581447cc0e4d578fa665d092e98.svg', 'https://habrastorage.org/getpro/habr/upload_files/791/cf7/e5f/791cf7e5f8773a5a55f0fb8db5dcbb6c.svg', 'https://habrastorage.org/getpro/habr/upload_files/923/d62/dd1/923d62dd1b66e309f652b1792faccd5a.svg', 'https://habrastorage.org/getpro/habr/upload_files/aa1/679/1e8/aa16791e8de35a70c642c9c042361c2f.svg', 'https://habrastorage.org/getpro/habr/upload_files/c82/20a/b9c/c8220ab9cd6d2ef09fd73ec2d53c3e91.svg', 'https://habrastorage.org/getpro/habr/upload_files/46d/a22/1f1/46da221f14f5521b4bd17d7d87f9f670.svg']"
12'720942'Топ 20 ботов которые постоянно сканируют ваши сайты. Не все из них одинаково полезны'Здравствуйте! На связи Максим Кульгин, моя компания  clickfraud.ru  защищает предпринимателей от ущерба, вызываемого действиями «плохих» роботов. Многие администраторы веб-сайтов настолько...'https://habr.com/ru/post/720942/'"Здравствуйте! На связи Максим Кульгин, моя компания clickfraud.ru защищает предпринимателей от ущерба, вызываемого действиями «плохих» роботов. Многие администраторы веб-сайтов настолько напуганы современными сетевыми угрозами, что без разбора готовы бороться против всех средств автоматизированного обхода. Оправдана ли такая глухая линия обороны? Вряд ли.

Существует огромное количество «хороших» роботов, без которых не то что не обойтись, а даже не выжить. Этот небольшой обзор поможет всем, кто ведет деятельность в интернете.

В конце статьи мы посмотрим, почему простое противодействие роботам бесполезно и кроме вреда и головной боли ничего не принесет. А заодно и подскажем: от кого защищаться и как именно.

Начнем с самого простого.

Любой маркетолог скажет, что содержимое сайта должно постоянно обновляться — снова и снова — только так можно заполучить благосклонность SEO (Search Engine Optimization, оптимизация под поисковые системы), а значит, и шанс на внимание со стороны целевой аудитории.

Однако бывает, что сайты содержат сотни и даже тысячи страниц. И что? Привлекать поисковики вручную? Если контента много и он обновляется часто — как гарантировать, что изменения действительно благотворно скажутся на SEO?

Вот тут-то и вступают в игру поисковые роботы! Такой робот прочитает карту сайта, сравнит даты последнего обновления (у себя и на сайте) — и проиндексирует новое содержимое!

Кто-то подумает, что поисковые роботы — это Google Bot, Yandex Bot, ну, может быть, ещё какой-то там bot. На самом деле их очень много! Тот, кто заинтересован в продвижении сайта должен знать о сетевых ботах хотя бы в общих чертах. Зачем? Чтобы использовать в свою пользу!

Здесь мы рассмотрим полный список всех роботов, которые неустанно читают сайты и о которых полезно знать. Но прежде чем нырнуть в мир, который невозможно было бы ни вообразить, ни описать ещё четверть века назад, давайте чуть ближе познакомимся с существами из цифрового мира.

Что такое поисковый робот?

Поисковый робот (их ещё называют ботами, пауками, сканерами, обходчиками) — это компьютерная программа, которая автоматически обходит веб-сайты по найденным ссылкам, читает и анализирует содержимое страниц, составляет краткую сводку для своего сервиса. Если робот работает на какую-то систему, то составление или обновление краткой сводки по веб-странице принято называть «индексацией».

Такой процесс необходим, чтобы пользователи могли мгновенно получать по своему запросу ссылки на нужные страницы из сотен миллионов существующих в интернете сайтов. Как правило, индексация — процесс автоматический, но в некоторых случаях она может быть инициирована и вручную.

Упорядочивание веб-страниц в выдаче, то есть выставление каждой некоторого рейтинга с точки зрения поисковой системы — сложная функция со множеством входных данных, наиболее влиятельные из которых: соответствие запросу, наличие внешних ссылок на страницу, авторитетность ссылающихся ресурсов и многое другое.

Всё это, включая время и труд, затраченные на создание страницы, не будет иметь ни малейшего значения, если поисковый робот на ней не побывает. Именно поэтому так важно не препятствовать роботам. Наоборот надо позволять им совершать обход и встречать как дорогих гостей: робот — друг человека! (Пока…)

Роботы выполняют свою работу постоянно — это единственный способ поддерживать точность и актуальность информации в постоянно меняющемся интернете. При этом поисковые системы — самые часто посещаемые ресурсы.

Самые популярные сайты

Не существует единого поискового робота, который собирает информацию для всех. У каждой поисковой системы, у каждого сервиса — свой собственный неповторимый набор роботов и алгоритмов. Поэтому те, кто создает и поддерживает работу веб-сайтов — разработчики и маркетологи — держат список «хороших» роботов, чтобы беспрепятственно пропускать их к ресурсам, при этом блокируя деятельность «плохих» роботов.

Как действует поисковый робот?

На первый взгляд просто. Рано или поздно он самостоятельно попадет на нужную страницу и проиндексирует её: соберет ключевые слова и фразы, попытается определить тематику, изучит ссылки, по которым пойдет дальше.

Путь робота до каждой веб-страницы очень извилист. Начинается он с известных URL-адресов, с некоторого перечня сайтов, полученного на основе уже собранной информации.

Веб-мастер может контролировать каким роботам какие страницы позволено читать. Делается это с помощью специального файла robots.txt

Этот же файл подскажет: какие страницы и когда обновлялись. А поисковый робот помнит: когда и какие страницы он обрабатывал последний раз. Такая согласованность в обмене информацией благотворно сказывается на эффективности всех участников.

Надо заметить, что инструкциям файла robots.txt подчиняются только послушные, «хорошие» роботы. «Плохие» же роботы, роботы-воры, роботы-скликиватели и прочие попросту проигнорируют файл robots.txt. Это хорошо, что у них нет технической возможности плюнуть в его содержимое и растереть.

В чем отличие роботов?

Различные типы поисковых роботов используются для разных целей. Они отличаются и по стоимости, и по предназначению, и по функциональным возможностям. Условно их можно разделить на три типа:

собственные роботы, созданные разработчиками компании для внутренних задач, таких как аудит и оптимизация;

коммерческие роботы; используются готовые (например, Screaming Frog) или разрабатываются на заказ;

с открытым исходным кодом; бесплатные для использования, создаются различными хакерами и энтузиастами по всему миру.

ТОП-12 поисковых роботов

Не существует универсального робота, одинаково пригодного для любых задач. Вот некоторые из наиболее распространенных на сегодняшний день.

1. Googlebot

Googlebot — универсальный поисковый робот Google, отвечающий за поиск сайтов, которые будут отображаться в одноименной поисковой системе.

Хотя технически существует две разные версии Googlebot (Googlebot Desktop и Googlebot Smartphone), большинство экспертов считают Googlebot одним единственным роботом.

Это обусловлено тем, что оба используют один и тот же уникальный токен продукта (известный как «User Agent», «пользовательский агент»), который фигурирует в robots.txt.

Когда Googlebot работает над сайтом, то производит запросы на чтение раз в несколько секунд (если только его не заблокировали в настройках robots.txt). Резервная копия прочитанных страниц сохраняется в единой базе данных, называемой Google Cache, которая доступна онлайн и позволяет просматривать старые версии сайтов.

Кроме того, Google Search Console — ещё один инструмент, которым с удовольствием пользуются веб-мастера, в том числе и для понимания того, как Googlebot сканирует сайт, а также для поисковой оптимизации страниц.

User Agent: Googlebot

Full User Agent String:

Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)

2. Bingbot

Bingbot был создан в 2010 году корпорацией Microsoft для своей поисковой системы Bing. Нужно гарантировать пользователям актуальность поисковой информации, а для этого необходимо иметь собственный поисковый робот для сканирования и индексации URL-адресов.

Как и Googlebot, Bingbot уважительно относится к предписаниям в файле robots.txt, так что разработчики или маркетологи могут определять: позволительно ли роботу от Microsoft сканировать содержимое сайта или нет.

Однако, в отличие от Googlebot, Bingbot может отличать версии сайтов для мобильных устройств: для чего как раз недавно перешел на новый тип пользовательского агента. Это, наряду с инструментами Bing для веб-мастеров, предоставляет большую гибкость в настройках того, как сайт отображается в результатах поиска.

User Agent: Bingbot

Full User Agent String:

Desktop – Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +https://www.bing.com/bingbot.htm) Chrome/W.X.Y.Z Safari/537.36

Mobile – Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/W.X.Y.Z Mobile Safari/537.36 (compatible; bingbot/2.0; +https://www.bing.com/bingbot.htm)

«W.X.Y.Z» подразумевают версию Microsoft Edge, например, 100.0.4896.127

3. Yandex Bot

Российским пользователям интернета представлять Яндекс не нужно. По эффективности поиска на русском языке Яндекс часто обходит всех конкурентов.

Yandex Bot — это робот, созданный специально для поисковой системы Яндекс.

Разумеется Yandex Bot подчиняется указаниям файла robots.txt.

Сверх того веб-мастера могут добавлять на страницы специальные теги Яндекс.Метрика — сервис, «который помогает получать наглядные отчеты, записи действий посетителей, отслеживать источники трафика и оценивать эффективность онлайн- и офлайн-рекламы».

Yandex Webmaster имеет много возможностей, которые невозможно осветить в рамках этой статьи.

Поддерживается IndexNow — простой способ для владельцев веб-сайтов мгновенно информировать поисковые системы о последних изменениях содержимого.

User Agent: YandexBot

Full User Agent String:

Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)

4. Apple Bot

Несложно догадаться, что Apple Bot создан для индексации страниц, чтобы отдавать максимально релевантные результаты в Siri и Spotlight.

Apple Bot учитывает множество факторов в момент принятия решения о том, какой контент следует предпочесть в предложениях для Siri и Spotlight:

вовлеченность пользователей;

релевантность поисковых запросов;

количество и качество ссылок;

сигналы, основанные на местоположении;

дизайн веб-страницы.

User Agent: Applebot

Full User Agent String:

Mozilla/5.0 (Device; OS_version) AppleWebKit/WebKit_version (KHTML, like Gecko) Version/Safari_version Safari/WebKit_version (Applebot/Applebot_version)

5. DuckDuck Bot

DuckDuckBot — робот для DuckDuckGo, поисковой системы, которая предлагает «бесшовную защиту конфиденциальности в вашем веб-браузере».

DuckDuckBot предоставляет API (Application Programming Interface, программный интерфейс), которым могут пользоваться веб-мастера, чтобы узнать: проиндексировал ли DuckDuckBot сайт и когда. Во время обхода он обновляет свою базу данных, в которой отражены IP-адреса и другие данные о посещенных местах.

Что это дает?

Это помогает выявить любых роботов-самозванцев, вредоносных ботов, которые пытаются выдавать себя за DuckDuckBot.

Вообще, кроме DuckDuckGo кто-нибудь озабочен в той же степени вопросами безопасности, конфиденциальности и сохранности личной жизни пользователей сети? Оставим вопрос висящим в воздухе… (Может его «приземлит» кто-нибудь в комментариях?)

User Agent: DuckDuckBot

Full User Agent String:

DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)

6. Baidu Spider

Baidu — ведущая китайская поисковая система, а Baidu Spider — её поисковый робот.

Поскольку Google запрещен в Китае, все кто ориентирован на китайский рынок должны стараться понравиться этой системе, и поискового робота Baidu Spider нужно встречать с распростертыми объятиями, позволяя ему индексировать сайт.

Чтобы идентифицировать Baidu Spider, пришедшего на сайт, нужно искать следующие пользовательские агенты:

baiduspider

baiduspider-image

baiduspider-video

и другие подобные, начинающиеся на baiduspider.

Для тех, кто наоборот — не ведет бизнес в Китае — возможно имеет смысл заблокировать Baidu Spider в файле robots.txt, что предотвратит обход сайта этим роботом, тем самым исключив любую вероятность появления информации на страницах результатов поисковой системы Baidu.

User Agent: Baiduspider

Full User Agent String:

Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)

7. Sogou Spider

Sogou — ещё одна китайская поисковая система, которая «первая проиндексировала 10 миллиардов китайских страниц».

Тем, кто ведет бизнес на китайском рынке нужно знать и об этой поисковой системе. Заявляется, что Sogou Spider следует предписаниям robots.txt, в том числе и параметрам задержки обхода. Получать информацию крайне трудно, так как практически вся официальная информация написана исключительно на путунхуа.

Как и в случае с Baidu Spider, тем кому не нужен рынок загадочной и экзотической восточной страны, следует отключить этот робот, чтобы не перегружать сервер понапрасну.

Full User Agent String:

Sogou Pic Spider/3.0( http://www.sogou.com/docs/help/webmasters.htm#07)

Sogou head spider/3.0( http://www.sogou.com/docs/help/webmasters.htm#07)

Sogou web spider/4.0(+http://www.sogou.com/docs/help/webmasters.htm#07)

Sogou Orion spider/3.0( http://www.sogou.com/docs/help/webmasters.htm#07)

Sogou-Test-Spider/4.0 (compatible; MSIE 5.5; Windows 98)

8. Facebook External Hit

Facebook External Hit, также известный как Facebook Crawler, обходит сайты, упомянутые на Facebook. Не следует забывать о том, что социальная сеть Facebook прошла долгий путь деградации, докатилась до того, что стала экстремистской организацией и теперь запрещена в РФ.

Использование собственного поискового робота позволяет Facebook создавать общедоступный предварительный просмотр каждой ссылки, размещенной на платформе. Заголовок, описание и уменьшенное изображение отображаются благодаря произведенному индексированию.

Посещение ссылки роботом должно благополучно завершаться в течение нескольких секунд, иначе Facebook не покажет сформированное содержимое в специальном фрагменте, сгенерированном перед публикацией.

User Agent: facebot

Full User Agent String:

facebookexternalhit/1.0 (+http://www.facebook.com/externalhit_uatext.php)

facebookexternalhit/1.1 (+http://www.facebook.com/externalhit_uatext.php)

9. Exabot

Exalead — компания по разработке программного обеспечения, созданная в 2000 году со штаб-квартирой в Париже. Компания предоставляет поисковые платформы для потребительских и корпоративных клиентов.

Exabot — робот для поисковой системы Exalead, построенной на их продукте CloudView.

Как и большинство поисковых систем, Exalead при ранжировании учитывает как обратные (внешние) ссылки, так и качество контента на веб-страницах. Робот создает «основной индекс» и компилирует результаты, которые и увидят пользователи поисковой системы.

Full User Agent String:

Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.5 (like Gecko) (Exabot-Thumbnails)

Mozilla/5.0 (compatible; Exabot/3.0; +http://www.exabot.com/go/robot)

10. Swiftbot

Swiftype — это пользовательская поисковая система для одного веб-сайта. Она сочетает в себе «лучшие технологии поиска, алгоритмы, платформу для приема контента, клиентов и инструменты аналитики».

Этот инструмент заинтересует прежде всего тех, у кого сложный сайт с большим количеством страниц. Swiftype предлагает полезный интерфейс для каталогизации и индексации всех страниц в автоматическом режиме.

Swiftbot — поисковый робот Swiftype. Однако, в отличие от других поисковых роботов Swiftbot обходит только те сайты, которые запрашивают клиенты Swiftype.

User Agent: Swiftbot

Full User Agent String:

Mozilla/5.0 (compatible; Swiftbot/1.0; UID/54e1c2ebd3b687d3c8000018; +http://swiftype.com/swiftbot)

11. Slurp Bot

Slurp Bot — поисковый робот Yahoo, который сканирует и индексирует страницы для этой, некогда самой популярной, поисковой системы.

Обладание собственным роботом необходимо для любой поисковой системы. Yahoo — это не только Yahoo.com, но и партнерские сайты: Yahoo News, Yahoo Finance, Yahoo Sports.

Без качественного поискового робота посетителю не предоставить релевантную поисковую выдачу, а значит, не получить его благосклонность, не продать рекламу, не заработать денег и — в конечном итоге — уйти в архивы истории.

На примере Yahoo видно, что робот — необходимое, но недостаточное условие для удержания успеха. И даже спонсирование фестивалей душевнобольных, свихнувшихся на теме своей половой принадлежности, не помогает обойти конкурентов.

Вернемся к технической составляющей. Правильно проиндексированный контент способствует максимально персонализированной выдаче поисковых результатов. Что в конечном итоге соответствует и интересам пользователя и карманам капиталистов-извращенцев.

User Agent: Slurp

Full User Agent String:

Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)

12. CCBot

CCBot — это робот на основе Nutch, разработанный Common Crawl, некоммерческой организацией, специализирующейся на безвозмездном предоставлении копии Интернета предприятиям, частным лицам и всем, кто интересуется онлайн-исследованиями. Бот использует MapReduce — программируемую платформу, позволяющую на основе больших объемов данных получать ценные обобщенные результаты.

Данные Common Crawl могут использоваться для самых разных задач: от совершенствования программ перевода до предсказания трендов. Например, GPT-3 был обучен как раз на одном из таких наборов данных.

Full User Agent String:

CCBot/2.0 (https://commoncrawl.org/faq/)

ТОП-8 коммерческих роботов для профессионалов SEO

Поисковые роботы — не единственные цифровые обитатели во всемирной сети. Данный раздел содержит краткий обзор аналогичных инструментов, которые используют специалисты SEO.

Многие создатели зарубежных коммерческих систем находят время для просмотра телевизора. Что в свою очередь приводит к идее ограничения доступа к своим сервисам для посетителей с российскими IP-адресами.

Проблема обходится довольно легко. Мы писали об инструментах для преодоления неоправданных дискриминационных мер в отношении пользователей с российскими IP-адресами.

1. Ahrefs Bot

Ahrefs Bot — специализированный робот, который компилирует и индексирует базу данных из 12 триллионов ссылок, предлагаемую популярным SEO-сервисом Ahrefs.

Ahrefs Bot ежедневно посещает 6 миллиардов страниц веб-сайтов. Он считается «вторым по активности поисковиком», уступая только Googlebot. Как и многие другие «хорошие» роботы, Ahrefs Bot следует правилам, прописанным в robots.txt.

2. Semrush Bot

Этот робот дает возможность Semrush (один из лидеров рынка продуктов для SEO) собирать и индексировать данные сайтов для дальнейшего использования клиентами сервиса.

Данные используются в общедоступной поисковой системе обратных ссылок Semrush и различных инструментах, таких как: аудит сайтов, аудит обратных ссылок, разные «помощники» по оптимизации контента и созданию перекрестных ссылок.

Как говорит Олег Щеголев, основатель и руководитель Semrush:

— Я съем свою шляпу, если вы найдете другой инструмент, который позволит вам делать все эти вещи:

получать доступ к более чем 24 миллиардам ключевых слов для 130 стран;

проводить углубленный аудит веб-сайта на основе более чем 130 проверок;

получать рекомендации по улучшению контента для повышения рейтинга в поисковых системах;

отслеживать и анализировать веб-сайты конкурентов и маркетинговые стратегии;

создавать и отслеживать свои PPC-кампании;

готовить, планировать и размещать контент в социальных сетях;

создавать и планировать фирменные отчеты.

3. Rogerbot

Rogerbot — это поисковый робот ведущего SEO-сайта Moz. Он специально собирает контент для аудита специалистами кампании Moz.

Поскольку SEO стало важно как никогда, в 2016 году Moz приняли трудное, но необходимое решение отказаться от экспансии во входящий маркетинг и направили всю энергию на то, что по их мнению, получается лучше всего — на поиск.

«Мы вносим значительные улучшения в наши основные продукты — Moz Pro и Moz Local — и продолжаем добиваться прозрачности и четкого понимания всех особенностей в этой сложной области.»

4. Screaming Frog

Screaming Frog — это поисковый робот, который SEO-специалисты используют для аудита и нахождения путей улучшения рейтинга в поисковых системах.

После того как обход запущен, можно просматривать данные в режиме реального времени, выявлять неработающие ссылки, находить места для улучшения, видеть дублирующий контент, замечать отсутствующие или неправильно заданные метаданные и многое другое.

Чтобы настраивать параметры обхода, надо приобрести лицензию Screaming Frog.

5. Lumar (бывший Deep Crawl)

Lumar — это «централизованный командный центр для поддержания технического состояния сайта».

Lumar гордится тем, что является «самым быстрым сканером веб-сайтов на рынке» и может похвастаться способностью сканировать до 450 URL-адресов в секунду.

6. Majestic

Majestic в первую очередь фокусируется на отслеживании и идентификации обратных ссылок для заданных URL-адресов.

Компания утверждает, что обладает «одним из наиболее полных источников данных об обратных ссылках в Интернете».

Поисковый робот сайта делает все эти данные доступными для клиентов компании.

7. cognitiveSEO

cognitiveSEO — еще программный инструмент для SEO, который используют многие профессионалы.

Поисковый робот cognitiveSEO дает возможность пользователям выполнять комплексные проверки сайта, которые могут быть использованы для корректировки архитектуры и определения стратегии SEO.

Робот будет сканировать все страницы и предоставлять «полностью настроенный набор данных», уникальный для каждого отдельного случая. Этот набор данных также будет содержать рекомендации по улучшению с точки зрения других поисковых систем.

8. Oncrawl

Oncrawl — «ведущий в отрасли SEO-поисковик и анализатор журналов» для клиентов корпоративного уровня.

Можно создавать «профили обхода», где дается много параметров для настройки: начальный URL-адрес, ограничения, максимальная скорость и многое другое. Сохранив профиль, легко запустить повторный обход с теми же установленными параметрами.

Защищать ли сайт от вредоносных роботов?

Не все роботы «хорошие». Некоторые из них могут негативно повлиять на скорость отдачи. Другие могут попытаться взломать сайт. Третьи имеют иные злые намерения.

Вот почему важно отличать электронных посетителей и знать как заблокировать доступ для непрошеных гостей.

Для этого придется наверняка предпринимать несколько шагов.

Прежде всего нужно иметь списки «хороших» и «плохих» роботов, чтобы определять: кого пускать, а кого нет.

Зачем робот пришел на сайт? Что ему нужно?

В некоторых случаях злоумышленники хотят вызвать перегрузку сервера и тогда нападают целые полчища роботов.

Но самый распространенный случай — для того, чтобы «скликать рекламу» или как-то иначе исказить статистику посещений.

Как защититься

Первый шаг — определять User Agent, IP-адрес, и на основе этих данных принимать решение о доступе. Возможно придется делать обратные запросы DNS. Это ключевые идентифицирующие факторы, которые связаны с каждым ботом.

Проблема в том, что «плохие» роботы могут мимикрировать под известные роботы поисковых систем. Здесь знание IP-адреса, сопоставление его с оригинальными IP-адресами неподдельных роботов могло бы помочь, но…

По-настоящему зловредные роботы маскируются не под «хороших» или «плохих» роботов, а под человека: воспроизводят движения мышки, нажатия клавиш, неодинаковые паузы между действиями!

Такой робот со стороны сайта выглядит как обычный пользователь, и вычислить врага можно только с помощью изощренных математических методов, недоступных из-за своей сложности практически никому.

Здесь помочь можем только мы, clickfraud.ru. Наши научные разработки оценены правительственными структурами, что позволило нашей компании стать резидентом Сколково и получить государственный грант на дальнейшее совершенствование наших разработок.

Кстати, многие предприниматели даже не подозревают, что их рекламу «скликивают» и они просто теряют деньги. В таком случае можно бесплатно и без всяких обязательств самостоятельно оценить подверженность мошенническим атакам и опробовать наши решения. Имеется и калькулятор потерь.

Расскажите в комментариях свой опыт взаимодействия с сетевыми роботами. Или, наоборот, как вас принимали за одного из таких и как приходилось доказывать, что вы человек.

Ещё я веду телеграм-канал «Русский ИТ бизнес», где без смузи и маркетинговой чепухи рассказываю об изнанке бизнеса."'https://habrastorage.org/getpro/habr/upload_files/e72/144/8fe/e721448fed33ce168fc9ffa0073de89e.png'"['https://habrastorage.org/r/w1560/getpro/habr/upload_files/a2e/a85/d2f/a2ea85d2fcccb978d25de695f2ee1f6b.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/e72/144/8fe/e721448fed33ce168fc9ffa0073de89e.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/9bd/87f/f7f/9bd87ff7f0828df7d6fccf892eaaa286.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/46e/a1e/2a1/46ea1e2a13c3f961ed06157200ca8668.png', 'https://habrastorage.org/getpro/habr/upload_files/e72/144/8fe/e721448fed33ce168fc9ffa0073de89e.png', 'https://habrastorage.org/getpro/habr/avatars/6a3/595/d5a/6a3595d5a310ba86011506b2ae5f62e9.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/67c/d8c/68b/67cd8c68b7d4931bca3170c0ae666f7e.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/ffe/404/912/ffe4049126e76a112fb04bf07482cd3b.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/6c8/8fd/1e4/6c88fd1e4df0c779b89390eba3f3f5e8.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/2b5/95e/24e/2b595e24eb987a2c6ccf3ee367a02235.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/37d/502/f8b/37d502f8b476fdcf9f40c57d561d1559.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/370/674/84b/37067484b726673cadcd605db0bb299d.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/55f/e12/fec/55fe12fec3cee61cf8a9219069ebf188.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/79c/fbb/1bd/79cfbb1bd33314c4048350a9a31a859a.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/740/6f9/559/7406f9559fa94c81f612ce441a492d1b.png', 'https://habrastorage.org/getpro/habr/company/84b/811/faa/84b811faa91bf503b4d9c3fd1fedd3dc.png', 'https://habrastorage.org/r/w32/getpro/habr/avatars/6a3/595/d5a/6a3595d5a310ba86011506b2ae5f62e9.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/9e4/6e5/083/9e46e50834c21adecc9d6bf97637724c.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/e6b/598/661/e6b598661e11d493d7255c546e699453.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/127/71e/b8c/12771eb8cc8b2675882dc2ee426d2055.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/d6d/dc3/0d1/d6ddc30d14d209401d28f261ffaff5ff.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/630/614/d2a/630614d2a023bf2ca19855ea3d2faed0.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/74d/785/655/74d785655a9581f4ecc00bd89e0842b3.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/f9f/115/c06/f9f115c06dab7e2d468cb02419af5a17.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/ec1/d36/5f8/ec1d365f8c3232080bb6cc1f75d894de.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/6b0/6b1/e4b/6b06b1e4b887d9596d26164f73985dad.png']"
13'720818'[Перевод] Brave Summarizer: ИИ для релевантного поиска'Мы внедряем новую технологию Поиска Brave, основанную на ИИ, — Summarizer, которая предоставляет краткие, ёмкие и содержательные ответы наверху результатов Поиска Brave по запросу пользователя,...'https://habr.com/ru/post/720818/'"Мы внедряем новую технологию Поиска Brave, основанную на ИИ, — Summarizer, которая предоставляет краткие, ёмкие и содержательные ответы наверху результатов Поиска Brave по запросу пользователя, опираясь исключительно на результаты поиска в сети. В отличие от строго генеративных моделей ИИ, которые часто выдают ничем не подкреплённые утверждения, мы натренировали наши большие языковые модели на обработку многочисленных источников информации в сети. Это позволяет им выдавать гораздо более сжатый и точный ответ связным языком.

Более того, Summarizer всегда предоставляет ссылки на то, откуда получены данные. Это показывает авторство информации и помогает пользователям оценить достоверность источника и степень доверия к нему, что необходимо для борьбы с потенциальными искажениями от эффекта авторитета, присущего большим языковым моделям.

Summarizer Brave уже доступен всем пользователям Поиска Brave на десктопах и в мобильной версии. Если вы хотели бы отключить Summarizer, это можно легко сделать в настройках.

Использование результатов из сети позволяет Summarizer в реальном времени предоставлять актуальную информацию. Важно помнить, что, несмотря на впечатляющие результаты моделей ИИ, пользователи не должны слепо верить всему, что производит искусственный интеллект, точно так же, как и вообще не стоит доверять всему, что написано в интернете: мы должны всегда применять критическое мышление ко всей информации, которую мы потребляем.

Помимо собственно резюмирования, наши модели ИИ могут заменить сниппеты, основанные на запросах, их резюмированными версиями, подсвечивая ответ там, где это возможно. Это позволяет нам предоставлять как основное резюме на основе агрегирования многих источников для создания исчерпывающего ответа, так и резюме одного источника (например, статьи в прессе). Мы предоставляем оба типа резюме вместе, и таким образом пользователи могут видеть и общий обзор наверху, и сниппеты с подсвеченными ответами.

Было — стало. Обратите внимание на подсвеченные ответы в сниппетах.

«Поиск Brave является наиболее быстро растущим поисковиком с момента появления Bing: мы обрабатываем 22 миллиона запросов в день, предоставляя независимые результаты поиска, основанного на нашем собственном индексе сети. С внедрением Summarizer, основанного на ИИ, мы делаем следующий шаг для улучшения релевантности результатов поиска. В отличие от ИИ-чатботов, которые могут выдавать сфабрикованные ответы, Summarizer предоставляет чётко написанное резюме наверху поисковой выдачи, агрегируя самые новые источники в сети и указывая на их происхождение в целях прозрачности. Эта открытая система уже доступна всем пользователям Поиска Brave для лучшей навигации по результатам поиска»,‎ — говорит Жозеп Пуйоль, директор Поиска Brave.

В отличие от многих компаний, предоставляющих схожие функции, мы не опираемся на сторонние технологии и не ограничены вопросами масштабирования. Summarizer основан на созданных и управляемых Brave моделях, которые настроены на максимальную эффективность времени создания результата. На сегодняшний день Поиск Brave обрабатывает дневные пики в 600 запросов в секунду, которые мы затем прогоняем через нашу ИИ-модель. Пока мы генерируем резюме на 17% запросов, и в ближайшем будущем увеличим это число масштабированием нашей системы. Мы полагаем, что наша ИИ-модель является самой большой подобной системой на сегодняшний день, т.к. она получает больше трафика в запросах в секунду, чем другие (мы применяем Summarizer ко всем запросам), а ни Bing, ни Google не открыли свои системы .

Помимо масштабируемости, мы вкладываемся в качество сгенерированных резюме. Тем не менее, так как наша модель находится ещё только в ранних фазах разработки, иногда вы можете видеть «химеры»‎, которые смешивают несвязанные сниппеты в один результат, или же оскорбительные или ложные тексты, но их количество будет постоянно уменьшаться по мере того, как мы будем улучшать наши модели и получать фидбек от пользователей.

Summarizer полностью разработан командой Поиска Brave и основан на наших неизменных принципах независимости и конфиденциальности. Мы не используем ChatGPT или его бекенд. Наш ИИ строится на трёх различных больших языковых моделях, которые решают различные задачи, а сами базовые модели построены на опенсорсных BART или DeBERTa, хостящихся на Hugging Face , которые были в значительной степени перетренированы на наших собственных данных результатов поиска.

Первая модель отвечает на вопрос, пытаясь получить конкретный ответ из текстовых сниппетов, если это возможно. Brave уже использует большие языковые модели для улучшения релевантности результатов поиска, и эта модель является развитием уже существующей технологии, обслуживающей наш граф знаний и возможности сниппетов. Разница заключается в количестве и длине анализируемых сниппетов.

После первой фразы получения ответов, полученные кандидаты классифицируются с помощью набора zero-shot классификаторов по большому количеству критериев (язык ненависти, вульгарность, спам и т.д.).

Наконец, окончательное множество текстов-кандидатов обрабатывается моделью резюмирования/перефразирования, которая старается переписать входной текст, удаляя повторы и создавая однородный текст для удобства чтения.

Мы планируем поделиться бóльшим количеством технических деталей, обращая особое внимание на масштабируемость, спустя несколько недель после полномасштабного релиза.

Обратите внимание, что в настоящее время Summarizer отключен в Очках Поиска Brave (нашей инновационной функции, позволяющей пользователям создавать собственные фильтры для изменения ранжирования результатов поиска). Мы продолжим совершенствовать наши модели для качественной совместимости с пользовательскими Очками, и в ближайшем будущем поделимся новой информацией о наших успехах.

Заключение

Summarizer от Поиска Brave является нашим очередным шагом по улучшению релевантности поиска и ответом на последние изменения, которые были вызваны выходом ChatGPT в прошлом декабре. Новость о том, что Microsoft собирается глубоко интегрировать модели OpenAI в их поисковую систему Bing сильно повлияла на рынок: несмотря на то, что этой системы ещё нет в публичном доступе, изначальный фидбек говорит об этой модели и как о впечатляющей , и даже как о пугающей .

Вне сомнений, индустрия сейчас генерирует много хайпа вокруг ИИ, но мы в Brave пока не полностью убеждены в том, что большие языковые модели радикально преобразят поиск. Тем не менее, при правильном применении подобные модели могут помочь пользователям в работе с результатами поиска, что и является нашим подходом к Summarizer. На настоящий момент чатоподобные интерфейсы являются новой технологией, и мы не убеждены, что они подойдут для всех поисковых задач.

Тем не менее, мы верим в потенциал больших языковых моделей и продолжим экспериментировать с их применением не только в поиске, но и в Бразуере Brave, где мы ожидаем поистине революционных изменений благодаря ассистентским возможностям больших языковых моделей."'https://habrastorage.org/getpro/habr/upload_files/91d/700/0d3/91d7000d3a65423d64b31b0d5d6dba51.png'"['https://habrastorage.org/r/w780q1/getpro/habr/upload_files/6a4/8b2/801/6a48b2801ca3664ed9e6a8895bdb67af.jpeg', 'https://habrastorage.org/getpro/habr/upload_files/91d/700/0d3/91d7000d3a65423d64b31b0d5d6dba51.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/company/29e/d16/a6e/29ed16a6e1c6c2e66a63c92e9e38659b.png', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/867/d66/015/867d660150cf0421795490030ef1c79c.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/7f3/c13/dbc/7f3c13dbcdf0ce6f5a63cb08ae196ccd.jpeg']"
14'720940'Активность найма на IT-рынке в феврале 2023'Ежемесячно мы смотрим, какие компании публикуют больше всего вакансий и изучаем активность найма в разрезе специализаций и квалификаций. А ещё собираем эффективные вакансии за месяц: те, которые...'https://habr.com/ru/post/720940/'"Ежемесячно мы смотрим, какие компании публикуют больше всего вакансий и изучаем активность найма в разрезе специализаций и квалификаций. А ещё собираем эффективные вакансии за месяц: те, которые привлекли больше всего внимания специалистов.

Собрали рейтинг активности найма в феврале — приглашаем посмотреть на лидеров прошлого месяца под кат.

Всего в феврале на Хабр Карьере можно было откликнуться на 3323 вакансии — на 32% меньше, чем в январе. Доля вакансий с возможностью удаленной работы — 63%, это на 4% меньше, чем в прошлом месяце.

Кто нанимал

Как и в нашем ежегодном рейтинге IT-работодателей , мы не сравниваем большие компании на тысячи сотрудников с маленькими стартапами. Рейтинг компаний по найму делится на четыре группы по размерам организаций: огромные (более 5000 сотрудников), большие (1000−5000 сотрудников), средние (100−1000 сотрудников) и небольшие (10−100 сотрудников).

Но в этом рейтинге мы собираем топ-3 компании из всех на Хабр Карьере, которые разместили больше всего вакансий за месяц. Вот топ февраля:

Лидеры по найму в феврале по размерам компаний:

Кого нанимали

Посмотрели на специализации и квалификации, которые чаще всего искали в феврале.

Вакансии по специализациям

Собрали все вакансии, которые компании публиковали в феврале — напоминаем, что их было 3323 — посчитали специализации, которые искали чаще всего, и составили топ-5.

Самая востребованная специализация на Хабр Карьере — бэкенд : 750 вакансий. Для системных аналитиков опубликовали 314 вакансии. На третьем месте вакансии для фронтендеров — их было 185. Для девопсов в феврале опубликовали 175 вакансий, а для мобильных разработчиков — 130.

Вакансии по квалификациям

Собрали все вакансии февраля на Хабр Карьере и отфильтровали по тем, в которых указана квалификация.

Больше половины вакансий, в которых указали квалификацию, опубликовали для мидлов — 1527 вакансий. Треть вакансий — для сеньоров: 950. Для лидов в феврале опубликовали 275 вакансий, а для джунов — 95. Для стажеров в феврале было 13 предложений.

Эффективные вакансии

Чтобы дополнить исследование, мы решили добавлять к ним вакансии, которые получили наибольшее число откликов на количество просмотров за месяц — мы назвали это эффективностью. Вот вакансии февраля:

Самая эффективная вакансия в феврале была от компании АльфаСтрахование — они искали к себе в команду фронтендера. Вакансию посмотрели 632 человека, из них откликнулись 147.

На втором и третьим местах — тоже вакансии для фронтендеров, но уже от компаний МТТ и DatsTeam.

Эффективность всех вакансий можно увидеть в крайнем правом столбце графика.

Как мы считали

Это не очень сложно: чтобы посмотреть, сколько вакансий компании опубликовали в феврале, мы взяли данные нашего сервиса. Сложили все вакансии, на которые можно было откликнуться в феврале — в них вошли в том числе те вакансии, которые были открыты в январе, но действовали в феврале. Далее мы посмотрели, кто размещал вакансии и разделили работодателей по размерам компаний. А потом просто выбрали компании, которые чаще всего размещали вакансии в феврале, и принесли вам.

Для расчёта эффективных вакансий нам также понадобились данные нашего сервиса, который умеет считать эффективность размещений — количество откликов, поделённое на количество просмотров. Нам оставалось только выбрать вакансии с высоким показателем.

А чтобы составить диаграммы по специалистам, мы взяли все вакансии за февраль и отфильтровали по необходимым для нас критериям: специализациям и квалификациям.

Если вы работодатель и собираетесь разместить вакансию на Хабр Карьере, но не знаете, как это сделать, то мы составили подробную инструкцию в хелпе . Если у вас есть вопросы — пишите нам на hr@habr.team, и мы обязательно на них ответим.

Больше о карьере в наших соцсетях: ВКонтакте, Telegram, Twitter, Яндекс.Дзен."'https://habrastorage.org/getpro/habr/upload_files/681/50e/dd4/68150edd43a4a1bbe1beefada1e07817.png'"['https://habrastorage.org/r/w1560/getpro/habr/upload_files/313/e69/6d1/313e696d1f45ed0d2f53ca4d77dcf9cc.png', 'https://habrastorage.org/getpro/habr/company/9c6/c45/8ec/9c6c458ecaaf05cefada4fa4fb83a3b4.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/84c/e20/116/84ce20116c82eed14b48b3d127dbb7f4.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/a3b/5d6/96e/a3b5d696eedc404b2e3a0f5af7125b64.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w32/getpro/habr/avatars/f55/eba/556/f55eba556d44f143a2af69452d2c2d03.png', 'https://habrastorage.org/getpro/habr/upload_files/681/50e/dd4/68150edd43a4a1bbe1beefada1e07817.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/dc2/4c7/8a9/dc24c78a9505998a59e9e73f16049fca.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/681/50e/dd4/68150edd43a4a1bbe1beefada1e07817.png', 'https://habrastorage.org/getpro/habr/avatars/f55/eba/556/f55eba556d44f143a2af69452d2c2d03.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/d3e/8be/564/d3e8be56491a60bf69f5fae3ddc8c076.png']"
15'719948'[Перевод] Произвольные красивые цвета: RGB, HSL, LCH и вот это вот всё'RGB какой-то отстой. Модель RGB, мало чем отличающаяся от ASCII, адресов памяти и наличия 86 400 секунд в сутках, является одним из тех инструментов, которые немного упрощают программирование, до...'https://habr.com/ru/post/719948/'"RGB какой-то отстой.

Модель RGB, мало чем отличающаяся от ASCII, адресов памяти и наличия 86 400 секунд в сутках, является одним из тех инструментов, которые немного упрощают программирование, до поры до времени.

Теоретически RGB — это группа цветовых пространств, которая позволяет указать дисплею, какое напряжение требуется каждому субпикселю. Однако на практике теперь у нас есть телефоны с дисплеями, которые позволяют отображать более 100% красного цвета, что является новым типом красного цвета, называемым суперкрасным. У нас есть другие дисплеи, в которых синего в два раза больше, чем красного или зелёного. И, вероятно, уже некоторое время ваши значения RGB не соответствуют напряжениям дисплея.

RGB сложно осмысливать рационально. Красный, зелёный и синий источники света ведут себя не так, как мы привыкли — вы можете видеть отдельные цвета вблизи, но по мере удаления они смешиваются вместе, и вы начинаете видеть только один цвет. С достаточно большого расстояния вы не сможете убедить свой разум в том, что существует три источника света. В настоящее время вы смотрите на миллионы крошечных массивов из 3 источников света, и всё же эффект настолько убедителен, что вы почти никогда не задумываетесь об этом.

Наконец, RGB трудно настраивать. Если вы начнёте с чёрного, вы можете увеличить количество “red” (красного) в палитре цветов RGB, что сделает всё более красным. Всё идет нормально. Затем вы начинаете увеличивать “green” (зелёный), и вы получаете… жёлтый? Это не очень интуитивно понятное цветовое пространство для навигации. Есть и другие представления цветов, которые легче поддаются изменению.

Цвета на годы

У меня есть личное приложение, где мне необходимо показать график за несколько лет. Каждому году на графике требуется другой цвет, поэтому каждый новый год я захожу в код, нахожу новый красивый цвет для нового года и развертываю приложение. Сколько лет я буду этим заниматься, пока не найду алгоритм, с помощью которого можно это автоматизировать?

Мне необходимы некоторые цвета, которые:

а) произвольны при генерации кода;

б) красиво выглядят;

в) определяются исключительно целым числом года.

Нам требуется реализовать такую функцию:

func color(for year: Int) -> Color

RGB действительно может удовлетворить только первому из моих критериев — эта модель может создавать случайные цвета со случайными числами:

Color(red: .random(in: 0..<1), blue: .random(in: 0..<1), green: .random(in: 0..<1))

К сожалению, цвета, сгенерированные таким образом, выглядят очень плохо. Они часто получаются грязными и румяными, а создание более чем одного цвета не сопровождается каким-либо узором или структурой. Цвета повсюду.

Это структурная проблема с RGB. RGB фокусируется на том, как создается цвет, а не на том, как он воспринимается.

К счастью, решение этой проблемы хорошо задокументировано. Есть несколько сообщений в блогах и постах (предупреждение: JavaScript), в которых излагается подход. Идея такова: используя цветовое пространство, основанное на оттенках, такое как HSL, вы можете сохранять два параметра постоянными saturation и lightness (насыщенность и яркость) и изменять только оттенок, давая вам несколько цветов, которые живут в одном и том же “family” (семействе).

(Существуют тонкие различия между HSL, HSB, HSV и HWB, но чередование оттенков в основном одинаково во всех цветовых моделях, и любая из них будет хорошо работать с этой техникой.)

Например, использование значения 0.8 для saturation и lightness дает хорошие пастельные тона:

Color(hue: .random(in: 0..<360), saturation: 0.8, lightness: 0.8)

Вы можете играть с этой палитрой цветов; переместите ползунок “hue” (оттенок), чтобы увидеть множество цветов в этом семействе.

С другой стороны, значения 0.6 для saturation и 0.5 для lightness дают более насыщенные цвета:

Color(hue: .random(in: 0..<360), saturation: 0.6, lightness: 0.5)

Эта палитра цветов показывает примеры этих цветов.

Проницательные читатели заметят, что в то время как собственные API-интерфейсы Apple принимают число от 0 до 1, этот фальшивый инициализатор, который я сделал, ожидает оттенок от 0 до 360. Я нахожу это более наглядным, потому что это значение представляет собой некоторое количество градусов. Здесь есть физическая аналогия с кругом оттенков. Круги зацикливаются сами на себе, поэтому 3590 в основном того же цвета, что и 10. Это позволяет вам выйти за пределы круга оттенков и изменить его на 3600, чтобы вернуться к разумному цвету.

Это позволяет нам реализовать большую часть нашей функции color(for year: Int):

func color(for year: Int) -> Color { let spacing = ... return Color(hue: (year * spacing) % 360, saturation: 0.8, lightness: 0.5) }

Интервал представляет собой количество градусов, которое необходимо пройти по кругу оттенков каждый раз, когда нам необходимо выбрать следующий цвет.

Какое оптимальное число выбрать здесь?

Вращение в пространстве оттенков

Если мы сделаем этот угол слишком близким к нулю, цвета будут располагаться слишком близко друг к другу на круге оттенков, что сделает их слишком похожими. Однако, если мы сделаем это слишком близко к значению 3600 (полный оборот), после изменения градусов на 360 они всё равно будут слишком похожи, за исключением того, что они будут идти назад по кругу оттенков. Может быть, мы хотим попробовать 1800 ? Это делает все остальные цвета абсолютно одинаковыми, так что это тоже не совсем правильно.

На самом деле, любое вращение, которое равномерно делится на 3600, через некоторое время приведет к повторению. А у 360 много факторов!

Одно из решений состоит в том, чтобы разложить компоненты на 360 значений, разделенных на количество лет, которые у нас есть, и тогда цвета будут меняться каждый раз, когда наступает новый год. Это создает радугу, которая, хотя и выглядит красиво, не имеет случайного представления, к которому я стремлюсь.

Однако есть лучший способ сделать это, и ответ содержится в видео на YouTube, которое я смотрел более 10 лет назад. Замечательный Ви Харт (Vi Hart) опубликовал серию видеороликов (раз, два, три) о том, как растениям необходимо отрастить свои новые листья таким образом, чтобы они не были заблокированы верхними листьями, что позволяет им получать максимум солнечного света. Во втором видео из этой серии есть соответствующий бит.

Число градусов вокруг стебля, из которого растение решает вырастить свой следующий лист — это именно то число, которое мы ищем. Это некоторый поворот, на который мы получим неперекрывающиеся листья — я имею в виду цвета.

Поскольку любое рациональное число приведет к повторяющимся цветам или перекрывающимся листьям, она ищет иррациональное число. В идеале “самое” иррациональное число. Она находит его в ϕ, это примерно 1.618. Нам необходимо проходить 1/1.618 часть круга оттенков каждый раз, когда нам требуется новый цвет, и это даст нам необходимые цвета.

Если цвета вам не нравятся, вы можете дополнительно повернуть, добавив фазовый сдвиг в уравнение:

func color(for year: Int) -> Color { let spacing = 360/1.618 return Color(hue: 300 + (year * spacing) % 360, saturation: 0.8, lightness: 0.5) }

Эта функция соответствует нашим критериям, цвета, которые получаются из неё:

а) произвольны;

б) выглядят неплохо;

в) определяются исключительно годом.

Следующий шаг

Если ваша единственная цель — несколько простых цветов для прототипа или побочного проекта, то того, что я рассказал до сих пор, будет достаточно. Но если вы хотите использовать это в более серьёзных и масштабных приложениях, вы можете сделать ещё один шаг.

Цветовая модель HSL имеет несколько серьезных недостатков. Она, как и RGB, была разработана для простоты вычислений, а не для точности базовых цветов. В частности, при повороте значения оттенка (что мы и делаем с помощью этой техники) вы обнаружите, что некоторые оттенки окрашены намного светлее, чем другие, даже если saturation и lightness остаются постоянными. Эти цвета выглядят светлее, хотя технически они являются одной и той же “lightness”.

Цветовое пространство LCH (luminance, chroma, hue) решает эту проблему. Насколько я могу судить, это золотой стандарт цветов на дисплее. Это даёт вам единообразие восприятия, что позволяет вам поворачивать оттенок и получать цвета, которые даже больше похожи друг на друга, чем вы могли бы получить с помощью HSL. Это также даёт некоторые преимущества, когда дело доходит до контраста при чтении текста.

На самом деле, если вы внимательно посмотрите на приведённые выше цвета (которые представляют собой цвета для 2015–2023 годов с использованием нашего алгоритма), этот зелёный лайм выглядит немного приглушенным по сравнению с его фиолетовым соседом.

Здесь вы можете поиграть с палитрой цветов LCH. Чтобы заставить LCH работать с UIColor, вы можете использовать эти четыре полезных принципа.

Использование LCH для создания моих цветов с помощью описанной выше техники вращения оттенков дало красивые цвета.

func color(for year: Int) -> Color { let spacing = 360/1.618 return Color(luminance: 0.7, chroma: 120, hue: 300 + (year * spacing) % 360) }

Этот браузер не поддерживает цвет LCH. Попробуйте Safari или мобильную версию Safari 15 или выше.

Все эти цвета имеют одинаковую lightness, и они отлично смотрятся для чего-то полностью процедурно сгенерированного. Они яркие, однородные и замечательные.

Модель, которую вы выбираете для жизни, создает ограничения, которые вы, возможно, не предполагали ограничивать. Любой цвет из любого из этих цветовых пространств может быть (более или менее) переведён в любое другое цветовое пространство с небольшой разницей. Поэтому цвета, которые мы получили в итоге, могут быть записаны в терминах красных, зеленых и синих значений (опять же, здесь махнув немного рукой). Но хотя RGB может представлять эти цвета, это не означает, что вы можете легко перемещаться по пространству таким образом, чтобы получить цвета, которые хорошо смотрятся вместе. Выбор правильного цветового пространства для начала делает проблему, по крайней мере, решаемой.

Поправимо, но до сих пор не решено. Эти произвольные красивые цвета могут быть сгенерированы с помощью процесса, стохастически открытого эволюцией, открытого учеными в 1830 году, и применённого на практике с использованием надёжного набора веб-стандартов, которые позволяют мне показать их вам в браузере.

В конце концов, стремление растений к солнечному свету стало ключом к созданию приятных цветов для моей маленькой диаграммы."'https://habrastorage.org/getpro/habr/upload_files/774/aba/37f/774aba37f6bd04ba0657dabcb7f9d1b5.webp'"['https://habrastorage.org/r/w32/getpro/habr/avatars/b7e/27b/bbd/b7e27bbbd2d1a34f1c0f6bca1d5e2f96.jpg', 'https://habrastorage.org/getpro/habr/avatars/b7e/27b/bbd/b7e27bbbd2d1a34f1c0f6bca1d5e2f96.jpg', 'https://habrastorage.org/getpro/habr/upload_files/743/f3e/a52/743f3ea52aafc6900f2f03b6876165a1.webp', 'https://habrastorage.org/getpro/habr/upload_files/e25/293/dea/e25293dea06b2d563fec23282aba3c0b.webp', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/upload_files/774/aba/37f/774aba37f6bd04ba0657dabcb7f9d1b5.webp', 'https://habrastorage.org/getpro/habr/upload_files/be8/99a/15e/be899a15eeb48f68b4345c3b35d11d61.webp', 'https://habrastorage.org/getpro/habr/upload_files/6a1/143/d02/6a1143d021ceee8979473d3795a12ddd.webp', 'https://habrastorage.org/getpro/habr/upload_files/6c2/370/e5d/6c2370e5de789e67ba2f32c238b2fb8c.webp']"
16'720822'Подойти к айти: хочу зарабатывать и не разрабатывать'Когда деревья не болели ковидом, на юг улетали, в основном, птицы, а в автобусе можно было кашлять, я решил подучиться и пошёл осваивать разработку программного обеспечения на большой, серьёзный...'https://habr.com/ru/post/720822/'"Аналитик данных

▍ Средняя заработная плата Junior

▍ Начальные требования к компетенциям для Junior

экономисты и финансисты могут заниматься финансовой математикой и разработкой в финтехе, быть аналитиком предприятий и улучшать бизнес-процессы, непрерывно анализируя разные срезы информации;

юристы могут создавать интересные сервисы для юристов либо переквалифицироваться в аналитика в юридической сфере (в зависимости от специализации — от патентного права до уголовного розыска и форензики);

переводчики и филологи могут заниматься компьютерной лингвистикой, семантикой и NLP (обработкой естественного языка);

социологи активнее других осваивали обработку данных ещё в 2000-х, поэтому они точно знают, чем сейчас могут заняться или как именно переквалифицироваться.

Специалист по кибербезопасности

▍ Средняя заработная плата Junior

▍ Начальные требования к компетенциям для Junior

Инженер по тестированию

▍ Средняя заработная плата Junior

▍ Начальные требования к компетенциям для Junior

Менеджер проекта

▍ Средняя заработная плата Junior

▍ Начальные требования к компетенциям для Junior

работа с требованиями: сбор, обработка, анализ, создание и развитие рабочих групп;

профессиональное управление проектами;

ручное тестирование;

основы дизайна и UI/UX;

бизнес-аналитика;

разработка технических заданий;

планирование и прогнозирование;

гибкие системы управления разработкой;

управление персоналом;

и — строго обязательно — знать сферу и окружение вашего проекта.

IT-консультант

▍ Средняя заработная плата Junior

▍ Начальные требования к компетенциям для Junior

▍ 1. Собственно IT-консультанты

▍ 2. Бизнес-аналитики

Специалист по маркетингу

▍ Средняя заработная плата Junior

▍ Начальные требования к компетенциям для Junior

HR

▍ Средняя заработная плата Junior

▍ Начальные требования к компетенциям для Junior

отлично разбирается в трудовом законодательстве и способен разобраться в любом законе;

коммуникабелен;

разбирается в поведенческой психологии, психологии организации и конфликтологии, а ещё лучше имеет классическое психологическое образование;

понимает, что его задача — защита интересов сотрудника и только потом интересов работодателя;

готов автоматизировать часть своей работы и выстроить адекватный документооборот;

разбирается в сфере работы организации, готов оценивать специалистов всесторонне;

корректен и вежлив;

идеально работает в команде и вписывается в новые команды (каждый подбор — командный проект).

Переводчик

▍ Средняя заработная плата Junior

▍ Начальные требования к компетенциям для Junior

российским компаниям, которые активно используют англоязычные ресурсы для разработки (сейчас всё меньше);

российским компаниям, которые всеми правдами и неправдами мимикрируют под западные и идут на мировые рынки (от Европы до Японии и Китая) — у таких колоссальный фронт работ на нескольких иностранных языках: рассылки, письма, поддержка, интерфейсы, поездки, представительство, реклама, обзоры, пиар и маркетинг…;

российским компаниям, которые уезжают — переводчики нужны для всего: от оперативной работы до юридических вопросов и поддержки при релокации;

иностранным компаниям, которые идут на российский рынок — да, такие есть, открытые и не афиширующие, востребованная функциональность в них совершенно разная.

Специалист службы поддержки

▍ Средняя заработная плата Junior

▍ Начальные требования к компетенциям для Junior

P.S.

Когда деревья не болели ковидом, на юг улетали, в основном, птицы, а в автобусе можно было кашлять, я решил подучиться и пошёл осваивать разработку программного обеспечения на большой, серьёзный вечерний курс в оффлайне (увы, его уже нет в помине, а жаль). Мне это было нужно для более уверенной работы. В группе были три девушки и семь парней, среди них: дизайнер, которая пришла, чтобы лучше понимать разработчиков; инженер, мечтавший уйти из оборонки; менеджер, которая хотела расти на рабочем месте; 1С-ник с шестизначной зарплатой, которого манил новый стек; биолог, который хотел перейти в команду кибермедицины и т. д. В общем, только двое собирались быть разработчиками, остальные рассматривали IT как способ поднять свою ценность. При этом они откровенно хотели обойти айти, то есть разработку, проектирование, кодинг и т. д. Кстати, программистами в итоге стали четверо, а дизайнер бросила всё и уехала рехабиться на какой-то остров — сразу после курса C++. У остальных планы сбылись.Так куда идти, если хочется в айти, но ты боишься питонов, хомяков, слонов и ненавидишь кофе?: 120-170 тыс. руб. (без программирования 60-100 тыс. руб.): SQL, Python, Microsoft Excel, PowerBIЭто интересная профессия, которая кроме всего прочего имеет несколько уровней в зависимости от подготовки: можно заниматься классической статистикой, финансовой математикой, можно писать SQL-запросы и обрабатывать полученные массивы данных, а можно ковырять big data с помощью R, Python, C++ и т. д. Опять же, можно сочетать навыки программирования и знание статистики с основной специальностью: с таким стеком знаний востребованы биологи, врачи, лингвисты, химики, физики, метрологи, строители и проч. Особенно радужные перспективы у специалистов естественно-научного профиля, которые готовы сделать шаг в сторону и освоить IT.Однако для работы в естественно-научном профиле нужно строгое соответствие по образованию. Если же вы учились в начале 2000-х и за вас решали родители (ладно-ладно, это я несу в статью свой комплекс), то:Ну а если вы всё ещё верите в науку и больны ею, то можно разрабатывать аналитические модели в разных сферах, экспериментировать с данными в лабораториях и т. д. Подобные запросы можно встретить у крупного финтеха, операторов связи, предприятий космоса и проч.: 80-140 тыс. руб.: знание систем безопасности, основ шифрования, виртуализация, Linux, сетевая модель OSI и основные протоколыАмбициозная и сложная профессия, вокруг которой ведётся множество споров: одни эксперты говорят, что делать в инфобезе без навыков разработки нечего, другие приводят массу примеров, где кодинг не нужен (работа с трафиком в компаниях связи, проектирование контура безопасности, консалтинг, аналитика безопасности). На самом деле, правы и те, и другие — опять же, всё зависит от ваших способностей, возможностей и зарплатных ожиданий. И если насчёт кодинга можно подискутировать, то английский язык в этой сфере — колоссальное преимущество и почти всегда необходимость.Большой плюс в том, что сегодня студенты могут прямо в вузе обучаться на специалиста по информационной безопасности, и им не нужно мучительно переучиваться и искать свой путь. В отдельных вузах есть программы переподготовки для взрослых (рекомендую только ради бумажки, учебный план того, что мне приходилось видеть, не вдохновляет).Новичку найти работу в сфере кибербезопасности очень сложно, особенно, если в вашем регионе не представлены крупные компании. Поэтому лучше перерасти этот уровень в той компании, где вы работаете: если у вас есть служба защиты, служба информационной или экономической безопасности, или, на худой конец, отдел системного администратора, попросите о горизонтальном перемещении и сочетайте практику с самообразованием. Так вы получите навыки для успешного перехода в «настоящую» безопасность.: для ручного тестирования (мы же договорились с минимумом программирования) около 60-65 тыс. руб.: ручное тестирование, функциональное тестирование, тестирование API, тестирование ПО, разработка тест-кейсов, баг-трекинг, тестирование мобильных приложений, сетевая модель OSI и основные протоколы.Помните мифических сирен, которые своим пением завораживали моряков и топили их корабли? С тестированием та же история: прекрасные сирены школы программирования проповедуют, что для начала карьеры в IT и около IT нет ничего лучше, чем тестирование. Увы, это не совсем так: во-первых, на тестировщика лучше поучиться, чтобы понять общие принципы и выстроить в голове «механику» этой работы; во-вторых, это серьёзная, тяжёлая и довольно скучная работа; в-третьих, хороший рост возможен только при дальнейшем погружении в программирование (автоматизация тестирования). Да, карьерный путь до старшего инженера по тестированию можно пройти за год, в этой сфере почти всё зависит от вас, но это будет нагруженный путь.И если в 2010-2013 году я бы вам посоветовал заняться ручным тестированием и на этом остановиться, чтобы долго и стабильно работать, то сейчас этот тезис выглядит глупо. Компании не готовы держать отдельных специалистов по ручному тестированию, требования к компетенциям становятся год от года всё выше. Да, для базовой автоматизации тестирования вам не понадобится глубокое знание разработки, но и карьера будет продвигаться так себе.: 80 тыс. руб.: управление проектами, Agile, Jira.Сразу обозначу свою личную позицию, а потом расскажу про рынок. Я считаю, что менеджер проекта обязан разбираться в технологиях, понимать стек, с которым работает его проект, уметь быть аналитиком и тестировщиком. Рынок считает иначе: в менеджеры проекта можно попасть после любой работы в бизнесе, а если вам повезло и бизнес был айтишным, рассчитывайте на хороший офер. С болью в сердце я вижу, как проектами в финтехе, IT FMCG, телемедицине, веб-разработке и т. д. занимаются бывшие маркетологи, SMM-щики, саппортёры, филологи. Считается, что хороший управленец не имеет специфики опыта и образования. Да-да, врождённый талант. Мы с вами ежедневно ощущаем это на массовых продуктах, которые используем.На самом деле, это как раз хороший способ остаться в IT и не заниматься разработкой. Более того, менеджеры проектов достаточно быстро растут и быстро находят новую работу. Если хотите стать достойным и востребованным специалистом, освойте ряд навыков:Опять буду груб (наболело!). Если вы освоите всё перечисленное, то команда проекта в лице разработчиков не будет плевать вам в спину, когда вы закроете дверь, а будет уважать и считаться. Работа менеджером без этих навыков — дискомфортное мельтешение.: 50-60 тыс. руб. в продуктовом консультировании, от 75 тыс. руб. в бизнес-аналитике: SQL, Microsoft Excel, анализ требований, базы данных, бизнес-аналитика, Python.Тут я разобью повествование на два небольших блока.Древнейшая профессия в IT :) Не то чтобы я намекал на сущность консультантов,… нет, просто эти ребята начали своё существование едва ли не вместе с программистами. Как правило, это были сотрудники крупных зарубежных вендоров прикладного бизнес ПО (CRM, ERP, САПР, АСУ, BI…), которые приходили в компании, обследовали процессы и команды, а затем вешали отборную лапшу на уши и давали откаты помогали команде технического директора выбрать подходящую конфигурацию, раскрутить контору на бабки спроектировать доработки, провести внедрение и обучение пользователей. Деловые, хваткие ребята. Кстати, почти всегда исключительно приятные.Сейчас по понятным причинам их стало значительно меньше, все освободившиеся разошлись по российским вендорам и системным интеграторам. Найти такую работу довольно трудно. Но, если бы была такая необходимость, я бы смотрел в сторону продуктового портфеля фирмы 1С и российских лидеров системной интеграции и разработки корпоративного ПО. Если повезёт, то можно найти интересную небольшую компанию и быстро вырасти до тимлида — думаю, на волне импортозамещения такой спрос точно будет. И да, ценно, что компании любят брать на эту должность толковых молодых ребят и специалистов из той сферы, для которой работает интегратор, — всему остальному обучают сами. То есть стать в такой компании джуном очень даже вероятно.Более актуальная на сегодня специальность: вы делаете то же самое, что консультант, только аналитика не формальная и умозрительная, а основанная на требованиях, данных, работе с бюджетом, глубоком анализе всех факторов развития бизнеса, показателей, а также рисков. Обычно с этой работой хорошо справляются экономисты, юристы, математики, инженеры профильных отраслей (особенно ценные сотрудники). Я бы назвал эту специальность гибридом ранее описанных менеджера проекта и аналитика данных.Обратите внимание на то что в обоих случаях работа ведётся на стороне клиента и важно иметь высокие коммуникационные и даже дипломатические навыки. Без этого никак.: от 50 до 90 тыс. руб., зависит от компании и задач.: умение анализировать, понимание глубинной теории пиара, продвижения и продаж, навыки работы с воронкой клиентов.А вы-то, читая текст выше, наверняка думали, что именно маркетолог — древнейшая профессия в IT. Не, ну тут по характеру деятельности почти в точку. Шучу. Так вот, маркетинг в IT-компаниях сильно отличается от всех остальных отраслей, поэтому будьте готовы к тому, что придётся погружаться в работу с клиентами, в мельчайшие детали функциональности продукта, который нужно «маркетить», то есть продвигать и развивать. Не буду описывать основные вехи работы: вам может понадобиться всё, от навыков социологии до знания SQL.Предупрежу только, что найти стажировку в маркетинге несложно, стать джуниором тоже легко и быстро, но от маркетинга в IT-компаниях страшно устаёшь, потому что нужно лавировать между согласованиями, релизами, требованиями пользователя и велением рынка. Выгорание наступает через 2-4 года такой работы. И да, пиарщиков и маркетологов полно, с избытком, а по-настоящему хороших крайне мало. Примерно так же мало, как адекватных, умеющих слушать руководителей. Думаю, эти два факта имеют прямую корреляционную связь.: 40-60 тыс. руб.: понимание принципов рекрутинга, знание законодательной базы.Хотел бы сказать, что это самая неайтишная специальность из околоайтишных, но язык не поворачивается: я знаю тимлидов разработки, которые ушли «с нуля» в HR и DevRel, чтобы быстро вырасти и подбирать в свои компании действительно удачных специалистов, которые разделяют опыт, знания и дух проектов.Стать HR в компании довольно просто, многие активно набирают стажёров на внешние коммуникации, внутренние коммуникации, кадры, деловой и кадровый документооборот и проч. Вырасти сложнее, потому что хороших HR прямо очень много не нужно — они есть даже не в каждой компании. Поэтому я рекомендую ориентироваться либо на компании с активной корпоративной культурой, либо на больших лидеров рынка, где ёмкость сотрудников служб управления персоналом стабильно высокая.Хороший HR обладает основательной базовой комплектацией знаний и умений:Внутри HR можно вырасти в деврела (DevRel) — мага и волшебника, который развивает таланты разработчиков, вдохновляет их на выступления на конференциях и на ведение блога на Хабре, решает возникающие проблемы, организует прямую или опосредованную связь разработчиков и руководства и даже помогаете с релокацией. DevRel довольно редкая вакансия, стать им с уровня джуна не получится, а вот дорасти или стать первым DevRel в компании, где вы занимались подбором и развитием персонала, возможно. Кстати, в отличие от нудного эйчара, деврелы сталкиваются с разнообразной и интересной работой. Но опять же, кому что нравится.: зависит от уровня подготовки и опыта, от 50-60 тыс. руб.: знание одного языка уровня fluent говорение и деловая переписка, способность читать и понимать профессиональную литературу, второй язык как преимущество (но опять же зависит от мультилингвальности компании).Работа переводчиков в IT настолько странная и многоплановая, что я даже не смогу описать вам однозначный профиль задач такого специалиста.Переводчики нужны:Как правило, в IT-компаниях как таковые переводчики встречаются редко. Чаще всего ценятся специалисты, способные и готовые совмещать операционные функции и иностранный язык. Оплата очень разная, от 50 тыс. руб. за несложные задачи в регионах до 300 тыс. и больше в случае высокой занятости и сложных задач, связанных с командировками. Почти всегда однозначно востребован английский язык, однако при работе по концепции «весь мир» востребованы по убыванию: немецкий, французский, испанский, японский, итальянский, китайский. Хотя думается, в свете переключения внимания на восточные и южноамериканский рынки не за горами и новый стек языков.: 65-75 тыс. руб.: хорошие коммуникативные навыки, отличное знание ПК, в IT часто востребовано знание багтрекеров, сетевых протоколов и модели OSI (базовое понимание). Как плюс — умение быстро и объективно разбираться в локальных и нормативно-правовых документах.Для меня это идеальный способ понять, готовы вы войти в IT или предпочтёте обойти :-) Настоящий нервный и активный фронтлайн, внутри которого вы прокачиваетесь как специалист, учитесь понимать требования, разбираться в документах. Работа сложная, не всегда адекватная по времени и нагрузке, но оно того стоит. Чаще всего это консультативная или инженерная работа, поэтому кодинг и математика минимальные, а чаще всего они в работе не встречаются.Работать внутри IT, но не заниматься разработкой и инженерной работой вполне возможно, интересно и не менее круто. Вообще, когда находишься в IT-компании, ты понимаешь, что «любимая айтишечка» движется силами не только программистов, админов и девопсов, но и фронтофиса, операционного офиса, коммерции, HR-департамента и т. д. Поражает, насколько разные люди способны объединяться в команды и делать мир технологичнее. Быть частью этого мира — захватывает.: в следующей части поговорим с вами о том, есть ли жизнь за пределами IT (Спойлер: ответ не столько однозначен)."'https://habr.com/share/publication/720822/f8ed8c28d0ce03abd2d09e6ffaff5eac/'"['https://habrastorage.org/r/w1560/getpro/habr/post_images/c4a/8c0/dca/c4a8c0dcaa63fa4573cb02e083b32dc5.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/company/027/b19/cea/027b19cea984722a8d806edcc56c2c00.png', 'https://habr.com/share/publication/720822/f8ed8c28d0ce03abd2d09e6ffaff5eac/', 'https://habrastorage.org/r/w1560/webt/sz/7j/pf/sz7jpfj8i1pa6ocj-eia09dev4q.png', 'https://habrastorage.org/getpro/habr/branding/ea2/32a/1c4/ea232a1c4d3c4f31e800a2221c764b84.jpg', 'https://habrastorage.org/getpro/habr/avatars/32f/cee/66c/32fcee66c8c0788509269ccb2d2bc11f.png', 'https://habrastorage.org/r/w32/getpro/habr/avatars/32f/cee/66c/32fcee66c8c0788509269ccb2d2bc11f.png']"
17'720936'[recovery mode] BlueTeam Trainings — разбор задания BlackEnergy категории Digital Forensics от CyberDefenders'Это вторая статья из цикла «BlueTeam Trainings». CyberDefenders является учебно‑тренировочной платформой, которая предоставляет возможность на практике проверять уже имеющиеся навыки и...'https://habr.com/ru/post/720936/'"Это вторая статья из цикла «BlueTeam Trainings». CyberDefenders является учебно‑тренировочной платформой, которая предоставляет возможность на практике проверять уже имеющиеся навыки и приобретать новые в области информационной безопасности.

Для прохождения этого задания достаточно иметь ПО Volatility(Wiki) и иметь базовое представление о том, как выглядит интерфейс этой программы. Скачать архив с дампом памяти можно тут. Приступим к работе!

Volatility Framework

Volatility Framework — это набор инструментов с открытым исходным Python кодом(GitHub). Фреймворк предназначен для извлечения цифровых артефактов из дампов оперативной памяти. Использовать volatility будем на ОС Windows, для этого воспользуемся Volatility 2.6 Windows Standalone Executable (x64) .

BlackEnergy

После скачивания архива приступим к его анализу! По непонятным причинам на платформе задание относится к категории «Medium», при этом ответить необходимо всего на 8 несложных вопросов.

Вопрос 1. Which volatility profile would be best for this machine?

Сам вопрос дает подсказку какое именно ПО необходимо использовать, чтобы определить профиль ОС, который наиболее подходит для анализа дампа памяти. Воспользуемся командой:

volatility.exe ‑f “D:\CYBERDEF.raw” imageinfo

‑f — позволяет указать путь к файлу, который необходимо исследовать с помощью volatility;

imageinfo — позволяет определить наиболее подходящий профиль ОС, с которого был снят дамп оперативной памяти.

Ответ WinXPSP2×86

Вопрос 2. How many processes were running when the image was acquired?

Необходимо узнать количество процессов, запущенных на момент снятия дампа с системы. Для решения этой задачи поможет аргумет pslsit.

Считаем количество процессов - ответ 19.

Вопрос 3. What is the process ID of cmd.exe?

Простейший вопрос, на который можно ответить с использованием предыдущего скриншота, так как третий столбец демонстрирует PID процесса cmd.exe. Ответ — 1960.

Вопрос 4. What is the name of the most suspicious process?

Довольно простое задание для экватора, так как мы снова можем ответить на него с помощью скриншота со списком процессов. Согласно определению из самого достоверного источника(Википедии) руткит — набор программных средств, обеспечивающих: маскировку объектов; управление; сбор данных. rootkit.exe подходит в качестве ответа!

Вопрос 5. Which process shows the highest likelihood of code injection?

Из списка процессов необходимо выбрать наиболее подходящий для внедрения кода. svchost.exe явялется основным процессом для загрузки служб ОС Windows, которые хранятся в DLL. Все службы, список которых можно посмотреть в services.msc, запускаюстя с помощью svchost.exe. Это один из неплохих вариантов для внедрения злоумышленником вредоносного исполняемого кода. Ответ — svchost.exe.

Вопрос 6. There is an odd file referenced in the recent process. Provide the full path of that file.

Для ответа на этот вопрос необходимо узнать путь к файлу, на который ссылается процесс svchost.exe. Для этого мы определяем, что анализируем процесс с PID 880. Для отображения открытых хендлов используем плагин handles и укажем тип — файлы. Команда будет выглядеть следующим образом:

volatility.exe -f ""D:\CYBERDEF.raw"" --profile=WinXPSP2x86 -p 880 handles -t file

По формату, в котором необходимо ввести ответ понимаем, что нам нужен файл из диска ""C"", который имеет три символа в названии и три в расширении. Под такое ""регулярное выражение"" подходит ответ C:\WINDOWS\system32\drivers\str.sys.

Вопрос 7. What is the name of the injected dll file loaded from the recent process?

DLL Injection — это техника, которая позволяет злоумышленнику внедрить DLL в процесс жертвы из памяти, а не с диска. Плагин ldrmodules позволяет отобразить перекрестные ссылки DLL с файлами, которые находятся в памяти. Воспользуемся командой:

volatility.exe -f ""D:\CYBERDEF.raw"" --profile=WinXPSP2x86 -p 880 ldrmodules

Распространенной практикой среди разработчиков вредоносных программ является попытка скрыть деятельность вредоносной программы, именно этот принцип и позволит определить нам зараженную библиотеку. Одним из методов является попытка скрыть DLL‑файлы, связанные с вредоносным кодом. Этого можно добиться путем отвязки подозрительной DLL от ProcessEnvironmentBlock (PEB). Таким образом можно обеспечить сокрытие вредоносной деятельности, но все еще остаются следы существования DLL, содержащиеся в VirtualAddressDescriptor (VAD). VAD — это механизм, идентифицирующий базовый адрес и полный путь DLL‑файла. Плагин ldrmodules сравнивает список процессов и определяет, находятся ли они в PEB. Анализ столбцов InLoad, InInit, InMem позволяют выявить «странную» библиотеку‑ответ msxml3r.dll.

Вопрос 8. What is the base address of the injected dll?

Для ответа на последний вопрос необходимо определить базовый адресс зараженной библиотеки. Для этого воспользуемся плагином malfind. malfind помогает найти скрытый или внедренный код/DLL в памяти пользовательского режима с помощью VAD. Цель malfind — найти DLL, которые не видят стандартные методы/инструменты, это можно проверить используя плагин dlllist, при использовании которого мы не встретим зараженную DLL. Для ответа на вопрос воспользуемся командой:

volatility.exe -f ""D:\CYBERDEF.raw"" --profile=WinXPSP2x86 malfind -p 880



Ответ — 0x980000.

Завершился второй разбор из серии BlueTeam Trainings, надеюсь данная статья была полезна широкому кругу лиц!"'https://habrastorage.org/getpro/habr/upload_files/f2f/381/82f/f2f38182fe1ea47454a5118b4c9b761f.png'"['https://habrastorage.org/getpro/habr/upload_files/f2f/381/82f/f2f38182fe1ea47454a5118b4c9b761f.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/305/6ed/3b6/3056ed3b66e6f8c47a86efa65f683f88.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/f9d/90b/76c/f9d90b76c959722fdb9baeb73202a2cc.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/945/e15/a44/945e15a44df011c762b3296864b3010b.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/379/a82/2d2/379a822d2e5dd7ec6143b64930f0eb2a.png', 'https://habrastorage.org/getpro/habr/avatars/b48/4c8/d69/b484c8d6953a2ad79457ea6845c45c1a.jpg', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/47c/956/5ca/47c9565cacc384a52b25decda883d3ff.png', 'https://habrastorage.org/r/w32/getpro/habr/avatars/b48/4c8/d69/b484c8d6953a2ad79457ea6845c45c1a.jpg']"
18'720652'[Перевод] Data Mesh: что это такое и для чего он нужен инженерам'Команда VK Cloud перевела статью о новом подходе к построению архитектуры данных Data Mesh с помощью lakeFS — системы управления версиями данных с открытым исходным кодом, которая преобразует...'https://habr.com/ru/post/720652/'"История данных и их аналитики

В больших компаниях возлагать ответственность за подключение всех источников данных на одну команду чревато провалом. Часто эти источники децентрализованы и географически распределены, что затрудняет даже банальный поиск ответственных. Подобный подход просто не работает. И тут на помощь приходит новая архитектура, которая называется Data Mesh.

Что такое Data Mesh

Архитектура Data Mesh: суть концепции

Архитектура Data Mesh: показания к применению

Проблемы архитектуры Data Mesh

Ограничения бюджета

Совместная работа доменов и команды по развитию платформы

Набор навыков по управлению данными

Нехватка технических навыков

Мониторинг Data-продуктов

Виртуализация и дублирование данных

Реализация: как преобразовать озеро данных в сервисы Data Mesh

Этапы реализации Data Mesh

Защитите имеющиеся данные в объектном хранилище, установив разрешения только на чтение.

Создайте репозиторий в lakeFS для каждого сервиса данных.

Загрузите уже имеющиеся исходные и выходные данные. Это операция на уровне метаданных — на самом деле транспортировка не происходит. Если некоторые датасеты используются для разных сервисов, они размещаются в нескольких репозиториях.

Напишите для каждого сервиса скрипт онбординга для каждого сервиса из репозиториев, которые предоставляют исходные данные. При каждом запуске этого скрипта должен выполняться новый коммит в главной ветке, с изменениями и обновлениями исходных данных.



Среда разработки для сервиса Data Mesh

Непрерывная интеграция данных в репозитории

Лучше создать ветку для приема данных. В идеале у каждого датасета должна быть собственная ветка для приема данных.

Дайте ей осмысленное название, например daily-sales-data.

С помощью Pre-merge протестируйте данные и убедитесь, что они соответствуют стандартам качества и передовых методов работы.

Если тест пройден, можно объединять данные с главной веткой. Если нет, система мониторинга высылает соответствующее уведомление. В случае неудачи у вас будет моментальный снимок репозитория на момент сбоя, и это поможет быстрее установить причину произошедшего. Данные не потеряны, ведь вы не передавали их в главную ветку.



Непрерывный деплоймент данных в репозитории

Оркестрация запускает DAG в выделенной ветке. Каждое задание выполняется в ветке, созданной из DAG.

После выполнения задачи инициируется вебхук, который проверяет качество данных.

Если тест пройден, данные из этого задания автоматически поступают в ветку DAG и начинается следующее задание.

Если тест не пройден, вебхук создает событие в системе оповещения со всеми релевантными данными. DAG перестает работать.

Когда выполнение завершается успешно, данные поступают обратно в главную ветку. Теперь их можно использовать для других сервисов или экспортировать из объектного хранилища в интерфейс уровня обслуживания.



Заключение

Команда VK Cloud развивает собственные Big Data-решения. Новым пользователям дарим три месяца на тестирование сервиса и консультацию архитектора по построению собственного решения.

Команда VK Cloud перевела статью о новом подходе к построению архитектуры данных Data Mesh с помощью lakeFS — системы управления версиями данных с открытым исходным кодом, которая преобразует хранилище объектов в Git-подобные репозитории. Разбираем, что такое Data Mesh, суть этого подхода и как с его помощью повысить эффективность работы с данными.Компании стали нуждаться в анализе данных, как только в обиход вошли первые компьютеры. В 1980-х компании создавали хранилища на основе реляционных баз данных, используя их в качестве систем принятия решений. Чем быстрее и больше разнообразной информации генерировали компании, тем очевиднее становились ограничения реляционных баз.С приходом 2000-х мы вступили в эпоху больших данных. Появились новые решения, предназначенные для анализа больших объемов разнообразных данных, генерируемых с огромной скоростью. В современных паттернах архитектуры и аналитики хранилища объединились с новыми технологиями для работы с большими данными.Однако при развертывании таких аналитических решений у компаний все еще возникали трудности. Архитектура оставалась монолитной, и одна команда всегда выступала в качестве поставщика платформы и занималась интеграцией данных. Такая система подходит для небольших организаций с высокой степенью централизации, а в крупных компаниях из-за такого подхода сразу же стали появляться длинные очереди за услугами интеграции и аналитических решений. В этом контексте централизация оказалась слабым местом крупного бизнеса.Data Mesh, что дословно можно перевести как «сеть данных», — это децентрализованный гибкий подход к работе распределенных команд и распространению информации. Главное в нем — междисциплинарные команды, которые публикуют и потребляют Data-продукты, благодаря чему существенно повышают эффективность использования данных.Понятие Data Mesh как архитектуры создания распределенных пайплайнов данных впервые ввела в обиход Жамак Дегани в статье How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh . Традиционно архитектура данных монолитна. Потребление, хранение, преобразование и вывод управляются через одно центральное хранилище (как правило, озеро данных). Data Mesh же позволяет упростить работу с распределенными пайплайнами, поддерживая отдельных потребителей, рассматривающих данные как продукт.Но что связывает домены и соответствующие активы данных? Это уровень универсальной взаимной совместимости, на котором применяется одинаковая инфраструктура, синтаксис и стандарты данных.Для понимания Data Mesh нужно знать четыре основных понятия:Это понятие пришло из парадигмы разработки ПО Domain Driven Design (DDD). Его используют для моделирования сложных программных решений. В Data Mesh домен данных — это способ определить, где начинаются и заканчиваются корпоративные данные. Границы зависят от компании и ее потребностей. Иногда разумно моделировать домены, учитывая бизнес-процессы или исходные системы.Важный компонент Data Mesh, связанный с применением к данным продуктового мышления. Чтобы Data-продукт работал, он должен приносить пользователям пользу в долгосрочной перспективе и быть пригодным к использованию, ценным и ощутимым. Он может быть реализован как API, отчет, таблица или датасет в озере данных.Data Mesh строится экспертами широкого профиля, которые создают универсальные продукты и управляют ими. В рамках этого подхода вы будете опираться на децентрализацию и согласование с бизнес-пользователями, которые разбираются в предметной области, какое значение имеют те или иные данные. При этом у вас будут специализированные команды, которые разрабатывают автономные продукты, не зависящие от центральной платформы. Поэтому не получится использовать сложные и узкоспециализированные инструменты для эксплуатации фундамента платформы на основе Data Mesh.Когда вы переходите на распределенную Data-платформу самообслуживания, нужно сосредоточиться на Governance. Если не уделять ему внимание, вы скоро окажетесь в ситуации, когда во всех доменах применяются разрозненные технологии, а данные дублируются. Поэтому и на уровне платформы, и на уровне данных нужно внедрить автоматизированные политики.Data-Mesh-решения позволяют компенсировать недостатки монолитных озер данных. Владельцы данных получают большую автономность и гибкость, открываются новые возможности для экспериментов, инноваций и совместной работы. В то же время снижается нагрузка на команды по обработке данных, задачи каждого потребителя решаются на местах в рамках единого пайплайна.В то же время благодаря платформе самообслуживания команды по обработке данных получают в свое распоряжение современные технологии для работы с данными с минимальными инвестициями (либо вообще без них). Кроме того, это универсальный и часто автоматизированный подход к стандартизации данных, Product Lineage и метрикам качества (их сбору и совместной работе с ними). В совокупности эти преимущества можно рассматривать как достойную конкуренцию традиционным архитектурам данных, которые страдают от отсутствия стандартизации на уровне источников и потребителей данных.Эти преимущества всего лишь верхушка айсберга. Вот еще несколько аргументов в пользу Data Mesh.До сих пор организации пытались обрабатывать объемные данные разных типов и сценариев использования, опираясь на централизованные решения. Но централизация подразумевает импорт или доставку данных с периферии в центральное озеро для последующего анализа. Это долго и дорого. Работа централизованной команды может стать узким местом компании, ведь данные создают множество сотрудников, а централизованной команде нужно расставить приоритеты и решить, с каких задач начинать.Data Mesh помогает ускорить анализ. В распределенной архитектуре мы рассматриваем данные как продукт, у которого есть владелец в лице отдельного бизнес-подразделения. Благодаря такой модели команды могут получить доступ и анализировать «периферийные» данные быстрее, чем когда-либо ранее.Объемы данных продолжают расти, и модель централизованного управления не справляется с увеличением масштабов . Гибкость бизнеса снижается, так как на извлечение пользы из данных и формулировку выводов уходит слишком много времени.Data Mesh решает эту проблему, возвращая крупному бизнесу гибкость и быстроту реакции на перемены. Из центра она делегирует владение датасетами доменам — отдельным командам или бизнес-пользователям. Это сокращает дистанцию между тем или иным фактом и его потреблением или процессом анализа.В ряде случаев организациям трудно соблюдать требования к конфиденциальности и месту расположения данных, которые хранятся в странах ЕС, но используются, например, в Северной Америке. Соблюдение этих требований — длительный и трудоемкий процесс, из-за которого периодически возникают задержки критически важной бизнес-аналитики, необходимой для сохранения конкурентных преимуществ.Data Mesh обеспечивает уровень связи, открывающий техническим и нетехническим пользователям непосредственный доступ к датасетам с возможностью выполнять запросы по месту нахождения информации. А также позволяет избежать их дорогостоящей передачи и требований к размещению данных в том или ином регионе.При внедрении Data Mesh нужно быть готовым к появлению некоторых проблем. Вот самые важные из них.Финансовой жизнеспособности проекта по созданию новой платформы угрожает несколько факторов. В частности, это неспособность платить за инфраструктуру, разработку дорогостоящих приложений, создание Data-продуктов или техобслуживание таких систем.Если команде по развитию платформы удастся создать инструмент, который закрывает техническую брешь, но объем данных и сложность Data-продуктов продолжат расти, цена решения может оказаться слишком высокой.С внедрением Data Mesh у доменов появляется много дополнительной работы. Ведь они привыкли быть просто пользователями отчетности, а теперь их надо как-то убедить, что овчинка стоит выделки. И когда они согласятся, придется координировать с ними важные релизы.Например, из-за доработки платформы иногда могут возникать радикальные изменения. Что, если такое произойдет у одного домена как раз в разгаре тестирования нового приложения? В этом случае они могут сорвать вам сроки на несколько месяцев.Отсутствие такого набора навыков — серьезное препятствие для компаний, стремящихся перейти на методологию Data Mesh. При децентрализации дата-менеджмента домены должны заняться этим самостоятельно. Действительно ли такое решение лучше центральной команды, которая обеспечивает интеграцию? Ответ на этот вопрос зависит от отраслевой специализации бизнес-доменов и происхождения данных.Делегирование доменам полного владения данными означает, что они должны заниматься проектом серьезно. Возможно, они наймут новых сотрудников или сами пройдут обучение, но не исключено, что вскоре требования окажутся для них непосильными. Когда производительность кардинально снизится, то там, то здесь будут постоянно появляться проблемы. Никакие инструменты здесь не помогут, потому что для решения проблем нужны знания в области дата-инжиниринга.Команде нужны соответствующие инструменты для создания Data-продуктов и мониторинга того, что происходит в компании. Возможно, некоторым доменам не хватает глубокого понимания технических метрик и их влияния на рабочие нагрузки. Команде по развитию платформы нужны ресурсы, позволяющие выявлять и решать проблемы, например, чрезмерной нагрузки или неэффективности.Сегодня сотрудники стремятся объединять данные из разных источников и не хотят подчиняться ограничениям «одного узла». Для этого существует два способа: виртуализация и дублирование данных. У каждого из них есть недостатки.создает семантическую модель за пределами источников данных без их физического переноса в другую БД. Она разбивает запросы пользователей, передает части запроса к источнику и собирает результаты в единое целое.Длянужно, чтобы команды обрабатывали данные, передаваемые от источников в приложения. Это может привести к резкому росту счетов на облачные сервисы. И мы говорим не только о стоимости хранения, но и о возможных расходах на исходящий трафик.С помощью инструмента lakeFS команды по развитию инфраструктуры данных могут предоставлять отдельные сервисы Data Mesh с собственным озером данных с историей версий через обычное объектное хранилище. В операциях Git-Like, доступных в lakeFS, есть все необходимые функции: Data Governance, непрерывный деплоймент и другие.Здесь перед нами стоит цель создать репозиторий lakeFS для каждого сервиса Data Mesh. Таким образом, каждый сервис будет работать изолированно, публикуя высококачественные данные для других сервисов или потребителей.Теперь у вас все готово! У каждого сервиса в изолированном режиме есть необходимые данные в собственном репозитории. Он может перемещаться между разными версиями в коммите. Главная ветка репозитория выступает в качестве единого источника истины.Чтобы проанализировать данные сервиса, нужно запустить процессы, потребляющие исходные данные и выдающие результат в репозитории lakeFS. Новый результат также передается в главную ветку, при этом создается новая версия, которую могут использовать остальные.Теперь пора настроить среду разработки и CI/CD для каждого сервиса Data Mesh. Именно это обеспечит эффективность работы и высокое качество результатов.Для грамотной разработки Data Mesh нам нужна среда разработки, которая позволяет вносить изменения в код сервиса, инфраструктуру или изолированные данные. Можно создать ветку из главной ветки репозитория и назвать ее dev-environment. Merges, направляемые в нее из главной ветки, позволяют экспериментировать с любой ее версией. Можно открыть ветку из dev-environment для тестирования на этапе разработки и закрыть ее сразу после окончания эксперимента. Можно последовательно проводить несколько экспериментов в одной ветке, используя Revert. Или экспериментировать одновременно в нескольких ветках, сравнивая результаты разных экспериментов.Есть также подробное руководство по созданию среды дата-разработки с помощью lakeFS Подключая новые источники данных или обновляя уже имеющиеся в репозитории, важно гарантировать соответствие спецификациям качества и техническим спецификациям. Когда мы описывали настройку репозитория для сервиса Data Mesh, мы предложили обновлять данные онбординга из исходных репозиториев прямо в главную ветку.Это не очень хорошая практика, ведь данные могут каскадом попасть в Data-пайплайны сервиса еще до того, как вы успеете проверить их качество. Вам же не нужны проблемы с качеством, простои или долгое восстановление? Вот что мы предлагаем в качестве альтернативы:Назначение этой инфраструктуры — убедиться в высоком качестве данных, предоставляемых другим сервисам или потребителям. Сложный сервис данных может выполнять тысячи небольших заданий за несколько часов. Поэтому нам нужна непрерывно действующая среда развертывания, которая автоматически восстанавливает сервис в случае обнаружения ошибок. Для этого можно объединить контроль версий (lakeFS) с автоматизированным управлением рабочими процессами (Airflow, Dagster или аналоги) и тестовым фреймворком.Принцип работы:Главная ветвь каждого репозитория выступает в качестве единого источника проверенной информации. Перед сохранением в ней исходные, промежуточные или выходные данные проверяются. Кроме того, данные тестируются по обе стороны интерфейса.Концепция Data Mesh пришла к нам из передовых методов разработки программного обеспечения, таких как Agile и микроциклы разработки. Перенос этих концепций в область анализа данных сопряжен с рядом трудностей, но, если сделать все правильно, он приносит огромную пользу.Термин «озеро данных» подразумевает монолитность, но на практике оно реализуется вместе с высокораспределенной технологией, такой как объектное хранилище. Потому команды по развитию платформы могут создавать Data Mesh, образуя изолированные Data-среды для Data-продуктов. Монолит можно разбить на маленькие озера — по одному на каждый продукт.Чтобы избежать дублирования данных, поверх озера нужен уровень абстракции, который обеспечивается с помощью lakeFS. Таким образом, каждый Data-продукт может использовать собственный репозиторий, а также потреблять данные других репозиториев и передавать в них свои."'https://habr.com/share/publication/720652/efa5f3229db8a4f853c0c53fdc7f25d3/'"['https://habrastorage.org/r/w1560/webt/3w/0e/nt/3w0entk-yamxbyf04azzrqir-ru.png', 'https://habrastorage.org/r/w780q1/webt/zy/x3/gz/zyx3gzzs60nxg2dpna0idjna4qg.jpeg', 'https://habrastorage.org/r/w32/getpro/habr/avatars/50c/1f5/38e/50c1f538ee51dd5d313207a135d60d57.jpg', 'https://habr.com/share/publication/720652/efa5f3229db8a4f853c0c53fdc7f25d3/', 'https://habrastorage.org/getpro/habr/branding/174/2c4/3a5/1742c43a5b504987a0fadf577a0bd4de.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/company/9ed/c74/6b4/9edc746b484c805ecad1f941b5f7068a.png', 'https://habrastorage.org/getpro/habr/avatars/50c/1f5/38e/50c1f538ee51dd5d313207a135d60d57.jpg', 'https://habrastorage.org/r/w780q1/webt/7w/xp/9y/7wxp9y2sv9aeucoonkfpnhvlpu4.jpeg', 'https://vk.com/rtrg?p=VK-RTRG-541074-aOhpd&event=habr_bigdata']"
19'720932'Как мы улучшаем выделение интентов в наших продуктах'Привет, Хабр! На связи Мурат Апишев, руководитель направления NLP R&D в Just AI. Одним из ключевых направлений компании является разработка инструментов для создания голосовых и чат-ботов. Задача...'https://habr.com/ru/post/720932/'"Привет, Хабр! На связи Мурат Апишев, руководитель направления NLP R&D в Just AI. Одним из ключевых направлений компании является разработка инструментов для создания голосовых и чат-ботов. Задача создания классификатора интентов в таких проектах является краеугольным камнем, и в этой статье я бы хотел поделиться некоторыми апдейтами наших продуктов в этом направлении. Речь пойдет о внедрении моделей классификации и парафраза на основе трансформеров. Приятного чтения!

Введение

В мире NLP сейчас происходит много интересного и почти фантастического. Но простые задачи обработки текстов не теряют своей актуальности, ведь для огромного количества компаний важно, чтобы их чат-боты определяли намерения пользователей быстро, качественно и дешево.

Для рядового специалиста по анализу данных и ML не составляет особого труда собрать хороший классификатор. Даже если данные сложные, можно сделать дополнительную разметку, подобрать модели, поработать над обучением и получить нужный результат. Но в мире платформ для чат-ботов (равно как и в облачных ML-сервисах общего назначения) упор делается на быстрое обучение по кнопке с минимальным вовлечением пользователя и пусть не идеальным, но достаточно хорошим качеством результата. Когда вы располагаете необходимым набором данных и/или задачи не очень сложные с т.з. близости классов и их сбалансированности, такого результата достичь несложно.

Проблема заключается в том, что обучающие выборки некоторых представителей малого и среднего бизнеса оставляют желать лучшего по всем критериям, и именно у этой категории клиентов обычно нет инхаус-специалистов и наработанных компетенций для работы с такими данными. Поэтому в подобных случаях нам бы хотелось улучшить работу NLU-решений на своей стороне.

Работа с интентами в JAICP

В нашей платформе JAICP до недавнего времени пользователь имел несколько опций для работы с интентами: паттерны, STS, Classic ML и Deep Learning (DL):

Паттерны — расширенный аналог регулярных выражений. Их можно использовать для решения сложных кейсов, но проблемы очевидны: сложность и длительность разработки и поддержки, нулевая переиспользуемость;

STS — простой алгоритм вычисления соответствий между текстами по семантической близости слов. Уже не требует ручной работы и предоставляет понятную информацию для дебага классификации, но подходит только для небольших датасетов и нагрузок на систему. Алгоритм работает медленно из-за обработки лингвистической информации (синонимы, нормальные формы и т.д.), но при этом небольшого количества тренировочных данных недостаточно для хорошего качества классификации;

Classic ML — это линейная модель на разнообразных текстовых признаках, которая показывает себя достойно в задачах с адекватным количеством объектов, но требует предобработки текста, сбалансированности классов и относительно большого объема данных, о чём редко беспокоится среднестатистический пользователь платформы без опыта работы с ML-алгоритмами;

DL — под этим названием в платформе подразумевается модель на основе CNN с одномерными свертками над предобученными векторами, которая в целом неплохо и быстро справляется с разными задачами, если адекватно выставить гиперпараметры обучения.

Практика показала, что многие авторы ботов пишут интенты на паттернах, потому что их работа более предсказуема и контролируема. Или же они используют классификатор STS, потому что он опять же самый понятный и относительно хорошо показывает себя на небольших датасетах. Эти подходы работают, но пользователи периодически выражают недовольство качеством распознавания интентов. Собрав фидбек, мы решили вплотную заняться этой проблемой.

Анализ показал, что в реализациях сервисов присутствуют недочеты, а также неоптимальность выбора дефолтных параметров, с которыми происходит обучение «по кнопке». В первую очередь мы решили исправить эти технические проблемы, но очевидной также стала необходимость создания нового сервиса для классификации на основе более совершенных мультиязычных моделей, который бы требовал от пользователя минимальной вовлеченности и подходил бы для практически всех типов датасетов, включая маленькие, несбалансированные и те, в которых разные языки смешиваются в одной фразе. Мы сразу сконцентрировали внимание на переносе обучения (transfer learning), тем более, что в алгоритме DL такой подход тоже используется. В последние годы стандартом для переноса обучения в задачах классификации и разметки стали BERT-like модели на основе кодировщика Transformer. Приятным следствием такого подхода является возможность использования одних и тех же моделей (или набора моделей в одном сервисе со стандартным интерфейсом) не только для классификации, но и для целого ряда иных задач (QA, FAQ, суфлёр и т.д.).

Тестирование собственного классификатора

Выбрав несколько подходящих базовых моделей с HuggingFace, мы с лингвистами компании провели тесты на наших внутренних данных. Проверялась прежде всего возможность получать адекватные результаты при минимальных выборках (1-5 обучающих текстов на класс). Модели использовались в качестве векторизаторов, поверх которых помещались либо KNN-индекс (мы используем для ускоренного поиска ближайших соседей NMSLIB) с различными стратегиями определения класса по соседям, либо обучаемая линейная голова.

Качество результатов оказалось неплохим, но не существенно выше, чем у нормально обученного на тех же данных алгоритма DL. Поэтому решили дообучать базовые модели для русского языка — у нас как раз и данные внутри подходящие есть и потребность в хорошем качестве наиболее высокая для русского. Данные диалоговые, поэтому модель мы учили предсказывать близость между диалоговой репликой и предшествовавшим ей контекстом. И это зашло хорошо.

После многочисленных подходов к снаряду удалось получить обновленные модели, которые в классификаторе с линейной головой по метрикам показали себя ощутимо лучше (+2-8 пунктов F1-меры на разных датасетах) базовых векторизаторов, что для первой версии мы посчитали приемлемым.

Для валидации качества работы наших классификаторов (назовем новый Transformer) мы провели тестирование на наборе русскоязычных датасетов (как открытых, так и наших внутренних), дополнительно сравнивая качество с Dialogflow как с одной из самых известных диалоговых платформ, в которой можно достаточно свободно проводить тесты. Ниже показаны параметры 9 датасетов на русском языке и результаты для них.

В публичных датасетах HWU-20-Ru и Chatbots-ru (из статьи от Ростелекома ) разбиение на train/test определено заранее для чистоты сравнения с результатами предыдущего исследования, в прочих разбили случайным образом в пропорции 60/40. Время обучения измерялось в секундах. В Dialogflow отказ от классификации засчитывался как ошибочная классификация.

Все наши алгоритмы запускаются в доработанных версиях с фиксированными дефолтными параметрами (они станут доступны всем пользователям JAICP после обновления платформы). Для честности сравнения обучение происходит как и в жизни, «по кнопке», без дополнительной настройки.

Набор данных Число классов Размер train-выборки Размер test-выборки Датасет 1 128 1154 770 Датасет 2 4 196 132 Датасет 3 52 432 288 Датасет 4 10 213 142 Датасет 5 20 411 275 Датасет 6 56 793 530 Датасет 7 99 1506 1004 Датасет 8

(HWU-20-Ru) 20 100 100 Датасет 9

(Chatbots-ru) 79 5563 1388

Характеристики датасетов, для публичных указаны названия.

Итак, результаты замеров:

Датасет 1

Алгоритм F1-micro F1-macro Время обучения Classic ML 0.60 0.54 44 DL 0.66 0.59 59 DialogFlow 0.61 0.56 13 Transformer 0.72 0.66 12

Датасет 2

Алгоритм F1-micro F1-macro Время обучения Classic ML 0.81 0.81 5 DL 0.80 0.81 19 DialogFlow 0.81 0.83 13 Transformer 0.81 0.83 9

Датасет 3

Алгоритм F1-micro F1-macro Время обучения Classic ML 0.74 0.62 17 DL 0.70 0.58 36 DialogFlow 0.79 0.69 12 Transformer 0.83 0.73 11

Датасет 4

Алгоритм F1-micro F1-macro Время обучения Classic ML 0.82 0.78 6 DL 0.86 0.87 24 DialogFlow 0.87 0.87 12 Transformer 0.85 0.85 13

Датасет 5

Алгоритм F1-micro F1-macro Время обучения Classic ML 0.88 0.83 7 DL 0.90 0.86 40 DialogFlow 0.92 0.91 13 Transformer 0.92 0.90 14

Датасет 6

Алгоритм F1-micro F1-macro Время обучения Classic ML 0.71 0.59 21 DL 0.75 0.65 64 DialogFlow 0.75 0.65 12 Transformer 0.76 0.67 17

Датасет 7

Алгоритм F1-micro F1-macro Время обучения Classic ML 0.77 0.58 44 DL 0.79 0.61 90 DialogFlow 0.80 0.68 14 Transformer 0.81 0.68 26

Датасет 8 (HWU-20-Ru)

Алгоритм F1-micro F1-macro Время обучения Classic ML 0.67 0.65 4 DL 0.80 0.79 13 DialogFlow 0.72 0.72 11 Transformer 0.92 0.92 12

Датасет 9 (Chatbots-ru)

Алгоритм F1-micro F1-macro Время обучения Classic ML 0.73 0.71 58 DL 0.78 0.77 201 DialogFlow 0.63 0.62 15 Transformer 0.85 0.83 92

Как и ожидалось, в среднем наиболее высокий прирост по сравнению с более простыми методами получается именно на проблемных датасетах с относительно небольшим числом примеров на класс (хотя и на последнем, самом большом наборе данных, качество ощутимо выросло). Скорость работы нового классификатора на момент замеров далека от оптимальной — мы находимся в процессе внедрения оптимизаций на уровне модели и сервиса, которые позволят получить значительный прирост производительности.

Альтернатива «ручной» генерации тренировочных фраз

Как было замечено выше, создание примеров для интентов — один из вызовов, с которыми сталкиваются пользователи диалоговой платформы. Далеко не всегда в наличии есть гора диалогов, из которой можно набрать достаточно разнообразных примеров для классификатора. На деле может не быть ничего, потому что чат-бот только создается или же примеров из жизни очень мало по всем или отдельным классам. Из-за этого много времени уходит на генерацию тренировочных фраз, например: «Как 20 раз сказать «хочу оформить заказ» разными словами». Как следствие, создаваемые таким образом выборки не только малы, но и достаточно однообразны по составу слов.

Для борьбы с этим можно использовать краудсорсинг (дорого, медленно, качественно) или автоматический парафраз (проще, но зато быстро и почти бесплатно). Так как кейсы применения различны, мы хотим использовать оба подхода. Большие генеративные модели за последние годы сильно продвинулись в решении разнообразных текстовых задач. Наиболее известный их представитель, ChatGPT, способен перефразировать тексты почти на человеческом уровне (а в ряде случаев и лучше обычного краудсорсера), и масштабы его использования для подобных целей будут расти (нами в том числе). Тем не менее, на момент старта разработки мы решили обучать собственную модель, поскольку это:

1) Более контролируемо и изменяемо;

2) Дешевле и быстрее;

3) Не требует пересылки данных третьей стороне. Можно дообучить свою небольшую модель под домен клиента и поставить решение в его внутренний контур.

Есть готовые относительно небольшие модели для парафраза, но свои мы учили самостоятельно, чтобы иметь возможность добавлять в них различные элементы управления результатом. В качестве данных использовались пары фраз из открытых источников и наш собственный большой набор данных, полученный с помощью обратного перевода (back translation), в сумме больше 2М пар. После ресерча и ряда консультаций было решено использовать в качестве базовой модели полный трансформер, потому что в seq-2-seq задачах он показывает себя стабильнее декодерных моделей. Мы выбрали предобученные T5 от Сбера и Google. В первой версии обучалась обычная seq-to-seq модель с кросс-энтропийным лоссом. Для генерации используется nucleus sampling, для удобства пользователей были протестированы и подобраны четыре режима генерации, регулирующие степень разнообразия результирующего текста. Тестирование моделей производилось силами внутренних асессоров, оценивалась адекватность парафраза на наборе реплик из логов чат-ботов по различным тематикам.

Несколько примеров относительно неплохих сырых вариантов для фраз на разные темы в разных режимах:

Исходная фраза Новая фраза Парафраз — изложение текста своими словами. Парафразы — это осмысление текста собственными словами. Парафраз — изложение текста своими словами. Перефразируя это, мы излагаем текст собственными словами. Google — американская транснациональная корпорация в составе холдинга Alphabet, инвестирующая в интернет-поиск, облачные вычисления и рекламные технологии. Google является американской транснациональной корпорацией, входящей в холдинг Альфабет, которая инвестирует в интернет поиск, облачное вычисление и рекламу. За окном снег, а значит скоро новый год, ура! У нас за окном уже снег — это означает, что скоро Новый год! За окном снег, а значит скоро новый год, ура! За окошком снег и, следовательно, скоро Новый год! Ура! За окном снег, а значит скоро новый год, ура! Снаружи лежит снежный покров — это означает приближающийся Новый Годы Ура! Фараон — современное наименование правителей Древнего Египта. Фараон - это современное название правителей Древних египтян. Фараон — современное наименование правителей Древнего Египта. Фараона — это современное имя рулевых древнего Египта. Санкт-Петербург назван в честь святого Петра Петербург носит имя Святого Петра. Сейчас идет конференция на которой рассказывают про обработку естественного языка Конференция, посвященная обработке естественных языков, проходит сейчас.

Впрочем, еще есть над чем поработать. Иногда модель без необходимости сбивается на транслитерацию (баги исходных данных) и английские варианты слов, обычно это свойственно более «вольным» режимам генерации:

Исходная фраза Новая фраза Парафраз — изложение текста своими словами. Paparafrus — это пересказ текста собственными словами.

А иногда коверкает отдельные слова (в т.ч. сущности) и существенно изменяет даты:

Исходная фраза Новая фраза Манчестер Юнайтед был основан в 1878 году под названием «Ньютон Хит» (Ланкашир энд Йоркшир Рейлуэй), в 1902 году изменил название на «Манчестер Юнайтед». Манчестер Юнайтедз был основан в 1978 году под названием Newton Hot (Lankshire orkshire Relue), а в 1902г. он был переименован в Манчестер Юнайтедз.

Как мы влияли на генерацию

Следующим шагом стало добавление возможности влиять на генерацию. В рамках парафраза полезной является возможность поощрять/запрещать появление каких-то слов и словосочетаний в результате. Запрещать модели использовать определенные токены можно чисто технически, накладывая ограничения на процесс генерации методами библиотеки transformers. Для поощрения использования определенных токенов мы добавили во вход модели дополнительный контролирующий сегмент, в котором перечисляются слова, появление которых в результате очень желательно.

Сперва для проверки гипотезы мы просто добавляли в половине выборки случайные слова и словосочетания из верных парафразов в этот сегмент на входе, т.е. никак не модифицировали лосс, и это уже заработало. Позже решили усилить эффект, добавив в слагаемые кросс-энтропии взвешивание, и для токенов из контролирующего сегмента штраф за неверную генерацию был существенно повышен. Тестирование показало, что к последней версии модели долю адекватно генерируемых вариантов парафразов в тестовой выборке с учётом требований по наличию слов удалось довести до 70%. Основные сложности связаны с сохранением имен, дат и числительных. Общая адекватность модели по первому тесту осталась на прежнем уровне.

Исходная фраза: «Хочу закрыть счет в вашем банке»:

Требования по наличию Требования по отсутствию Новая фраза - - Я хочу закрыть свой счет у вас в банке. аккаунт - Я хочу закрыть свой аккаунт у вас в банке. аккаунт хочу Я хотел бы закрыть свой аккаунт у вас в банке.

Исходная фраза: «Курьер сообщил, что у него нет части заказа, я хочу вернуть деньги»:

Требования по наличию Требования по отсутствию Новая фраза - - Курьер сказал, что части заказа у него не было, хочу вернуть наличные. монеты - Курьер сказал, что части заказов у него не было, мне монеты хочется вернуть. новый год заказ, заказа Курьер в новый год сказал, что не имеет части ордера, хочу возврата.

Больше возможностей управления

Поскольку хороший вариант перефразирования не всегда получается с первого раза, для контроля качества и отбора мы добавили к сервису ранжирование по семантической близости с помощью внешней модели. Отбор по топу семантической близости парафраза к исходной реплике помогает избавиться от неадекватных результатов в ряде случаев (естественно, если среди кандидатов в принципе есть адекватные). Приведем несколько примеров фильтрации:

Исходная фраза Ранжированный список новых фраз От какой суммы будет бесплатная доставка? С какой суммы будет осуществляться бесплатная доставка? На какую сумму будет осуществляться бесплатная доставка? На какую сумму будет производиться бесплатная доставка? Какая сумма будет свободной доставки? От какой суммы вы будете получать бесплатную доставку? С какой суммы будет БЕСПЛАТНАЯ доставка? С какой суммы будет бесплатный груз? Сколько будет бесплатной доставки? С какой суммы будет бесплатный проезд? Какова будет бесплатная доставка в течение всей этой суммы? Курьер сообщил, что у него нет части заказа, я хочу вернуть деньги. Курьер рассказал, что части заказа у него нет, хочу вернуть свои деньги. Курьер сказал, что части заказа у него нет, хочу возвратить деньги. Курьер сказал, что части заказа у него нет, хочу вернуть деньги обратно. Курьер сказал, что часть заказа у него отсутствует, хочу вернуть деньги обратно. Курьер сказала, что не имеет части заказа и что я хочу вернуть ему деньги. Курьер сказал, что он не имеет части заказа и хочет вернуть мне деньги. Курьер сказал, что не имеет части заказа и хочет вернуть деньги.. Курьер сказал, что он не имеет части заказа и хочет возвратить деньги. Курьер сказал, что он не имеет части заказа и хочет возвратить деньги. Курьер сказал, что он не имеет части заказа и хочет вернуть деньги.

При этом такой отбор может улучшить качество в ущерб разнообразию, так что в определенных случаях ранжирование можно выключать или даже использовать в обратную сторону.

В качестве дополнительной опции мы добавили в модель возможность симплификации текстов на основе датасета RuSimpleSentEval и библиотеки для промпт-тюнинга ru-prompts от Sber AI:

Исходная фраза Новая фраза Новая фраза с симплификацией Общая площадь Соединённого Королевства составляет 242 500 кв.км, а численность населения в 2020 году оценивалась в более чем 67 млн человек Общая площадь Великобритании составляет 2424,5 тысячи квадратных метров, а население в 2020 г. оценивалось более 67 миллионов. Общая площадь Королевства составляла 242,5 тыс. кв. км, и в 2020 г. население составляло 67 миллионов человек. Пётр I Алексеевич, прозванный Великим (9 июня 1672 года - 8 февраля 1725 года) — последний царь всея Руси (с 1682 года) и первый Император Всероссийский (с 1721 года). Петр I Алексеевич по прозвищу Великий (9.06.2012 - 8.02.1725) был последним царем всей России (1682) и первым императором Всероссийским (1721). Пётр I Алексевич был последним царем Руси с 1682 и первым Императором Всея Руси с 1721. Пушкин один из самых авторитетных литературных деятелей первой трети XIX века. Пушкин — один из наиболее авторитетных литературоведов первой трети девятнадцатого века. Пушкин — авторитетный литературный деятель первой трети 19 века. В природном очаге заражение обычно происходит через укус блохи, ранее питавшейся на больном грызуне. В природных очагах инфекция возникает, как правило, при укусе блох, которые ранее питались больным грызуном. Заражение происходит из-за укуса блохи.

На первом этапе внедрения парафраза мы просто предлагаем сгенерировать другие варианты написанной пользователем тренировочной фразы, которые он сможет сам поправить и добавить в выборку или, вдохновившись ими, написать свой собственный вариант. Пока мы даже не нагружаем пользователя выбором силы перефразирования и делаем это рандомно. А уже после этого этапа MVP и получения первого фидбека мы хотим дать пользователю возможность более гибко настраивать генерацию с помощью описанных выше настроек модели. Например, управление появлением слов может быть полезным в связке с предварительным выделением важных сущностей, появление которых в парафразе является обязательным.

Подводя итоги

Разработка новых инструментов для классификации интентов проводилась одновременно с разработкой новой версии всего внутреннего NLU-движка CAILA (проект реализуется при поддержке РФРИТ) продуктов Just AI. CAILA 2.0, а точнее app.caila.io — это платформа для встраивания, хостинга и масштабирования NLP-моделей. Именно этих возможностей нам не хватало для использования хороших тяжелых моделей в продакшене, поэтому мы много работали над созданием новой микросервисной архитектуры и над переносом в неё (в ряде случаев — с улучшением, донастройкой и оптимизацией) уже используемых в наших продуктах NLU-сервисов. Таким образом, сейчас первым клиентом CAILA 2.0 становится наша же платформа JAICP. На очереди — наши клиенты on-premise, а со временем мы планируем развивать облачную платформу ещё и как маркетплейс NLP-сервисов.

Итак, мы рассказали в общих чертах о том, как улучшаем наши продукты с помощью трансформеров и надеемся, что лучшее качество классификации из коробки позволит разработчиком ботов сосредоточиться на бизнес-логике своих решений. Мы же продолжим дальнейшую работу, будем улучшать и ускорять наши модели!"'https://habrastorage.org/getpro/habr/upload_files/d32/98e/c7e/d3298ec7e091270ecb7a80d19c611c18.png'"['https://habrastorage.org/r/w32/getpro/habr/avatars/de3/cbe/85d/de3cbe85d4b75bd49b50a3b8cc71b491.jpg', 'https://habrastorage.org/getpro/habr/upload_files/d32/98e/c7e/d3298ec7e091270ecb7a80d19c611c18.png', 'https://habrastorage.org/getpro/habr/company/b5d/4fc/a56/b5d4fca561723ab28c7f6a60e56393a9.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/avatars/de3/cbe/85d/de3cbe85d4b75bd49b50a3b8cc71b491.jpg']"
